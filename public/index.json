
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Descriptive text about SurveyHarmonies project.\nDownload our description and open call for collaboration.\nGet involved in services: our ongoing projects, team of contributors, open-source libraries and use our data for publications. See some use cases.\nFollow news about us.\nContact us .\n","date":1680684000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1680684000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://surveyharmonies.eu/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Descriptive text about SurveyHarmonies project.\nDownload our description and open call for collaboration.\nGet involved in services: our ongoing projects, team of contributors, open-source libraries and use our data for publications.","tags":null,"title":"SurveyHarmonies","type":"authors"},{"authors":["daniel_antal"],"categories":null,"content":"Daniel Antal is an experienced data scientist, consultant, economist, and the co-founder of Reprex, a Netherlands-based startup that brings the benefits of big data to small organizations with shared resources and research automation. He applies data science practice, open-source software development with sound economics and valuation techniques.\nHe is also a research affiliate at the Centre for Competition Policy and at the Institute for Information Law of the University of Amsterdam.\n","date":1678442400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1678465080,"objectID":"0d25123d0ac03749a9d015a7325a60af","permalink":"https://surveyharmonies.eu/authors/daniel_antal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/daniel_antal/","section":"authors","summary":"Daniel Antal is an experienced data scientist, consultant, economist, and the co-founder of Reprex, a Netherlands-based startup that brings the benefits of big data to small organizations with shared resources and research automation.","tags":null,"title":"Daniel Antal","type":"authors"},{"authors":["Reprex"],"categories":null,"content":"Reprex, is a Netherlands-based start-up company that makes big data reliable and accountable, delivering trustworthy analytics and AI solutions. We validate multiple data sources and are able to merge private and proprietary data with open data. We bring novel insight into policy and business problems, as well as scientific research. Our work addresses the potentially negative effects of black-box proprietary algorithms.\n","date":1656486720,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1656486720,"objectID":"5b625b07d7705f5a7a9a05bcaa182b56","permalink":"https://surveyharmonies.eu/authors/reprex/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/reprex/","section":"authors","summary":"Reprex, is a Netherlands-based start-up company that makes big data reliable and accountable, delivering trustworthy analytics and AI solutions. We validate multiple data sources and are able to merge private and proprietary data with open data.","tags":null,"title":"Reprex","type":"authors"},{"authors":["rOpenGov"],"categories":null,"content":"rOpenGov is an international open source developer network for open government data analytics in R and has released various packages for retrieval, refinement, and analysis of open data from statistical authorities over the past decade. The network was formally introduced at the NIPS Machine Learning Open Source Software workshop in 2013, and has now active contributors from multiple countries. The project aims to facilitate the seamless incorporation of open government data into reproducible statistical and probabilistic programming workflows.\n","date":1623844800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1623844800,"objectID":"1c30eb6dcd447650988eddc5a6b16517","permalink":"https://surveyharmonies.eu/authors/ropengov/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ropengov/","section":"authors","summary":"rOpenGov is an international open source developer network for open government data analytics in R and has released various packages for retrieval, refinement, and analysis of open data from statistical authorities over the past decade.","tags":null,"title":"rOpenGov","type":"authors"},{"authors":["Amadea Music"],"categories":null,"content":"Amadea Music is a UK-registered company that in partnership with The state51 Conspiracy provides an optimum solution for digital distribution of music content. They work with all the world’s music services, and run a well organized online platform which operates 24/7. The terms that they give to their clients (artists, labels and distributors) make their digital distribution deal one of the fairest in the world. Amadea Music’s team consists of people who like to work in the world of music, which motivates them to give their best to make the partners happy and well taken care of. The team at Amadea believes that they should not only work as a standard service company but help artists, labels and distributors adapt to and develop in the ever-changing music industry.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"31977e44300e22fdbaadb8e02ca8991a","permalink":"https://surveyharmonies.eu/authors/amadea_music/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/amadea_music/","section":"authors","summary":"Amadea Music is a UK-registered company that in partnership with The state51 Conspiracy provides an optimum solution for digital distribution of music content. They work with all the world’s music services, and run a well organized online platform which operates 24/7.","tags":null,"title":"Amadea Music","type":"authors"},{"authors":["musicautor"],"categories":null,"content":"Musicautor/Музикаутор is an organization of composers, authors and music publishers for collective management of copyright. Musicautor offers more than 95% of the world’s musical repertoire for the Bulgarian market. It represents over 3000 Bulgarian authors and also over 2 500 000 authors from all over the world, who are members of sister societies, with whom Musicautor has contracts for reciprocal representation.\nMusicautor actively works with the International Confederation of Societies of Authors and Composers (CISAC) as a member since 1993, the international organisation representing mechanical rights societies (BIEM). Since 2018 Musicautor is a member in European Grouping of Societies of Authors and Composers (GESAC).\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"10124998373e40e4915fcb9832ea1fa2","permalink":"https://surveyharmonies.eu/authors/artisjus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/artisjus/","section":"authors","summary":"Musicautor/Музикаутор is an organization of composers, authors and music publishers for collective management of copyright. Musicautor offers more than 95% of the world’s musical repertoire for the Bulgarian market. It represents over 3000 Bulgarian authors and also over 2 500 000 authors from all over the world, who are members of sister societies, with whom Musicautor has contracts for reciprocal representation.","tags":null,"title":"Artisjus","type":"authors"},{"authors":["james_edwards"],"categories":null,"content":"James Edwards, PhD is a senior researcher at SINUS Market and Social Research in Berlin, Germany. After completing his doctorate in Systematic Musicology at UCLA in 2015, he conducted research as a visiting scholar at Okinawa International University and taught as an adjunct professor at Lewis \u0026amp; Clark College prior to joining SINUS in 2017. His work at SINUS consists of mixed-methods social research design and implementation for governmental and non-governmental organisations in Germany and beyond; concurrently, he has remained active in the field of musicology, working primarily on socio-ecological and socio-technical systems approaches to musical culture in Japan. He assists the DMO in curating primary survey data, as well as in musicological dimensions of DMO research and dissemination.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d6a0b9e317bb36c94097b4409b2ef594","permalink":"https://surveyharmonies.eu/authors/james_edwards/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/james_edwards/","section":"authors","summary":"James Edwards, PhD is a senior researcher at SINUS Market and Social Research in Berlin, Germany. After completing his doctorate in Systematic Musicology at UCLA in 2015, he conducted research as a visiting scholar at Okinawa International University and taught as an adjunct professor at Lewis \u0026 Clark College prior to joining SINUS in 2017.","tags":null,"title":"James Edwards, PhD","type":"authors"},{"authors":["musicautor"],"categories":null,"content":"Musicautor/Музикаутор is an organization of composers, authors and music publishers for collective management of copyright. Musicautor offers more than 95% of the world’s musical repertoire for the Bulgarian market. It represents over 3000 Bulgarian authors and also over 2 500 000 authors from all over the world, who are members of sister societies, with whom Musicautor has contracts for reciprocal representation.\nMusicautor actively works with the International Confederation of Societies of Authors and Composers (CISAC) as a member since 1993, the international organisation representing mechanical rights societies (BIEM). Since 2018 Musicautor is a member in European Grouping of Societies of Authors and Composers (GESAC).\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e348274891fbab1d09d3eb98ade25cf8","permalink":"https://surveyharmonies.eu/authors/musicautor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/musicautor/","section":"authors","summary":"Musicautor/Музикаутор is an organization of composers, authors and music publishers for collective management of copyright. Musicautor offers more than 95% of the world’s musical repertoire for the Bulgarian market. It represents over 3000 Bulgarian authors and also over 2 500 000 authors from all over the world, who are members of sister societies, with whom Musicautor has contracts for reciprocal representation.","tags":null,"title":"Musicautor","type":"authors"},{"authors":["team"],"categories":null,"content":"Our observatories are managed by Reprex, a Dutch open data and AI startup. Our observatories are developed in an open collaboration with developers and curators, and a team of devoted service developers who hope that our observatories will remain both technically and financially feasible. If you would like to involved in the business feasibility, we are open to increasing the range of capabilities that our team encompasses, from communications and marketing to human resources, data analysis, and research \u0026amp; development. Feel free to drop us a line and propose your own way of collaborating.\nPassion About Our Topic You are passionate about one of our topics, but you do not feel that you have the skills yet to become a data curator or a developer. You have a curiosity in the field of economic policies, particularly in computational antitrust, innovation research, and understanding the statistically under-represented micro- and small enterprises, join our Economy Data Observatory as a volunteer. You are passionate about environmental research of climate change, designing urban, social and economic mitigation strategies, or undertanding how people think about climate change, join our Green Deal Data Observatory team as a volunteer. You want to know how musicians can make a living after the pandemic? How can we make sure that music made by womxn, small country artists or artists of color gets and equal chance? Are you interested in the future of ethical AI and data governance? Join our Digital Music Observatory team as a volunteer. Get inspired\nPassion For Trustworthy AI, Open Data and Open Source Projects You want to learn to write scientifically valid, open source code in R or Python, but you are a beginner. We help you anywhere - you can even start copyediting or technical documentation (it is a must in open source development) and create tutorials for you to interact with our data and products (if if helps you, it will help others.)\nAs a business economist or legal professional, you are interested how open data, open source software, research automation and ethical, trustworthy AI products can find their market.\nAs a blogger, data journalist or marketeer you would like to help to make open data, and transparent, ethical, open AI more widely known and used.\nTechnical Requirements Make sure that you read the Contributors Covenant. You must make this pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. Participating in our data observatories requires everybody to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. It’s better this way for you and for us!\nPlease, follow us on social media, it really helps us finding new users and showing that we are able to grow our ecosystem.\nGreen Deal Data Observatory on Linkedin and Green Deal Data Observatory on Twitter Economy Data Observatory on Linkedin and Economy Data Observatory on Twitter Digital Music Data Observatory on Linkedin and Digital Music Data Observatory on Twitter Send us this text file with your biography elements.\nIf you feel you need chatting on onboarding, contact us on Keybase - it’s lightweight, discrete, encrypted, your mother, partner and friends are not there, it is free, open source, and can share/exchange files, too. Otherwise in email.\nMore about contributing: Automated Observatory Contributors’ Handbook.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3727ee9a01674e6999f24524b05a5897","permalink":"https://surveyharmonies.eu/authors/team/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/team/","section":"authors","summary":"Our observatories are managed by Reprex, a Dutch open data and AI startup. Our observatories are developed in an open collaboration with developers and curators, and a team of devoted service developers who hope that our observatories will remain both technically and financially feasible.","tags":null,"title":"Observatory Business Associate","type":"authors"},{"authors":["PROPHON"],"categories":null,"content":"PROPHON is a society for collective management of neighbouring rights in music and for 20 years now have sustained productive partnership between performing artists and producers and all business users of their recordings. Prophon’s team administers the rights of works from the Bulgarian and international repertoire.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e043d97cfcff4583614a7268e7d4411b","permalink":"https://surveyharmonies.eu/authors/prophon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/prophon/","section":"authors","summary":"PROPHON is a society for collective management of neighbouring rights in music and for 20 years now have sustained productive partnership between performing artists and producers and all business users of their recordings.","tags":null,"title":"PROPHON","type":"authors"},{"authors":["thermowatt"],"categories":null,"content":"Thermowatt is an established Hungarian engineering company acting in the fast-growing renewable energy sector utilizing raw (untreated) communal sewage waste water energy for heating and cooling of buildings in city environments. Its patented technology to filter sewage water out of sludge, pass the brown water through a set of heat exchangers and generate low-temperature heating water through commercially available heat pumps to ensure adequate heating and cooling of larger building complexes is recognized worldwide. The company was set up in 2007 and has installed its technology in 6 instances thus far.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a22d247fc7a020f8a5780904ded18b4b","permalink":"https://surveyharmonies.eu/authors/thermowatt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/thermowatt/","section":"authors","summary":"Thermowatt is an established Hungarian engineering company acting in the fast-growing renewable energy sector utilizing raw (untreated) communal sewage waste water energy for heating and cooling of buildings in city environments.","tags":null,"title":"Thermowatt","type":"authors"},{"authors":["volunteer"],"categories":null,"content":"This is a sample profile for a volunteer who contributes developing our Net Zero Data Observatory with various actions, such as\nhelping with our social media presence connecting us to data journalists and climate journalist to use or free, open data, maps and visualizations find NGOs and activist who need data for their climate related action. ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7a6a5fe1a39f3a7e9bc1c079fe71395b","permalink":"https://surveyharmonies.eu/authors/volunteer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/volunteer/","section":"authors","summary":"This is a sample profile for a volunteer who contributes developing our Net Zero Data Observatory with various actions, such as\nhelping with our social media presence connecting us to data journalists and climate journalist to use or free, open data, maps and visualizations find NGOs and activist who need data for their climate related action.","tags":null,"title":"Volunteer Contributor","type":"authors"},{"authors":["SurveyHarmonies","musicaire"],"categories":null,"content":" Reporting the impacts of the entire value chain. The Eviota project aims to create sustainability reports connected to the financial accounts of companies, NGOs, and civil society actors. The first phase concentrates on greenhouse gases and air pollutants. We want to create reliable estimates of the carbon and other pollutants footprint of music-related (social) enterprises based on their spending (“connected financial and sustainability reporting”.)\nCreating connected sustainability reports has many advantages.\nIt shows consumers, donors, and buyers that the company cares for sustainable growth. The organization can ask for grants related to increasing sustainability. In the EU, the company will be eligible for cheaper green loans, green insurance, and green investments. Large corporations, including music event donors, may require their supply chain to produce credible sustainability metrics. There are some difficulties that we want to overcome in this project.\nConnected financial and sustainability reports are complex, they require plenty of data, and mandatory only for ’large’ corporations. Just like small companies can make ‘simplified tax returns’ and ‘simplified financial reports’, we aim to create a less complex and cheap simplified, connected financial and sustainability report. Due to the high complexity, they take a long time to create and are costly: the European Commission estimates their total cost at €10,000. In Europe, there are a handful of large organizations present in the music industry. So, there is no regulatory push for music enterprises to engage in sustainability reporting. However, this means that they cannot benefit from the advantages above.\nTable of Contents Our approach How does it work? What is the report? Methodology Open collaboration Why are we developing this service? Future plans: Social Sustainability and Anti-Bribary MusicAIRE Green Recovery in the Music Sector Our approach Most sustainability calculators are very complex because they use many data inputs from the company. Our mission is to reduce the complexity; however, this would require plenty of experience to define the shortcuts. We will compare all spending (upstream value chain or suppliers) and all income (downstream value chain or buyers) to the know spending of all similar organizations in your country in the comparison year.\nTo reduce the data need, we only take into consideration cost/income groups that meet the financial materiality treshold, i.e. 3% of your total costs or total business-to-business sales. We offer free, manual calculation in the first phase to ensure we define these simplifications well. To reduce the time needed to collect data about your purchases and sales, we will rely on a part of the “trial balance”, because this is available in your accounting system (and can be exported by your accounting software.) The trial balance is an annual summary of the general ledger accounts. We need only the expenses and revenues accounts, and do not need assets, liabilities, gains and losses.\nA part of a fictitious Italian trial balance with Italian and English language labels. The blurred numbers are randomized from an actual trial balance and presented in a different currency than the original. Why the trial balance? We start from a document that every company has, and does not require extra management time to prepare, the so-called trial balance. This is an accounting document that can be obtained from the company’s accountant.\nA trial balance is a report that lists the balances of all general ledger accounts of a company at a certain point in time. The accounts reflected on a trial balance are related to all major accounting items, including assets, liabilities, equity, revenues, expenses, gains, and losses. It is primarily used to identify the balance of debits and credits entries from the transactions recorded in the general ledger at a certain point in time.\nNo extra management time is needed: it is already recorded by every company’s accountant. The general ledger is recorded by your accountant. We do not need the ledger, only the annual account summaries of revenues and expenses. It is not subjective. It states exactly what you were spending on. It is more or less standardized across Europe—and almost all countries of the world, with the exception of the U.S. and some other countries. We need to use the same working document that your accountant uses to maintain an important objectivity criterion: connectivity. This way your annual report will be consistent, if you say in the financial part that you spend 1000 euro on energy, then we will calculate the greenhouse gas emissions based on KWh volume of the the energy that cost you 1000 euros. This way we avoid a lot of data entry into the calculator. At this stage, you we do not offer an uploader, because we want to test manually different trial balances before automating the uploading process.\nHow does it work? In the future, we hope our …","date":1680684000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680684000,"objectID":"2f97ff05565853cb2be325ca1dd6bb75","permalink":"https://surveyharmonies.eu/sector/cap/","publishdate":"2023-04-05T09:40:00+01:00","relpermalink":"/sector/cap/","section":"sector","summary":"We will help small music organizations in their sustainability reporting, where detail data and reporting standards are only available for greenhouse gas emissions.","tags":["Music Eviota","Eviota","Sustainability","European Green Deal","MusicAIRE"],"title":"Cultural Access \u0026 Participation","type":"sector"},{"authors":["Daniel Antal","musicaire"],"categories":["Public talk"],"content":"The Eviota project aims to create sustainability reports connected to the financial accounts of companies, NGOs, and civil society actors. The first phase concentrates on greenhouse gases and air pollutants. We want to create reliable estimates of the carbon and other pollutants footprint of music-related (social) enterprises based on their spending (“connected financial and sustainability reporting”.)\nReporting the impacts of the entire value chain. 16:30: Meet and greet 17:00: Short presentation 17:15: Discussion of your reporting The discussion will be based on the accounting documents sent to us in advance. We will create a first version of a simplified sustainability report about your music or film company (for profit, or non-profit.) we will not share any financial information in the meetup, only visualizations of risk heatmaps, and common reporting and sustainability problems.\nWhy connect to your accounting software? The accounting system already records all those economic events that may have an impact on how your company, directly or indirectly, causes greenhouse gas emissions, uses precious water, or may help to close (or, accidentally, widen) the gender pay gap.\nAs soon as the modification of the EU accounting directive take effect, you will have to connect your financial accounts to your sustainability reporting.\nNo extra management time is needed: it is already recorded by every company’s accountant. The general ledger is recorded by your accountant. We start from the annual summary of supplier and buyer ledgers, or the trial balance that has this information. It is not subjective. It states exactly what you were spending on. It is more or less standardized across Europe—and almost all countries of the world, with the exception of the U.S. and some other countries. It is easy to reconcile with US GAAP based documents. We need to use the same working document that your accountant uses to maintain an important objectivity criterion: connectivity. This way your annual report will be consistent, if you say in the financial part that you spend 1000 euro on energy, then we will calculate the greenhouse gas emissions based on KWh volume of the the energy that cost you 1000 euros.\n","date":1678442400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678442400,"objectID":"d1234e9052278506a4977270a5ffa3da","permalink":"https://surveyharmonies.eu/talk/sustainability-measurement-and-reporting-for-the-cfo/","publishdate":"2022-02-23T19:00:00+01:00","relpermalink":"/talk/sustainability-measurement-and-reporting-for-the-cfo/","section":"event","summary":"Reprex will organize events on the International Open Data Day. Save the date in your calendar.","tags":["Eviota","ESG","controlling","ECSP"],"title":"Sustainability measurement and reporting for the CFO","type":"event"},{"authors":["Daniel Antal","musicaire"],"categories":["Public talk"],"content":"The Eviota project aims to create sustainability reports connected to the financial accounts of companies, NGOs, and civil society actors. The first phase concentrates on greenhouse gases and air pollutants. We want to create reliable estimates of the carbon and other pollutants footprint of music-related (social) enterprises based on their spending (“connected financial and sustainability reporting”.)\nReporting the impacts of the entire value chain. 16:30: Meet and greet 17:00: Short presentation 17:15: Discussion of your reporting The discussion will be based on the accounting documents sent to us in advance. We will create a first version of a simplified sustainability report about your music or film company (for profit, or non-profit.) we will not share any financial information in the meetup, only visualizations of risk heatmaps, and common reporting and sustainability problems.\nWhy connect to your accounting software? The accounting system already records all those economic events that may have an impact on how your company, directly or indirectly, causes greenhouse gas emissions, uses precious water, or may help to close (or, accidentally, widen) the gender pay gap.\nAs soon as the modification of the EU accounting directive take effect, you will have to connect your financial accounts to your sustainability reporting.\nNo extra management time is needed: it is already recorded by every company’s accountant. The general ledger is recorded by your accountant. We start from the annual summary of supplier and buyer ledgers, or the trial balance that has this information. It is not subjective. It states exactly what you were spending on. It is more or less standardized across Europe—and almost all countries of the world, with the exception of the U.S. and some other countries. It is easy to reconcile with US GAAP based documents. We need to use the same working document that your accountant uses to maintain an important objectivity criterion: connectivity. This way your annual report will be consistent, if you say in the financial part that you spend 1000 euro on energy, then we will calculate the greenhouse gas emissions based on KWh volume of the the energy that cost you 1000 euros.\n","date":1678289400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678289400,"objectID":"bcb2ade38b21d01d14a605cea9ea8dc1","permalink":"https://surveyharmonies.eu/talk/new-trends-in-controlling-the-strategic-and-digital-challenge/","publishdate":"2022-02-23T19:00:00+01:00","relpermalink":"/talk/new-trends-in-controlling-the-strategic-and-digital-challenge/","section":"event","summary":"Reprex will organize events on the International Open Data Day. Save the date in your calendar.","tags":["Eviota","ESG","controlling","ECSP"],"title":"New trends in controlling: the strategic and digital challenge","type":"event"},{"authors":["Daniel Antal"],"categories":null,"content":" Slide navigation Next: ️\u0026gt; or Space | Previous :️\u0026lt; Start: Home | Finish: End Overview: Esc| Speaker notes: S Zoom: Alt + Click️| Fullscreen: F 🖱 Highlighted text: clickable link (to our project page.)\nContents Introduction 2. Our Approach 3. Main Features 4. Connected Financial and Sustainability Reporting 5. Why are we developing it? 6. Is There a Film/TV Industry Version? 7. Questions Introduction Timeline 2019: European Green Deal and Sustainable Finance Disclosures Regulation.\n2020: 2030 EU climate target (-55%); EFRAG to standardize connected financial and sustainability reporting (IFRS, GRI, SASB/IIRC, TCFD, WICI, UN).\n2021: Strategy for financing the transition to a sustainable economy; CSRD announced (changes in EU accounting and audit law); Reprex Eviota concept based on our iotables 0.4.7, which was updated with environmental impact analysis.\n2022: EU Taxonomy (“Climate Delegated Act” see: FAQ). Reprex Music Eviota with MusicAIRE and wins Hague Innovators Award Audience Prize; Horizon Europe Research \u0026amp; Innovation winner.\nFuture Timeline 2023: Corporate Sustainability Reporting Directive (CSRD); first 50,000 companies apply the new rules for the financial year 2024 (report in in 2025.) Investors, banks, insurance companies, granting organizations, corporate donors, even audiences want to see more and more proof of conducting sustainable business.\n2024: CSRD for the first 50,000 companies.\n2025: First mandatory connected reporting for 50,000 large companies and listed SMEs.\n2026: The European Commission plans to extend the EU taxonomy to six environmental objectives and rules for about 50,000 companies by 2026. Many small companies will have to make mandatory disclosures and reporting.\nSustainability Reporting Architecture Our approach Cultural \u0026amp; Creative Sectors Industries Approach Main Features Connected Financial and Sustainability Reporting In 2020 the European Commission requested technical advice, mandating the European Financial Reporting Advisory Group, EFRAG, to undertake preparatory work for the elaboration of possible EU non-financial reporting standards in a revised EU Non-Financial Reporting Directive (i.e., harmonizing financial reporting and audit with connected sustainability reporting) within the framework of the European Green Deal sustainable finance package and the Corporate Social Responsibility Directive.\nOngoing Standardization 🖱 Link: EUROPEAN LAB – Project Task Force on Preparatory Work for the Elaboration of Possible EU Non-Financial Reporting Standards\nWhy Are We Developing Eviota for Music? CSRD/EFRAG Timeline in the EU Is There A Film Industry Version? Yes, it is coming! Ask for a demo on Email | Keybase | LinkedIn.\nQuestions? Ask: Email | Keybase LinkedIn: Daniel Antal | Reprex ","date":1678284000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678288500,"objectID":"c2b3ff04e8ff27d1f96da45b19248f56","permalink":"https://surveyharmonies.eu/slides/2023_sustainability-measurement-reporting-for-cfo/","publishdate":"2023-03-08T15:00:00+01:00","relpermalink":"/slides/2023_sustainability-measurement-reporting-for-cfo/","section":"slides","summary":"ESG","tags":["Green Deal Music Observatory","Eviota","MusicAIRE"],"title":"Sustainability measurement and reporting for the CFO - the case of the music and film industries","type":"slides"},{"authors":["SurveyHarmonies","music","competition","greendeal","ccsi"],"categories":["Public talk"],"content":" Reprex is a finalist for The Hague Innovators Award 2022, and the prize of the audience, in the startup category with our respectable competitors, Sibö, WECO, STHRIVE and ECOBLOQ. Reprex will organize events on the International Open Data Day. Save the date in your calendar.\n","date":1678017600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678017600,"objectID":"07279d567263e18a0f54bb080cc2ebf3","permalink":"https://surveyharmonies.eu/talk/open-data-day-in-the-hague-2023/","publishdate":"2022-11-13T19:00:00+01:00","relpermalink":"/talk/open-data-day-in-the-hague-2023/","section":"event","summary":"Reprex will organize events on the International Open Data Day. Save the date in your calendar.","tags":["Listen Local","OpenMusE"],"title":"Open Data Day in The Hague 2023","type":"event"},{"authors":["Daniel Antal"],"categories":[],"content":" Slide navigation Next: ️\u0026gt; or Space | Previous :️\u0026lt; Start: Home | Finish: End Overview: Esc| Speaker notes: S Zoom: Alt + Click️| Fullscreen: F 🖱 Highlighted text: clickable link (to our project page.)\nJump directly to the work packages:\nWP Project Coordination WP Sustainability WP Survey Recyclying WP Heritage Reuse WP Big Data \u0026amp; AI That Works for Everyone Or click next for the conceptual overview »\nShort Conceptual Video WP Project Coordination 1. Reprex aims to coordinate this project with a dedicated, experienced Creative Europe project manager who is familiar with the music and audiovisual creation.\n2. Because of the project size and subsidy rate (60%) every partner must contribute to financing WP Project Coordination in proportion to their effective subsidies.\nWP Sustainability Extend the usability of Eviota (a Scope 2-3 music ESG reporting tool) and GreenEyes (Scope 1 film ESG reporting tool) to work with music, film, books, and publishing.\nIntroduce these new methods to business school and vocational training courses.\nGet regulatory approval to use in the context of the green finance package.\nWP Survey Recycling Utilize the survey harmonization and recycling tools developed in music to be available for film, television, photography, and book publishing users.\nExtend harmonized question banks, and remove interoperability and semantic barriers to reuse data from previous audience and policy surveys for these CCSIs.\nSubsidize the initial costs in use to transfer from non-reusable market research to reusable, collaborative market research.\nWP Heritage Reuse Utilize the best practices from music archives, film archives, and private photography archives to remove copyright law, ethical, interoperability, and semantic barriers to mix archival/heritage and new content in music, film, and photography (including in books.)\nCreate a special-purpose record label and audiovisual distributor that specializes in difficult, out-of-commerce/archival/commercial derivative work distribution on global streaming platforms.\nBuild a best practice for the legal and ethical public performance of derivative works that are made from archival/heritage (which were not deposited with the intent to be commercially used or widely circulated) and new creations.\nWP AI \u0026amp; Big Data That Works For Everyone Utilize the best practices from data feminism, music, and film distribution of niche, marginalized content, and create a general framework that reduces inequalities created by lower data representation (of women, small countries, small languages) that result in new injustice on algorithmic platforms. See publication.\nBuild new music, film, and photography information services that reduce the data imbalances, and make small language (for example, Estonian) or historically underrepresented (for example, female artists) more likely to be recommended by AI-driven algorithmic platforms such as YouTube, Spotify, or Amazon.\nCreate open source solutions and open knowledge that enables small, independent music, documentary, photography, and book publishers to build audiences, and monetize content with big data and AI algorithms that work for everyone, not only male artists, English language content, and mainstream works.\nQuestions? Ask: Email | Keybase LinkedIn: Daniel Antal | Reprex ","date":1677777120,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678465080,"objectID":"2d31ae7a01a89ba31e1fc4a5a261a2d4","permalink":"https://surveyharmonies.eu/slides/crea-innovlab-2023/","publishdate":"2023-03-02T18:12:00+01:00","relpermalink":"/slides/crea-innovlab-2023/","section":"slides","summary":"CREA-CROSS-2023-INNOVLAB Pitch","tags":["innovation"],"title":"Data Observatory Labs","type":"slides"},{"authors":["SurveyHarmonies"],"categories":null,"content":" Check out our slides. We are still looking for certain partners. We want to build a network of Innovation Labs, connecting labs and businesses that bring these novel scientific and innovation results nearer to civil society actors, individual creators, and microenterprises in services. We bring data-, sustainability-, rights management innovation, and novel distribution models nearer to the grassroots level of creation. We want to transform scientific and technical development into business development available for small creative organizations without a data engineering/science function.\nTable of Contents WP 1 Coordination WP Survey Recycling WP Heritage Reuse WP AI \u0026amp; Big Data That Works For Everyone Potential partners In 90 secs: conceptual introduction WP 1 Coordination Reprex aims to coordinate this project with a dedicated, experienced Creative Europe project manager who is familiar with the music and audiovisual creation.\nBecause of the project size and subsidy rate (60%) every partner must contribute to financing WP Project Coordination in proportion to their effective subsidies.\nWP Survey Recycling Utilize the survey harmonization and recycling tools developed in music to be available for film, television, photography, and book publishing users.\nExtend harmonized question banks, and remove interoperability and semantic barriers to reuse data from previous audience and policy surveys for these CCSIs.\nSubsidize the initial costs in use to transfer from non-reusable market research to reusable, collaborative market research.\nWP Heritage Reuse Utilize the best practices from music archives, film archives, and private photography archives to remove copyright law, ethical, interoperability, and semantic barriers to mix archival/heritage and new content in music, film, and photography (including in books.)\nCreate a special-purpose record label and audiovisual distributor that specializes in difficult, out-of-commerce/archival/commercial derivative work distribution on global streaming platforms.\nBuild a best practice for the legal and ethical public performance of derivative works that are made from archival/heritage (which were not deposited with the intent to be commercially used or widely circulated) and new creations.\nWP AI \u0026amp; Big Data That Works For Everyone Utilize the best practices from data feminism, music, and film distribution of niche, marginalized content, and create a general framework that reduces inequalities created by lower data representation (of women, small countries, small languages) that result in new injustice on algorithmic platforms. See publication.\nBuild new music, film, and photography information services that reduce the data imbalances, and make small language (for example, Estonian) or historically underrepresented (for example, female artists) more likely to be recommended by AI-driven algorithmic platforms such as YouTube, Spotify, or Amazon.\nCreate open source solutions and open knowledge that enables small, independent music, documentary, photography, and book publishers to build audiences, and monetize content with big data and AI algorithms that work for everyone, not only male artists, English language content, and mainstream works.\nPotential partners Creative enterprises with good YouTube (+Vimeo and other) distribution and rights management track record. Market research, particularly survey based in film and television. Services for private collections/collectors. Who are we? Digital Music Observatory CCSI Data Observatory Green Deal Observatory In 90 secs: conceptual introduction ⚙️/ subtitles/ 🇳🇱 🇬🇧 🇧🇦 🇨🇿 🇭🇺 🇩🇪 🇱🇹 🇫🇷 🇸🇰 🇪🇸 🇹🇷 + Catalan.\n","date":1677760800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678465080,"objectID":"c305b6fa70150f2de4e86996250ed2a3","permalink":"https://surveyharmonies.eu/project/crea-innovlab-2023/","publishdate":"2023-03-02T13:40:00+01:00","relpermalink":"/project/crea-innovlab-2023/","section":"project","summary":"We bring data-, sustainability-, rights management innovation, and novel distribution models nearer to the grassroots level of creation.","tags":["Data Observatory Labs","Innovation","Eviota","Survey harmonization","music","film"],"title":"Data Observatory Labs","type":"project"},{"authors":["Daniel Antal"],"categories":null,"content":" Slide navigation Next: ️\u0026gt; or Space | Previous :️\u0026lt; Start: Home | Finish: End Overview: Esc| Speaker notes: S Zoom: Alt + Click️| Fullscreen: F 🖱 Highlighted text: clickable link (to our project page.)\nContents Introduction 2. Our Approach 3. Main Features 4. Connected Financial and Sustainability Reporting 5. Why are we developing it? 6. Is There a Film/TV Industry Version? 7. Questions Introduction Sustainability Reporting Architecture Our approach Main Features Connected Financial and Sustainability Reporting In 2020 the European Commission requested technical advice, mandating the European Financial Reporting Advisory Group, EFRAG, to undertake preparatory work for the elaboration of possible EU non-financial reporting standards in a revised EU Non-Financial Reporting Directive (i.e., harmonizing financial reporting and audit with connected sustainability reporting) within the framework of the European Green Deal sustainable finance package and the Corporate Social Responsibility Directive.\nOngoing Standardization 🖱 Link: EUROPEAN LAB – Project Task Force on Preparatory Work for the Elaboration of Possible EU Non-Financial Reporting Standards\nWhy Are We Developing Eviota for Music? CSRD/EFRAG Timeline in the EU Is There A Film Industry Version? Yes, it is coming! Ask for a demo on Email | Keybase | LinkedIn.\nQuestions? Ask: Email | Keybase LinkedIn: Daniel Antal | Reprex ","date":1677575580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677575580,"objectID":"426eac5643933ef21e237d432c9826a9","permalink":"https://surveyharmonies.eu/slides/2023_controlling-strategic-digital-challenge/","publishdate":"2023-02-28T11:13:00+02:00","relpermalink":"/slides/2023_controlling-strategic-digital-challenge/","section":"slides","summary":"ESG","tags":["Green Deal Music Observatory","Eviota","MusicAIRE"],"title":"Music Eviota","type":"slides"},{"authors":["SurveyHarmonies"],"categories":["Public talk"],"content":" Table of Contents Green Deal, this time for real Beta test our sustainabilitiy reporting tool Digital Music Observatory: survive in the music business without a data engineer Harmonizing surveys on gender (in)equaliaty Green Deal, this time for real Daniel Antal, co-founder of Reprex will participate in the panel about on Thursday, November the 24th at 3.15pm about environment and sustainability in the music industry and introduce Eviota, our simplified, connected financial and sustainability reporting tool.\nAt LineCheck X, you can find out more about our projects\nYou can get a free, total value chain based sustainability report. (Get in touch with us before 18 November) We will demonstrate how you can eradicate packaging waste from your music events by making the packaging edible. And you can find out how we are trying to find where energy is going down the sewage, literally, with Thermowatt and the Green Deal Data Observatory. Beta test our sustainabilitiy reporting tool We are looking for beta testers for our simplified sustainability report that is made from your trial balance. We sign and NDA about your accounting data if you test with us, and you can test with older data, too. Beta testing is free. Our solution benefits the music MSMEs and CSOs in several ways:\nThe European Commission estimates that the cost of connecting sustainability and finanical reporting will cost on average €10,000 for corporations. We want to bring down the voluntary reporting costs for MSMEs below €500 euro to benefit from green loans, green insurance and other green financing. It provides them with a size adequate sustainability management and reporting tool that helps first the management of greenhouse gas emissions, and later sustainable water use, pollutions, biodiversity, and recycling in their entire value chain (for example, it flags environmental risks in the supply base of a festival including equipment rentals, transport, security firms, catering facilities, etc.) by connecting standard accounting documents of the MSME with SNA and EEA science based benchmarks. Our system will be extendible to management of social sustainability. We are also showcasing harmonized gender inequality data collection with our other project (see below.) Digital Music Observatory: survive in the music business without a data engineer ⚙️/ subtitles/ 🇳🇱 🇬🇧 🇧🇦 🇨🇿 🇭🇺 🇩🇪 🇱🇹 🇫🇷 🇸🇰 🇪🇸 🇹🇷 + Catalan. If you are there, please leave a 👍, too :)\nWe will introduce Surveyharmonies, a survey recycling and survey harmonization tool. You can participate in our international gender bias survey in January 2023. Our team will also participate in the KeyChange Creative Lab on Friday, 25 November at 11.00. You can learn about our Listen Local project, which helps local music ecosystems remain visible on global platforms. We will offer you a cup of tea or coffee in an edible cup. Harmonizing surveys on gender (in)equaliaty You can create better surveys with less cost: you only need to ask the information, or change of information, that is not included in our harmonized datasets. Shorter, better questionnaires, smaller samples sizes, huge cost savings.\nWhen you make questionnaire-based research, you immediately get a history (the same question asked years ago) and an international comparison (the same question asked in other countries.)\nOften you do not even have to pay for the survey, because somebody else has already made a similar taxpayer funded research and we can just get the data for you.\nSurvey harmonization is a powerful research tool to increase the usability of questionnaire-based empirical research. Read more on our blog. ","date":1669302900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669302900,"objectID":"fb6efba3c8cb5bc28bbe86b26a41b8e8","permalink":"https://surveyharmonies.eu/talk/linecheck-milano-it/","publishdate":"2022-09-13T10:20:00+02:00","relpermalink":"/talk/linecheck-milano-it/","section":"event","summary":"Reprex, Edibles, Thermowatt at the Digital Music Observatory and the Green Deal Data Observatory in Linecheck X.","tags":["Survey recycling","Gender biases","Eviota","Sustainability reporting","Zero waste","Edibles"],"title":"LineCheck, Milano, IT","type":"event"},{"authors":[],"categories":["Public talk"],"content":" Reprex is a finalist for The Hague Innovators Award 2022, and the prize of the audience, in the startup category with our respectable competitors, Sibö, WECO, STHRIVE and ECOBLOQ. The transition towards a sustainable and inclusive economy depends on collaboration. That is why we are bringing together startups, scale-ups, investors, policymakers, and other impact makers from around the world in The Hague for the 7th edition of ImpactFest.\nPlease share our video message on YouTube among your colleagues and friends.\nWith the The Hague Innovators Challenge, the municipality of The Hague challenges startups, scale-ups, and students to present their innovative ideas for global issues, as described in the UN Sustainable Development Goals (SDGs).\n💜 We ask you, humbly, to support us with a vote or by sharing our appeal. We\u0026#39;re part of the #opensource, #opendata, and #openscience movement that depends on your support to stay online and thriving, but many of our users or simply look the other way🤦🏻‍♀️https://t.co/Qcdh7saPpW\n— Competition Data Observatory (@CompDataObs) November 6, 2022 The nominees receive a substantive program aimed at further development and the growth of the plan or organization. At the end of the substantive program, all nominees submit a definitive action plan and this is pitched to a professional jury. The jury chooses one winner per category.\n","date":1668502800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668502800,"objectID":"595c7f31c80fc87fcc0c11cd8bc1f5a8","permalink":"https://surveyharmonies.eu/talk/reprex-nominated-for-the-hague-innovators-award/","publishdate":"2022-09-13T10:20:00+02:00","relpermalink":"/talk/reprex-nominated-for-the-hague-innovators-award/","section":"event","summary":"Reprex is a finalist for The Hague Innovators Award 2022, and the prize of the audience, in the startup category with our respectable competitors, Sibö, WECO, STHRIVE and ECOBLOQ.","tags":["Listen Local","OpenMusE"],"title":"Reprex Nominated for The Hague Innovators Award","type":"event"},{"authors":[],"categories":null,"content":"Reprex’s co-founder, Daniel Antal talked in the Eindhoven Innovation Café about these issues. You can watch the recorded version of the the livestream that starts at 5 minutes and 22 seconds:\nThis is a past event. Check out our forthcoming events or write to Daniel Antal or to antaldaniel. Or send an email.\nThe event invitation text and links Big data and AI creates inequalities. It puts historically marginalized people, like ethnic minorities, and womxn, at a disadvantage. Because AI and checking on AI require plenty of data, usually only giant corporations, the wealthiest governments, and university entities can make it work for them. Reprex is a Hague-based, international startup that wants to impact various sustainable development goals by enabling smaller organizations to join their smaller datasets, use open data, create linked available data, and collaboratively make a change.\nReprex is a finalist for the Hague Innovation Award for impact startup (please 🙏, vote for us!). Daniel Antal, one of the co-founders, will talk about their approach to building an international coalition of music organizations to pool data and challenge data monopolies using organizational techniques, a collaboration ethos, and data from the open-source developer world.\nUsing the example of independent music creators, who often find themselves in a position where it is more expensive to claim their money from global platforms, he will talk about how to reduce inequalities in the world of big data and AI with collaboration on web 3.0. In the Q\u0026amp;A he will take questions on how to apply their know-how, and generally linked open data to other art+tech or creative segments or problems for which everybody is too small, like meeting the Paris Accord greenhouse gas targets bit by bit, small company by small company.\nIn the Q\u0026amp;A, we can discuss many things How can Reprex help an individual creator in music, or in fashion and design, or any other area? What sort of help it can give to researchers, research institutes, specialist consultancies, law firms, and other knowledge-based actors? What sort of partners is Reprex looking for in Eindhoven?\nCheck out our projects Digital Music Observatory and Listen Local Cultural \u0026amp; Creative Sectors and Industries Observatory and short call for potential partners. Green Deal Data Observatory and simple, connected, financial and sustainability reporting for creative enterprises and others Reprex: the impact startup Check out our accomplishments since the foundation in 2020 ","date":1667496600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667496600,"objectID":"4c7a8d9aee3b2455bdc1c477c498ab54","permalink":"https://surveyharmonies.eu/talk/big-data-for-all-building-collaborative-data-observatories/","publishdate":"2022-08-10T12:20:00+02:00","relpermalink":"/talk/big-data-for-all-building-collaborative-data-observatories/","section":"event","summary":"Introducing the collaborative data observatories with the example of the music industry and sector.","tags":["Eindhoven","ImpactCity","Reprex","Digital Music Observatory","Open Data"],"title":"Big Data for All: Building Collaborative Data Observatories","type":"event"},{"authors":["Daniel Antal"],"categories":[],"content":"Big Data Creates Inequalities Only the largest corporations, best-endowed universities, and rich governments can afford data collection and processing capacities that are large enough to harness the advantages of AI.\nSlide navigation Fullscreen: F\nNext: ️\u0026gt; or Space | Previous :️\u0026lt; Start: Home | Finish: End Overview: Esc| Speaker notes: S Zoom: Alt + Click 🖱️ Big data that works for all Reprex: No matter how big is the problem or how small is your team, we fill your reports, dashboards, newsletters, books with data and its visualization.\nConnected financial and sustainability reporting based on open data Eviota: We map your material impacts in your value chain and connect it with environmental or social data that is re-used from the public sector.\nData problems: Reprex Most SMEs, and civil society organizations do not have a data scientist/engineer in their team, maybe not even an IT person or a HR professional to make such a hire.","date":1663768800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664874780,"objectID":"7a3e79e5e5924eb9ecbd20c01f5aabcb","permalink":"https://surveyharmonies.eu/slides/reprex-esg-pitch/","publishdate":"2022-09-21T16:00:00+02:00","relpermalink":"/slides/reprex-esg-pitch/","section":"slides","summary":"ESG","tags":["Green Deal Music Observatory","Eviota"],"title":"Reprex","type":"slides"},{"authors":null,"categories":null,"content":"Stop uploading your work onto the web 2.0 in a pdf, epub, or word file. Build a self-refreshing resource that re-refreshes the statistics, legal texts, tables, visualizations, footnotes and bibliography instead. This way you’ll have a greater impact: you’ll connect to global knowledge graphs, and your work can be reused much better.\nOur “smart document” is always live: it contains code that searches for data updates or changes in the law. It is a document that includes reproducible research code and makes sure that your document contains the latest information. It is a resource that, when uploaded to the web 3.0, cannot only be downloaded, but finds itself its audience, places itself into global libraries, data exchanges, automated websites.\n📊 Downloads or updates reliable statistical data, utilizing the SDMX standard, for example, Eurostat datasets, which is placed into your text as a table, a data visualization, and its source as a footnote and bibliographic citation.\n⚖️ Updates legal citations from Eur-LEX, i.e. contains the latest form of policy or legal documents, and flags for the researchers any important changes in the lifecycle of the citation (legal text goes out of force, amended, important court decisions get connected).\n🎨 Exchange cultural Digital Objects with Europeana or other cultural heritage or knowledge organizations, such as out-of-print books, music works, or 3D design objects.\nWe admit that at this point we mainly serve policy wonks or scientists who are very tech savvy. But we are working hard to package it into a highly usable end-user product.\nWhat is reproducible research? Reproducible research is what it is: it can be reproduced. Easier said than done. We create open source software that accompanies the entire research workflow, from the fieldwork or big data collector, downloading documents via the analysis, visualizations, citations, web dissemination and publication. We do all those little steps that computers do better than humans: logging, documenting, testing, validating, archiving. This allows our users to do what humans do best: think.\nOur data observatories are open scholarly platforms that support reproducible research. Our policy documents bring this functionality to your personal computer, and make it available for an NGO, a lawyer, a consultant, or an individual researcher.\nFeature list Each external resource, i.e. a policy document, a legal text, a cultural heritage object, a catalog entry, a dataset is clearly identified and downloaded, processed for the document into a footnote, citation, table or standard visualization.\nNew artifacts, such as visualizations, tables, receive a unique document object identifier (DOI) that clearly states their source, the person who oversaw the creation, the date, and the version.\nThese artifacts are added into the text in pre-defined places, such as the “Chart on GDP growth” placeholder containing the latest chart on GDP growth, while the citation in the bibliography contains the new version of the artifact (i.e. the chart with a DOI.)\nThe artifacts, such as datasets, tables, visualizations, codebooks, reference lists, are uploaded with a new version to an open science repository such as FigShare or Zenodo. This ensures that the creator’s intellectual rights are respected, and different, unauthorized versions of the table, chart, or other artifact in unknown news outlets, social media, are not connected to the creator or publisher.\nZenodo, via OpenAIRE, connects your work with global libraries, and if they meet the quality criteria, they are often immediately placed into the catalogues of global libraries. Our smart policy documents are not only uploaded onto the web, but connect to global knowledge graphs, or the web 3.0.\nThe author and publisher of the ‘smart policy document’ receives notifications of significant changes, i.e. non-trivial new data at Eurostat, or non-trivial connecting policy documents, court decisions, which should trigger a revision of the smart policy document’s textual contents. Needless to say, behind the scenes, we handle those trivial changes, too.\nAfter human review of the new version, it is created as Word docx, EPUB, and PDF file, and with a new version and DOI, it is uploaded to open science repositories that synchronize this publication with global library systems.\n","date":1662249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662249600,"objectID":"648c4b0e3991171327d838573f8680cc","permalink":"https://surveyharmonies.eu/apps/smart-policy-documents/","publishdate":"2022-09-04T00:00:00Z","relpermalink":"/apps/smart-policy-documents/","section":"apps","summary":"Stop uploading your work onto the web 2.0 in a pdf, epub, or word file. Build a self-refreshing resource that re-refreshes the statistics, legal texts, tables, visualizations, footnotes and bibliography instead.  This way you’ll have a greater impact: you’ll connect to web 3.0 of the global knowledge graphs, and your work can be reused much better.","tags":["Smart Policy Documents"],"title":"Smart Policy Documents","type":"apps"},{"authors":null,"categories":null,"content":"Stop uploading your work onto the web 2.0 in a pdf, epub, or word file. Build a self-refreshing resource that re-refreshes the statistics, legal texts, tables, visualizations, footnotes and bibliography instead. This way you’ll have a greater impact: you’ll connect to global knowledge graphs, and your work can be reused much better.\nOur “smart document” is always live: it contains code that searches for data updates or changes in the law. It is a document that includes reproducible research code and makes sure that your document contains the latest information. It is a resource that, when uploaded to the web 3.0, cannot only be downloaded, but finds itself its audience, places itself into global libraries, data exchanges, automated websites.\n📊 Downloads or updates reliable statistical data, utilizing the SDMX standard, for example, Eurostat datasets, which is placed into your text as a table, a data visualization, and its source as a footnote and bibliographic citation.\n⚖️ Updates legal citations from Eur-LEX, i.e. contains the latest form of policy or legal documents, and flags for the researchers any important changes in the lifecycle of the citation (legal text goes out of force, amended, important court decisions get connected).\n🎨 Exchange cultural Digital Objects with Europeana or other cultural heritage or knowledge organizations, such as out-of-print books, music works, or 3D design objects.\nWe admit that at this point we mainly serve policy wonks or scientists who are very tech savvy. But we are working hard to package it into a highly usable end-user product.\nWhat is reproducible research? Reproducible research is what it is: it can be reproduced. Easier said than done. We create open source software that accompanies the entire research workflow, from the fieldwork or big data collector, downloading documents via the analysis, visualizations, citations, web dissemination and publication. We do all those little steps that computers do better than humans: logging, documenting, testing, validating, archiving. This allows our users to do what humans do best: think.\nOur data observatories are open scholarly platforms that support reproducible research. Our policy documents bring this functionality to your personal computer, and make it available for an NGO, a lawyer, a consultant, or an individual researcher.\nFeature list Each external resource, i.e. a policy document, a legal text, a cultural heritage object, a catalog entry, a dataset is clearly identified and downloaded, processed for the document into a footnote, citation, table or standard visualization.\nNew artifacts, such as visualizations, tables, receive a unique document object identifier (DOI) that clearly states their source, the person who oversaw the creation, the date, and the version.\nThese artifacts are added into the text in pre-defined places, such as the “Chart on GDP growth” placeholder containing the latest chart on GDP growth, while the citation in the bibliography contains the new version of the artifact (i.e. the chart with a DOI.)\nThe artifacts, such as datasets, tables, visualizations, codebooks, reference lists, are uploaded with a new version to an open science repository such as FigShare or Zenodo. This ensures that the creator’s intellectual rights are respected, and different, unauthorized versions of the table, chart, or other artifact in unknown news outlets, social media, are not connected to the creator or publisher.\nZenodo, via OpenAIRE, connects your work with global libraries, and if they meet the quality criteria, they are often immediately placed into the catalogues of global libraries. Our smart policy documents are not only uploaded onto the web, but connect to global knowledge graphs, or the web 3.0.\nThe author and publisher of the ‘smart policy document’ receives notifications of significant changes, i.e. non-trivial new data at Eurostat, or non-trivial connecting policy documents, court decisions, which should trigger a revision of the smart policy document’s textual contents. Needless to say, behind the scenes, we handle those trivial changes, too.\nAfter human review of the new version, it is created as Word docx, EPUB, and PDF file, and with a new version and DOI, it is uploaded to open science repositories that synchronize this publication with global library systems.\n","date":1662249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662249600,"objectID":"73d1b3231759a00fb3e1027dd2399cac","permalink":"https://surveyharmonies.eu/apps/smart-reporting-resources/","publishdate":"2022-09-04T00:00:00Z","relpermalink":"/apps/smart-reporting-resources/","section":"apps","summary":"Stop uploading your work onto the web 2.0 in a pdf, epub, or word file. Build a self-refreshing resource that re-refreshes the statistics, legal texts, tables, visualizations, footnotes and bibliography instead.  This way you’ll have a greater impact: you’ll connect to web 3.0 of the global knowledge graphs, and your work can be reused much better.","tags":["Smart Reporting Resources"],"title":"Smart Reporting Resources","type":"apps"},{"authors":null,"categories":null,"content":"We are building and ecosystem of open data, open software and trustworthy algorithms around our data observatories. We collaborate with scientific software developers, and their communities, such as rOpenGov. Our software tools have many thousand users, but they require coding skills. We will soon start to deploy more user-friendly applications that can be used by sustainability reporting professionals, or lawyers preparing factual green disclosures.\nOur Eviota project, which builds a multi-language reporting interface for preventing greenwashing. It will have a user-frontend for our environmental (and economic) impact assessment software tool, iotables. The iotables R package implements most of the functionality laid out in the economic and environmental input-output analysis features in the Eurostat and the respective UN statistical technical guidance on the topic.\nOur smart document are always alive: they contain code that searches for data updates or changes in the law. It is a document that includes reproducible research code and makes sure that your document contains the latest information. It is a resource that, when uploaded to the web 3.0, cannot only be downloaded, but finds itself its audience, places itself into global libraries, data exchanges, automated websites.\n","date":1661860320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662249600,"objectID":"2f7b02f7e78d7d2d235bae3c09f9934f","permalink":"https://surveyharmonies.eu/apps/apps/","publishdate":"2022-08-30T13:52:00+02:00","relpermalink":"/apps/apps/","section":"apps","summary":"We are building and ecosystem of open data, open software and trustworthy algorithms around our data observatories. We collaborate with scientific software developers, and their communities, such as rOpenGov. Our software tools have many thousand users, but they require coding skills.","tags":["Apps"],"title":"Application Development","type":"apps"},{"authors":null,"categories":null,"content":" Reporting the impacts of the entire value chain. Our minimum viable product will create sustainability reports (or report components) for greenhouse gases and sustainable water use with applying the Global GHG Accounting \u0026amp; Reporting Standard for the Financial Industry and EFRAG’s proposed concept on connecting European accounting standards and information with sustainability. We will help small music organizations in their sustainability reporting, where detail data and reporting standards are only available for greenhouse gas emissions. The Music Eviota project is supported by the MusicAIRE.\nOpen collaboration Our project is based on open collaboration. Our proposal, if funded, will provide us with resources to supply further music businesses, music civil society organizations and researchers with high-quality data (during the duration of the project for free.) We are already looking for interested parties to put our data and research projects into use and validate their usability and quality in real-life policy or business development scenarios.\nWhy are we developing this service? The European Green Deal, which includes the proposed Corporate Sustainability Reporting Directive, and the sustainable finance package, aims to set the European economy on a permanent decarbonization and sustainability increasing path with adjusting the rules how economic activities are financed by bank loans, insurance, investments, and direct subsidies. From 2023, it will be cheaper to get loans, insurance, and other types of funding for organizations that can prove that they follow the environmental, social and governance path set out in the Paris Agreement and other UN, OECD, and EU agreements.\nRequirements for connecting financial and sustainability reporting. Correct and reliable sustainability management will come with many financial advantages and increased responsibility. The European Financial Reporting Advisory Board is currently preparing the new combined financial and sustainability reporting standard that will be used in banks, insurance, investment, granting, and the large companies of Europe in their entire supply and purchaser chain. The European Commission estimates that compliance costs until the end of 2023 will amount to 4 billion euros, with reporting and auditing costs mounting 10,000 euros per organization. While music small and medium sized organizations (MSMEs) and limited liability civil society organizations (CSOs) will be exempted from mandatory sustainability management and audited reporting, they can still comply in a non-audited and voluntary way.\nOur solution benefits the music MSMEs and CSOs in several ways:\nIt provides them with a size adequate sustainability management and reporting tool that helps first the management of greenhouse gas emissions, and later sustainable water use, pollutions, biodiversity, and recycling in their entire value chain (for example, it flags environmental risks in the supply base of a festival including equipment rentals, transport, security firms, catering facilities, etc.) by connecting standard accounting documents of the MSME with SNA and EEA science based benchmarks. Our system will be extendible to management of social sustainability. Our previous research shows that particularly the live music industry that needs a large workforce, suffers from underuse of, and discrimination of female workers in various technical and even managerial roles. Our system will be able to flag risks of gender paygap and related issues in the entire value chain and of course, provide good benchmarks for internal activities. Our review of the environmental, social and governance risk management (ESG sustainability management) suggests that complying with ESG standards is not only a pre-requisite to get cheaper loans (less important) and cheaper insurance (very important in live music), but also a requirement by corporate sponsors of events, and even a large part of the audience. While some music organizations already provide sustainability reporting, they are not standardized and are less factful as they are not connected to accounting information at every point. Our solution aims to give much credibility to both the sustainability reports and non-financial disclosures of the financial reports (which are not mandatory for MSMEs but increase their trustworthiness on an elective basis if they are included.)\nGrowing interest for ESG in select countries. ","date":1661860320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662298740,"objectID":"678c7f4295bd07441d4fe712f17548bb","permalink":"https://surveyharmonies.eu/apps/eviota/","publishdate":"2022-08-30T13:52:00+02:00","relpermalink":"/apps/eviota/","section":"apps","summary":"Connected financial and sustainability reporting","tags":["Eviota"],"title":"Eviota","type":"apps"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1661740320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661740320,"objectID":"7daf5ccb3f40a74c7328bd7b91e143ae","permalink":"https://surveyharmonies.eu/project/greenwashing/","publishdate":"2022-08-29T04:32:00+02:00","relpermalink":"/project/greenwashing/","section":"project","summary":"Fight greenwashing with reliable evidence.","tags":["Greenwashing"],"title":"Greenwashing","type":"project"},{"authors":["Daniel Antal"],"categories":null,"content":"Interoperable, FAIR datasets The primary aim of dataset is create well-referenced, well-described, interoperable datasets from data.frames, tibbles or data.tables that translate well into the W3C DataSet definition within the Data Cube Vocabulary in a reproducible manner. The data cube model in itself is is originated in the Statistical Data and Metadata eXchange, and it is almost fully harmonzied with the Resource Description Framework (RDF), the standard model for data interchange on the web[^1].\nA mapping of R objects into these models has numerous advantages:\nMakes data importing easier and less error-prone; Leaves plenty of room for documentation automation, resulting in far better reusability and reproducability; The publication of results from R following the FAIR principles is far easier, making the work of the R user more findable, more accessible, more interoperable and more reusable by other users; Makes the placement into relational databases, semantic web applications, archives, repositories possible without time-consuming and costly data wrangling (See From dataset To RDF). Our package functions work with any structured R objects (data.fame, data.table, tibble, or well-structured lists like json), however, the best functionality is achieved by the (See The dataset S3 Class), which is inherited from data.frame().\nContact For contact information, contributors, see the package homepage.\nCode of Conduct Please note that the dataset project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.\nClick the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1660176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660208400,"objectID":"26ed66c19df0a261920b3b3afb0f0370","permalink":"https://surveyharmonies.eu/software/dataset/","publishdate":"2022-08-11T00:00:00Z","relpermalink":"/software/dataset/","section":"software","summary":"Publish your datasets with FAIR metadata in open science repositories and place them on knowledge graphs","tags":["Reproducible research","SDMX","W3C","semantic web"],"title":"dataset: Create Interoperable FAIR Datasets","type":"software"},{"authors":["Daniel Antal","Reprex"],"categories":["R-bloggers"],"content":" Visit the documentation website of statcodelists on statcodelists.dataobservatory.eu/. The goal of statcodelists is to promote the reuse and exchange of statistical information and related metadata with making the internationally standardized SDMX code lists available for the R user. SDMX – the Statistical Data and Metadata eXchange has been published as an ISO International Standard (ISO 17369). The metadata definitions, including the codelists are updated regularly according to the standard. The authoritative version of the code lists made available in this package is https://sdmx.org/?page_id=3215/.\nClick to expand table of contents of the post Table of Contents Purpose Installation Code of Conduct Purpose Cross-domain concepts in the SDMX framework describe concepts relevant to many, if not all, statistical domains. SDMX recommends using these concepts whenever feasible in SDMX structures and messages to promote the reuse and exchange of statistical information and related metadata between organisations.\nCode lists are predefined sets of terms from which some statistical coded concepts take their values. SDMX cross-domain code lists are used to support cross-domain concepts. What are these cross-domain coded concepts?\nGeographical codes, like NL: the Netherlands in the CL_AREA code list. Standard industry codes J631 for Data processing, hosting and related activities in Europe. (NACE Rev 2 in Europe, beware, it is J592in Australia and New Zealand, see CL_ACTIVITY_ANZSIC06.) Occupations, like OC2521 for Database designers and administrators in CL_OCCUPATIONS Time fomatting standards, like CCYY for annual data series in CL_TIME_FORMAT. Check out the available codlists on the package homepage.\nThe use of common code lists will help users to work even more efficiently, easing the maintenance of and reducing the need for mapping systems and interfaces delivering data and metadata to them. A very obvious advantage of using the code systems is that you can retrieve data from national sources indifferent of the natural language used in North Macedonia, Japan, the U.S. or the Netherlands. While the data labels may change to be locally human-readable, computers and geeks can read the codes and understand them immediately. Provided that they use the standard codes.\nOur data observatories are rolling out SDMX coding across all datasets to help data ingestion and interoperability, data findability and data reuse. statcodelists can help the use of standard SDMX codes in your R workflow–both for downloading data from statistical agencies and to produce publication-ready datasets that the rest of the world (and even APIs) will understand.\nInstallation You can install statcodelists from CRAN:\ninstall.packages(\u0026#34;statcodelists\u0026#34;) Further recommended code values for expressing general statistical concepts like not applicable, etc., can be found in section Generic codes of the Guidelines for the creation and management of SDMX Cross-Domain Code Lists.\nFor further codelists used by reliable statistical agency but not harmonized on SDMX level please consult the SDMX Global Registry Codelists page.\nThe creator of this package is not affiliated with SDMX, and this package was has not been endorsed by SDMX.\nCode of Conduct Please note that the statcodelists project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.\n","date":1656486720,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656486720,"objectID":"93e74f22e59182e38c653816a3412e7d","permalink":"https://surveyharmonies.eu/post/2022-06-29-statcodelists/","publishdate":"2022-06-29T08:12:00+01:00","relpermalink":"/post/2022-06-29-statcodelists/","section":"post","summary":"A new building block of our observatories went through code peer review and was released yesterday. The statcodelists R package aim to promote the  reuse and exchange of statistical information and related metadata with making the internationally standardized SDMX code lists available for the R user.","tags":["R","Metadata","SDMX","Codelists"],"title":"stacodelists: use standard, language-independent variable codes to help international data interoperability and machine reuse in R","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":"Retrospective data harmonization The aim of retroharmonize is to provide tools for reproducible retrospective (ex-post) harmonization of datasets that contain variables measuring the same concepts but coded in different ways. Ex-post data harmonization enables better use of existing data and creates new research opportunities. For example, harmonizing data from different countries enables cross-national comparisons, while merging data from different time points makes it possible to track changes over time.\nRetrospective data harmonization is associated with challenges including conceptual issues with establishing equivalence and comparability, practical complications of having to standardize the naming and coding of variables, technical difficulties with merging data stored in different formats, and the need to document a large number of data transformations. The retroharmonize package assists with the latter three components, freeing up the capacity of researchers to focus on the first.\nSpecifically, the retroharmonize package proposes a reproducible workflow, including a new class for storing data together with the harmonized and original metadata, as well as functions for importing data from different formats, harmonizing data and metadata, documenting the harmonization process, and converting between data types. See here for an overview of the functionalities.\nThe new labelled_spss_survey() class is an extension of haven’s labelled_spss class. It not only preserves variable and value labels and the user-defined missing range, but also gives an identifier, for example, the filename or the wave number, to the vector. Additionally, it enables the preservation – as metadata attributes – of the original variable names, labels, and value codes and labels, from the source data, in addition to the harmonized variable names, labels, and value codes and labels. This way, the harmonized data also contain the pre-harmonization record. The stored original metadata can be used for validation and documentation purposes.\nThe vignette Working With The labelled_spss_survey Class provides more information about the labelled_spss_survey() class.\nIn Harmonize Value Labels we discuss the characteristics of the labelled_spss_survey() class and demonstrates the problems that using this class solves.\nWe also provide three extensive case studies illustrating how the retroharmonize package can be used for ex-post harmonization of data from cross-national surveys:\nAfrobarometer Arab Barometer Eurobarometer The creators of retroharmonize are not affiliated with either Afrobarometer, Arab Barometer, Eurobarometer, or the organizations that designs, produces or archives their surveys.\nWe started building an experimental APIs data is running retroharmonize regularly and improving known statistical data sources. See: Digital Music Observatory, Green Deal Data Observatory, Economy Data Observatory.\nCitations and related work Citing the data sources Our package has been tested on three harmonized survey’s microdata. Because retroharmonize is not affiliated with any of these data sources, to replicate our tutorials or work with the data, you have download the data files from these sources, and you have to cite those sources in your work.\nAfrobarometer data: Cite Afrobarometer Arab Barometer data: cite Arab Barometer. Eurobarometer data: The Eurobarometer data Eurobarometer raw data and related documentation (questionnaires, codebooks, etc.) are made available by GESIS, ICPSR and through the Social Science Data Archive networks. You should cite your source, in our examples, we rely on the GESIS data files.\nCiting the retroharmonize R package For main developer and contributors, see the package homepage.\nThis work can be freely used, modified and distributed under the GPL-3 license:\ncitation(\u0026#34;retroharmonize\u0026#34;) #\u0026gt; #\u0026gt; To cite package \u0026#39;retroharmonize\u0026#39; in publications use: #\u0026gt; #\u0026gt; Daniel Antal (2021). retroharmonize: Ex Post Survey Data #\u0026gt; Harmonization. R package version 0.1.17. #\u0026gt; https://retroharmonize.dataobservatory.eu/ #\u0026gt; #\u0026gt; A BibTeX entry for LaTeX users is #\u0026gt; #\u0026gt; @Manual{, #\u0026gt; title = {retroharmonize: Ex Post Survey Data Harmonization}, #\u0026gt; author = {Daniel Antal}, #\u0026gt; year = {2021}, #\u0026gt; doi = {10.5281/zenodo.5006056}, #\u0026gt; note = {R package version 0.1.17}, #\u0026gt; url = {https://retroharmonize.dataobservatory.eu/}, #\u0026gt; } Contact For contact information, contributors, see the package homepage.\nCode of Conduct Please note that the retroharmonize project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.\nClick the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1656374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660201200,"objectID":"cfc0014323d64a4fbf879405539ea261","permalink":"https://surveyharmonies.eu/software/statcodelists/","publishdate":"2022-06-28T00:00:00Z","relpermalink":"/software/statcodelists/","section":"software","summary":"The goal of retroharmonize is to facilitate retrospective (ex-post) harmonization of data, particularly survey data, in a reproducible manner.","tags":["statcodelists"],"title":"statcodelists: Use Standardized Statistical Codelists","type":"software"},{"authors":["SurveyHarmonies","musicaire"],"categories":null,"content":" Reporting the impacts of the entire value chain. The Eviota project aims to create sustainability reports connected to the financial accounts of companies, NGOs, and civil society actors. The first phase concentrates on greenhouse gases and air pollutants. We want to create reliable estimates of the carbon and other pollutants footprint of music-related (social) enterprises based on their spending (“connected financial and sustainability reporting”.)\nCreating connected sustainability reports has many advantages.\nIt shows consumers, donors, and buyers that the company cares for sustainable growth. The organization can ask for grants related to increasing sustainability. In the EU, the company will be eligible for cheaper green loans, green insurance, and green investments. Large corporations, including music event donors, may require their supply chain to produce credible sustainability metrics. There are some difficulties that we want to overcome in this project.\nConnected financial and sustainability reports are complex, they require plenty of data, and mandatory only for ’large’ corporations. Just like small companies can make ‘simplified tax returns’ and ‘simplified financial reports’, we aim to create a less complex and cheap simplified, connected financial and sustainability report. Due to the high complexity, they take a long time to create and are costly: the European Commission estimates their total cost at €10,000. In Europe, there are a handful of large organizations present in the music industry. So, there is no regulatory push for music enterprises to engage in sustainability reporting. However, this means that they cannot benefit from the advantages above.\nTable of Contents Our approach How does it work? What is the report? Methodology Open collaboration Why are we developing this service? Future plans: Social Sustainability and Anti-Bribary MusicAIRE Green Recovery in the Music Sector Our approach Most sustainability calculators are very complex because they use many data inputs from the company. Our mission is to reduce the complexity; however, this would require plenty of experience to define the shortcuts. We will compare all spending (upstream value chain or suppliers) and all income (downstream value chain or buyers) to the know spending of all similar organizations in your country in the comparison year.\nTo reduce the data need, we only take into consideration cost/income groups that meet the financial materiality treshold, i.e. 3% of your total costs or total business-to-business sales. We offer free, manual calculation in the first phase to ensure we define these simplifications well. To reduce the time needed to collect data about your purchases and sales, we will rely on a part of the “trial balance”, because this is available in your accounting system (and can be exported by your accounting software.) The trial balance is an annual summary of the general ledger accounts. We need only the expenses and revenues accounts, and do not need assets, liabilities, gains and losses.\nA part of a fictitious Italian trial balance with Italian and English language labels. The blurred numbers are randomized from an actual trial balance and presented in a different currency than the original. Why the trial balance? We start from a document that every company has, and does not require extra management time to prepare, the so-called trial balance. This is an accounting document that can be obtained from the company’s accountant.\nA trial balance is a report that lists the balances of all general ledger accounts of a company at a certain point in time. The accounts reflected on a trial balance are related to all major accounting items, including assets, liabilities, equity, revenues, expenses, gains, and losses. It is primarily used to identify the balance of debits and credits entries from the transactions recorded in the general ledger at a certain point in time.\nNo extra management time is needed: it is already recorded by every company’s accountant. The general ledger is recorded by your accountant. We do not need the ledger, only the annual account summaries of revenues and expenses. It is not subjective. It states exactly what you were spending on. It is more or less standardized across Europe—and almost all countries of the world, with the exception of the U.S. and some other countries. We need to use the same working document that your accountant uses to maintain an important objectivity criterion: connectivity. This way your annual report will be consistent, if you say in the financial part that you spend 1000 euro on energy, then we will calculate the greenhouse gas emissions based on KWh volume of the the energy that cost you 1000 euros. This way we avoid a lot of data entry into the calculator. At this stage, you we do not offer an uploader, because we want to test manually different trial balances before automating the uploading process.\nHow does it work? In the future, we hope our …","date":1654764000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668933600,"objectID":"d875a740c7a8a9cbf06f05a7aa47a9e2","permalink":"https://surveyharmonies.eu/project/music/","publishdate":"2022-06-09T09:40:00+01:00","relpermalink":"/project/music/","section":"project","summary":"We will help small music organizations in their sustainability reporting, where detail data and reporting standards are only available for greenhouse gas emissions.","tags":["Music Eviota","Eviota","Sustainability","European Green Deal","MusicAIRE"],"title":"Music Eviota","type":"project"},{"authors":["SurveyHarmonies","musicaire"],"categories":null,"content":" Reporting the impacts of the entire value chain. The Eviota project aims to create sustainability reports connected to the financial accounts of companies, NGOs, and civil society actors. The first phase concentrates on greenhouse gases and air pollutants. We want to create reliable estimates of the carbon and other pollutants footprint of music-related (social) enterprises based on their spending (“connected financial and sustainability reporting”.)\nCreating connected sustainability reports has many advantages.\nIt shows consumers, donors, and buyers that the company cares for sustainable growth. The organization can ask for grants related to increasing sustainability. In the EU, the company will be eligible for cheaper green loans, green insurance, and green investments. Large corporations, including music event donors, may require their supply chain to produce credible sustainability metrics. There are some difficulties that we want to overcome in this project.\nConnected financial and sustainability reports are complex, they require plenty of data, and mandatory only for ’large’ corporations. Just like small companies can make ‘simplified tax returns’ and ‘simplified financial reports’, we aim to create a less complex and cheap simplified, connected financial and sustainability report. Due to the high complexity, they take a long time to create and are costly: the European Commission estimates their total cost at €10,000. In Europe, there are a handful of large organizations present in the music industry. So, there is no regulatory push for music enterprises to engage in sustainability reporting. However, this means that they cannot benefit from the advantages above.\nTable of Contents Our approach How does it work? What is the report? Methodology Open collaboration Why are we developing this service? Future plans: Social Sustainability and Anti-Bribary MusicAIRE Green Recovery in the Music Sector Our approach Most sustainability calculators are very complex because they use many data inputs from the company. Our mission is to reduce the complexity; however, this would require plenty of experience to define the shortcuts. We will compare all spending (upstream value chain or suppliers) and all income (downstream value chain or buyers) to the know spending of all similar organizations in your country in the comparison year.\nTo reduce the data need, we only take into consideration cost/income groups that meet the financial materiality treshold, i.e. 3% of your total costs or total business-to-business sales. We offer free, manual calculation in the first phase to ensure we define these simplifications well. To reduce the time needed to collect data about your purchases and sales, we will rely on a part of the “trial balance”, because this is available in your accounting system (and can be exported by your accounting software.) The trial balance is an annual summary of the general ledger accounts. We need only the expenses and revenues accounts, and do not need assets, liabilities, gains and losses.\nA part of a fictitious Italian trial balance with Italian and English language labels. The blurred numbers are randomized from an actual trial balance and presented in a different currency than the original. Why the trial balance? We start from a document that every company has, and does not require extra management time to prepare, the so-called trial balance. This is an accounting document that can be obtained from the company’s accountant.\nA trial balance is a report that lists the balances of all general ledger accounts of a company at a certain point in time. The accounts reflected on a trial balance are related to all major accounting items, including assets, liabilities, equity, revenues, expenses, gains, and losses. It is primarily used to identify the balance of debits and credits entries from the transactions recorded in the general ledger at a certain point in time.\nNo extra management time is needed: it is already recorded by every company’s accountant. The general ledger is recorded by your accountant. We do not need the ledger, only the annual account summaries of revenues and expenses. It is not subjective. It states exactly what you were spending on. It is more or less standardized across Europe—and almost all countries of the world, with the exception of the U.S. and some other countries. We need to use the same working document that your accountant uses to maintain an important objectivity criterion: connectivity. This way your annual report will be consistent, if you say in the financial part that you spend 1000 euro on energy, then we will calculate the greenhouse gas emissions based on KWh volume of the the energy that cost you 1000 euros. This way we avoid a lot of data entry into the calculator. At this stage, you we do not offer an uploader, because we want to test manually different trial balances before automating the uploading process.\nHow does it work? In the future, we hope our …","date":1654764000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668933600,"objectID":"2ab7ff014376ce864c6a2c6bcfc11e1c","permalink":"https://surveyharmonies.eu/sector/film/","publishdate":"2022-06-09T09:40:00+01:00","relpermalink":"/sector/film/","section":"sector","summary":"We will help small music organizations in their sustainability reporting, where detail data and reporting standards are only available for greenhouse gas emissions.","tags":["Music Eviota","Eviota","Sustainability","European Green Deal","MusicAIRE"],"title":"Music Eviota","type":"sector"},{"authors":["SurveyHarmonies","musicaire"],"categories":null,"content":" Reporting the impacts of the entire value chain. The Eviota project aims to create sustainability reports connected to the financial accounts of companies, NGOs, and civil society actors. The first phase concentrates on greenhouse gases and air pollutants. We want to create reliable estimates of the carbon and other pollutants footprint of music-related (social) enterprises based on their spending (“connected financial and sustainability reporting”.)\nCreating connected sustainability reports has many advantages.\nIt shows consumers, donors, and buyers that the company cares for sustainable growth. The organization can ask for grants related to increasing sustainability. In the EU, the company will be eligible for cheaper green loans, green insurance, and green investments. Large corporations, including music event donors, may require their supply chain to produce credible sustainability metrics. There are some difficulties that we want to overcome in this project.\nConnected financial and sustainability reports are complex, they require plenty of data, and mandatory only for ’large’ corporations. Just like small companies can make ‘simplified tax returns’ and ‘simplified financial reports’, we aim to create a less complex and cheap simplified, connected financial and sustainability report. Due to the high complexity, they take a long time to create and are costly: the European Commission estimates their total cost at €10,000. In Europe, there are a handful of large organizations present in the music industry. So, there is no regulatory push for music enterprises to engage in sustainability reporting. However, this means that they cannot benefit from the advantages above.\nTable of Contents Our approach How does it work? What is the report? Methodology Open collaboration Why are we developing this service? Future plans: Social Sustainability and Anti-Bribary MusicAIRE Green Recovery in the Music Sector Our approach Most sustainability calculators are very complex because they use many data inputs from the company. Our mission is to reduce the complexity; however, this would require plenty of experience to define the shortcuts. We will compare all spending (upstream value chain or suppliers) and all income (downstream value chain or buyers) to the know spending of all similar organizations in your country in the comparison year.\nTo reduce the data need, we only take into consideration cost/income groups that meet the financial materiality treshold, i.e. 3% of your total costs or total business-to-business sales. We offer free, manual calculation in the first phase to ensure we define these simplifications well. To reduce the time needed to collect data about your purchases and sales, we will rely on a part of the “trial balance”, because this is available in your accounting system (and can be exported by your accounting software.) The trial balance is an annual summary of the general ledger accounts. We need only the expenses and revenues accounts, and do not need assets, liabilities, gains and losses.\nA part of a fictitious Italian trial balance with Italian and English language labels. The blurred numbers are randomized from an actual trial balance and presented in a different currency than the original. Why the trial balance? We start from a document that every company has, and does not require extra management time to prepare, the so-called trial balance. This is an accounting document that can be obtained from the company’s accountant.\nA trial balance is a report that lists the balances of all general ledger accounts of a company at a certain point in time. The accounts reflected on a trial balance are related to all major accounting items, including assets, liabilities, equity, revenues, expenses, gains, and losses. It is primarily used to identify the balance of debits and credits entries from the transactions recorded in the general ledger at a certain point in time.\nNo extra management time is needed: it is already recorded by every company’s accountant. The general ledger is recorded by your accountant. We do not need the ledger, only the annual account summaries of revenues and expenses. It is not subjective. It states exactly what you were spending on. It is more or less standardized across Europe—and almost all countries of the world, with the exception of the U.S. and some other countries. We need to use the same working document that your accountant uses to maintain an important objectivity criterion: connectivity. This way your annual report will be consistent, if you say in the financial part that you spend 1000 euro on energy, then we will calculate the greenhouse gas emissions based on KWh volume of the the energy that cost you 1000 euros. This way we avoid a lot of data entry into the calculator. At this stage, you we do not offer an uploader, because we want to test manually different trial balances before automating the uploading process.\nHow does it work? In the future, we hope our …","date":1654764000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668933600,"objectID":"36c4739b8f0d323164a0ea5fcad4acc5","permalink":"https://surveyharmonies.eu/sector/music/","publishdate":"2022-06-09T09:40:00+01:00","relpermalink":"/sector/music/","section":"sector","summary":"We will help small music organizations in their sustainability reporting, where detail data and reporting standards are only available for greenhouse gas emissions.","tags":["Music Eviota","Eviota","Sustainability","European Green Deal","MusicAIRE"],"title":"Music Eviota","type":"sector"},{"authors":["Martin Senfleben","Thomas Margoni","Daniel Antal","Balazs Bodó","Stef van Gompel","Christian Handke","Martin Kretschmer","Joost Poort","João Quintais","Sebastian Felix Schwemer"],"categories":["Publications"],"content":" Engage with us on LinkedIn @DigitalMusicObs or check out our open data and open repositories, code, tutorials Our article on the trade-offs between data harmonisation and interoperability on the one hand, and transparency and accountability of content recommender systems, and the necessity to take metadata problems in Europe more seriously has been published in the Journal of Intellectual Property, Information Technology and Electronic Commerce Law. The problems we see in this area threaten the market position and visibility of European music, books, films in autonomous, AI-driven platfomrs like YouTube, Apple, Amazon.\nOur Listen Local project in Slovaki a has identified metadata problems—in fact, the correct description of the biographies of artists, adding their composer, producer, and performer data correctly, filling out correct genre and other musicological information—as a main culprit for independent and small country artists to not get recommended or paid on global platforms.\nThe article of Martin Senfleben, Thomas Margoni, Daniel Antal, Balazs Bodó, Stef van Gompel, Christian Handke, Martin Kretschmer, Joost Poort, João Quintais, Sebastian Felix Schwemer was published in JIPITEC, vol. 13, iss. 1, pp. 67-86, 2022. We hope to carry on this important project in our planed Horizon Europe project.\n“In the creative industries, this need for enhanced data quality and interoperability is particularly strong. Without data improvement, unprecedented opportunities for monetising the wide variety of EU creative and making this content available for new technologies, such as artificial intelligence training systems, will most probably be lost. […] While the US have already taken steps to provide an integrated data space for music as of 1 January 2021, the EU is facing major obstacles not only in the field of music but also in other creative industry sectors. […] A trade-off between data harmonisation and interoperability on the one hand, and transparency and accountability of content recommender systems on the other, could pave the way for successful new initiatives.”\nLinks JIPITEC published version\nSSRN Preprint version\n","date":1650357360,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650357360,"objectID":"929ae2c2f8df311545a5a7bf188dec94","permalink":"https://surveyharmonies.eu/post/2022-04-19-european-visibility/","publishdate":"2022-04-19T08:36:00Z","relpermalink":"/post/2022-04-19-european-visibility/","section":"post","summary":"Our article on the trade-offs between data harmonisation and interoperability on the one hand, and transparency and accountability of content recommender systems, and the necessity to take metadata problems in Europe more seriously has been published in the Journal of Intellectual Property, Information Technology and Electronic Commerce Law.","tags":["Publications","Metadata"],"title":"Ensuring the Visibility and Accessibility of European Creative Content on the World Market","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":" Engage with us on LinkedIn @DigitalMusicObs or check out our open data and open repositories, code, tutorials If you are a creator yourself, please, fill out the survey of 2022 in any EU language. If you represent an artist organization, please, make sure that the survey finds its way to your newsletter and get in touch with us to get the results—and any further data you need for your work. The Digital Music Observatory has harmonized and collected surveys in all European countries about how musicians work and earn their living, and how audiences differ in various countries, metropolitan regions in terms of typical age, visiting probability, spending capacity, and other important factors. We helped music organizations to significantly increase the royalty pay-outs of artists in two countries, and we contributed for the advocacy of fairer compensation and fairer taxation in others.\nThis year we teamed up with the ReCreating Europe research consortium of renowned experts in the field of copyright, geography and economics of creativity, sociology of innovation, communication and media studies, cultural policies, Open Knowledge and access to culture, cultural policies, minority rights and disability rights. We are carrying out a truly pan-European survey in all EU languages about important topics for music and other creators:\nUnderstanding how much creators are aware of the legal and financial terms that online platforms use that distribute their work Understanding and interest in how AI Algorithms contribute to the successful or not successful dissemination of their work, or can support their creative processes Understanding and evaluation of fairness in the way internet platforms distribute earnings to creators Attitudes to plagiarism, piracy, and copyright protection Access to Covid-19 relief funds No personal data is involved. What do we ask from individual creators, bands, collectives? The researchers of the reCreating Europe Consortium would like to carry out an interview with you in English. If you are not able to carry out an interview in English, or do not want to, we would be grateful if you would still fill out the survey form (about 10 minutes) in any of the EU official languages, And invite a colleague/friend confident in English to do the same and volunteer for the interview at the end of the survey. Music organizations We would like to ask you to ask your members to do the same above, in your newsletter, Facebook page, or other means of communication. The research with free datasets, visualizations, infographics will be placed for free in the Digital Music Observatory and the Cultural and Creative Sectors Industries Data Observatory and can help your own HR, advocacy, education work. (We have many other similar data assets that are already there, or we can give to you.) Therefore, it is fully compliant with GDPR—we do not want to know who fills out our survey, the interviews are voluntary, and sharing the survey can contribute to your core business.\nBloggers We would be very happy if you would write about this survey or invite creators to participate in our research. We can give you infographics, data visualizations prior to academic publication to write about the results.\nIn our Central \u0026amp; Eastern European Music Industry Report 2020 as a case-study on evidence-based policymaking we compared how musicians and their audiences live in several countries. Our research principles We believe in transparency, openness, and high-quality work. We carry out an open collaboration with representatives of music professionals, NGOs, and universities. Because in the European music ecosystem, most professional and artists are freelancers or micro-entrepreneurs, we also try to form collaborations with individuals. All our data is open, interoperable, reusable data that comes with the highest quality of documentation and help for reuse.\nWe would appreciate it immensely if you would support this important research by disseminating the call to participate in this study.\nHistory Before you start a new questionnaire-based research, get in touch with us! Maybe we have history for your questionnaires. We can make your survey cheaper, better, and more informative. The Digital Music Observatory and its predecessor, CEEMID, has been working with harmonized surveys for 8 years. We have compiled the biggest database of interview transcripts with concert audiences (more than 70,000 interviews in all European countries, soon to be extended to more than 100k) and the world’s biggest harmonized survey dataset about music creators (4000 responses from 12 European countries.) We use the Open Data Directive, originally for government-funded research data, recently extended to taxpayer funded scientific research, to access datasets that are invisible for the music industry.\nWe using survey harmonization and data integration techniques to join hundreds of questionnaire-based research in Europe on music audiences. We are now improving …","date":1647781800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647781800,"objectID":"adb1d1c10be655a3637dc93729a18a3e","permalink":"https://surveyharmonies.eu/post/2022-03-20-pan-european-creator-survey/","publishdate":"2022-03-20T14:10:00+01:00","relpermalink":"/post/2022-03-20-pan-european-creator-survey/","section":"post","summary":"Understanding coping strategies with online platforms, their Algorithms, piracy and free use, and Covid-19 relief measures. A truly Pan-European data collection.","tags":["Digital Music Observatory","Survey harmonization","Value of music","Trustworthy AI"],"title":"Pan-European Creator Survey","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":" We follow the Eurostat indicator design guidelines to make sure that our users – cultural ministries, representative organizations of music, large music organization and companies, researcher get the information that they need in usable and precise indicators. Many people only trust statistics from the Eurostat service, a U.S. federal service, or from their national statistical offices. Our data provided by market research firms with a premium price. Our aim is to create open key business performance indicators, evidence-based policy indicators and other statistical indicators for academic research that has a similar or higher level of usability, reliability, and quality. We want to fill the data gaps for the Music Economy, the Diversity of Circulation of European Music, Music \u0026amp; Society and Music Innovation data left by public authorities and international organization with timely, easy-to-import, easy to use indicators and visualizations.\nWe work using the agile open collaboration method that allows music organizations, companies, universities, think-tanks, and individual researchers to share data, efforts, and results. We are asking our users to nominate a knowledgeable data curator who communicates the data needs to us. The curators are not data experts: they tell us what their organization or project needs, and they evaluate if our technically sound datasets and visualizations truly meet your needs, or they must be redesigned. We make sure that whatever the data curators approved will be refreshed, if needed, monthly or quarterly, and placed into our data services for free.\nOur quality assurance program is sometimes the same as in national statistical offices, and sometimes it is different. We use only open-source statistical software, and we follow the Open Policy Analysis Guidelines for full transparency in our data handling. We send both our software code and our results to scientific peer review, which is often more rigorous than other internal quality controls. We use so-called computerized unit-tests, often almost 100 of them, that search for signs of any data corruption.\nSee our previous blogpost: How We Add Value to Public Data With Imputation and Forecasting? with an example on radio market data. We involve business, policy and academic users in the entire cycle of indicator design to make sure that they receive the information they need in their decision making process or resaerch, and that the information is correct and well documented.\nWe place the data, the codebook in the EU OpenAire and Zenodo repositories with a DOI and versioning for data integrity, and into the Figshare repository for free, resuable, Creative Commons visualizations.\nWe have a similar Rest API to the Eurostat data warehouse for automatic database connections. But we always make sure that the data is available in the easiest to use form for SPSS, STATA, Excel, Numbers, or OpenOffice.\nCurrently we have only ad hoc funding, and we could not process all the data we have. In fact, we have more than 1000 indicators in our treasure trove — if you need something, just contact us and ask, we are more than likely to be able to give you something. If you need more data, consider joining or R\u0026amp;D Consortium as a music sector affiliated partner to seek EU funds to provide you with free and high quality data. ","date":1645086240,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645089840,"objectID":"01342159a1f070bbe1018cdd275c511e","permalink":"https://surveyharmonies.eu/post/2022-02-17-better-indicator-design/","publishdate":"2022-02-17T09:24:00+01:00","relpermalink":"/post/2022-02-17-better-indicator-design/","section":"post","summary":"We want to create open indicators that business users, music organizations, evidence-based policy makers can trust as much as data from Eurostat, U.S. federal agencies, or premium market research companies. Using similarly rigorous statistical standards, open data, open science, and the innovations of data science we want to fill the data gaps of the European music industry with timely, easy-to-import, visualized, documented, high-quality data.","tags":["Digital Music Observatory","Indicator design","Open data","Open Science","Open Policy Analysis"],"title":"How We Design Better Indicators?","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":"Survey harmonization is a powerful research tool to increase the usability of questionnaire-based empirical research. When the same questions are asked from similarly selected German and French people, music audiences and musicians, then we can make meaningful comparisons between the public opinions of the two countries, or the different perceptions of fans and makers of music. And if we repeat this procedure time after time, we can see if opinion is shifting more in Germany or France, or the living conditions of musicians are catching up with the rest of the country population.\nIn our Central \u0026amp; Eastern European Music Industry Report 2020 as a case-study on evidence-based policymaking we compared how musicians and their audiences live in several countries. The Digital Music Observatory and its predecessor, CEEMID, has been working with harmonized surveys for 8 years. We have compiled the biggest database of interview transcripts with concert audiences (more than 70,000 interviews in all European countries, soon to be extended to more than 100k) and the world’s biggest harmonized survey dataset about music creators (4000 responses from 12 European countries.) We use the Open Data Directive, originally for government-funded research data, recently extended to taxpayer funded scientific research, to access datasets that are invisible for the music industry.\nWe using survey harmonization and data integration techniques to join hundreds of questionnaire-based research in Europe on music audiences. We are now improving our capacities to bring analysis to sub-national level, like in the example of Wales. You can create better surveys with less cost: you only need to ask the information, or change of information, that is not included in our harmonized datasets. Shorter, better questionnaires, smaller samples sizes, huge cost savings.\nWhen you make questionnaire-based research, you immediately get a history (the same question asked years ago) and an international comparison (the same question asked in other countries.)\nOften you do not even have to pay for the survey, because somebody else has already made a similar taxpayer funded research and we can just get the data for you. Harmonizing surveys requires advanced data science and statistics knowledge, which is what we provide with the scientific partners of the Digital Music Observatory. We have developed an open-source software, free to use, for this purpose. It is not for the faint heart – but users of our observatory can just leave their data for us and let us run the code.\nBefore you start a new questionnaire-based research, get in touch with us! Maybe we have history for your questionnaires. We can make your survey cheaper, better, and more informative. See our use case for gender diversity in music. ","date":1645027200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645082100,"objectID":"122f52e86e2f2d350c43601045c83b53","permalink":"https://surveyharmonies.eu/post/2022-02-16-survey-harmonization/","publishdate":"2022-02-16T17:00:00+01:00","relpermalink":"/post/2022-02-16-survey-harmonization/","section":"post","summary":"Survey harmonization is a powerful research tool to increase the usability of questionnaire-based empirical research.  When the same questions are asked from similarly selected German and French people, music audiences and musicians, then we can make meaningful comparisons between the public opinions of the two countries, or the different perceptions of fans and makers of music.","tags":["Digital Music Observatory","Survey harmonization","Cultural Access and Participation","Data integration"],"title":"What is Survey Harmonization?","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":"Our ambition is to build a comprehensive data source and online data analysis tools for the European music industry. This requires a map of the music ecosystem — we need to understand where value is created and money is exchanged, and we need to observe how much is this value and how much is being paid for it.\nThe value chain is a concept describing the full chain of a businesses\u0026#39; activities in the creation of a product or service – from the initial reception of materials all the way through its delivery to market, and everything in between. Some services, like car manufacturing and banking services, are offered by a large corporation that creates the service alone. The music industry is different. In Europe it comprises of hundreds of thousands of freelancers, microenterprises, and a few small-and medium sized or large enterprises that form a creative network to deliver projects.\nOur initial view of the European music value chain is depicted in the chart.\nIf you think we put something in the wrong place, or you do not find an activity important to you: let us know!. Our value chain model was perfected in various research and development projects that aimed to understand the problems and increase the revenues of the Hungarian, Slovak, Croatian, and UK music industries. We have asked over 8 years more than 4000 music professionals in 12 countries about how they work and how they get paid (See our Central European Music Industry Report.)\nIt is based on the standard, accepted description of the music industry with the “three revenue stream model” in the United States (Hull et al. 2011) and the European Union (Leurdijk et al. 2012; Leurdijk and Ottilie 2012). It is built upon the actual application of competition law in EU jurisdictions (Antal 2017, 2019), and it is intended for further research and publication.\nThe value chain is a good tool to localize information sources and plan to collect data in a systematic way. For example, in the case of our music value chain we must focus on some challenges:\nUnlike in other industries, most hours of music are listened to without paying consumers (radio, YouTube, ad-supported streams, home copying, torrenting). Understanding the advertising revenue streams is important.\nThe music industry is both very local and global. Small labels sell all over the world on digital platforms. But they should focus on markets where there is an interest for their music or where the prices are high, where they have a real chance to get playlisted.\nWe must understand why some languages, or women are disadvantaged in this value chain, and what intervention can help this situation.\nThe aim of our mapping is to help the industry solve real problems: keep its revenues in line with the growth of uses in more and more electronic gadgets where people use music, compensate home copying, or close the value gap, or find out where women get underpaid or underrepresented. Our value chain helps us designing data collection plans for complex analysis of music uses, prices and revenues that we will introduce in a subsequent blogpost.\nYou can read more about it in our dynamic The Relevant Market for Music Streaming: Market Definition and Measurement Challenges document is a versioned work-in-progress publication, available from the Zenodo open science repository and our opinion on the UK competition authorities music streaming market inquiry (Competition \u0026amp; Markets Authority 2022). References Antal, Daniel. 2017. “The Competition of Unlicensed, Licensed and Illegal Uses on the Markets of Music and Audiovisual Works [A szabad felhasználások, a jogosított tartalmak és az illegális felhasználások versenye a zenék és audiovizuális alkotások hazai piacán].” Artisjus - not public.\n———. 2019. “The Competition of Unlicensed, Licensed and Illegal Uses on the Markets of Music and Audiovisual Works [A szabad felhasználások, a jogosított tartalmak és az illegális felhasználások versenye a zenék és audiovizuális alkotások hazai piacán].” Artisjus - not public.\nCompetition \u0026amp; Markets Authority. 2022. “Music and Streaming Market Study. Statement of Scope.” Competition \u0026amp; Markets Authority. https://assets.publishing.service.gov.uk/media/61f17285d3bf7f0546a99df2/Music_and_streaming_Statement_of_Scope_final.pdf.\nHull, Geoffrey P., Thomas W. Hutchison, Richard Strasser, and Geoffrey P. Hull. 2011. The Music Business and Recording Industry Delivering Music in the 21st Century. New York: Routledge. http://search.ebscohost.com/login.aspx?direct=true\u0026amp;scope=site\u0026amp;db=nlebk\u0026amp;db=nlabk\u0026amp;AN=345262.\nLeurdijk, Adnra, Sivlian de Munck, Tijs van den Broek, Arjana van der Plas, Walter Manshanden, and Elmer Rietveld. 2012. “Statistical, Ecosystems and Competitiveness Analysis of the Media and Content Industries: A Quantiative Overview.” EUR 25277 EN. Edited by Jean Paul Simon. Seville:Spain: Joint Research Centre of the European Commission - Institute for Prospective Technological Studies. http://ftp.jrc.es/EURdoc/JRC69435.pdf.\nLeurdijk, Adnra, …","date":1644940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644995700,"objectID":"c5a57bbf26e0bb4e06558e97efc43143","permalink":"https://surveyharmonies.eu/post/2022-02-15-music-value-chain/","publishdate":"2022-02-15T17:00:00+01:00","relpermalink":"/post/2022-02-15-music-value-chain/","section":"post","summary":"Our ambition is to build a comprehensive data source and online data analysis tools for the European music industry. This requires a map of the music ecosystem — we need to understand where value is created and money is exchanged, and we need to observe how much is this value and how much is being paid for it.","tags":["Digital Music Observatory","value chain","music ecosystem"],"title":"The Music Value Chain","type":"post"},{"authors":null,"categories":null,"content":"The European Union is introducing a ground-breaking accounting standard which will call on about 49,000 enterprises to assess their ‘double materiality’, i.e. both the financial and sustainability impacts of their own activities, as well as those of their buyers and suppliers. Does your company buy cattle or sheep products to produce meat? You must consider the methane emissions of your suppliers. Do you manufacture cars? You must account for the fuel emissions of the buyers and users of your product.\nWhy Eviota? We named Eviota after a small fish that lives symbiotically among the tentacles of the mushroom coral. It not only rhymes with our software, iotables, but it refers to the fragile ecosystem of coral reefs: miraculous and beautiful forms of life under threat from global warming. The first step of saving our planet is to objectively detect where your organization’s stakeholders – suppliers, buyers, workers, technology – leave an impact on the environment. The Eviota project aims to create sustainability reports connected to the financial accounts of companies, NGOs, and civil society actors. The first phase concentrates on greenhouse gases and air pollutants. We want to create reliable estimates of the carbon and other pollutants footprint of music-related (social) enterprises based on their spending (“connected financial and sustainability reporting”.)\nCreating connected sustainability reports has many advantages.\nIt shows consumers, donors, and buyers that the company cares for sustainable growth. The organization can ask for grants related to increasing sustainability. In the EU, the company will be eligible for cheaper green loans, green insurance, and green investments. Large corporations, including music event donors, may require their supply chain to produce credible sustainability metrics. There are some difficulties that we want to overcome in this project.\nConnected financial and sustainability reports are complex, they require plenty of data, and mandatory only for ’large’ corporations. Just like small companies can make ‘simplified tax returns’ and ‘simplified financial reports’, we aim to create a less complex and cheap simplified, connected financial and sustainability report. Due to the high complexity, they take a long time to create and are costly: the European Commission estimates their total cost at €10,000. In Europe, there are a handful of large organizations present in the music industry. So, there is no regulatory push for music enterprises to engage in sustainability reporting. However, this means that they cannot benefit from the advantages above.\nTable of Contents Our approach How does it work? What is the report? Methodology Open collaboration Why are we developing this service? Future plans: Social Sustainability and Anti-Bribary MusicAIRE Green Recovery in the Music Sector Our approach Most sustainability calculators are very complex because they use many data inputs from the company. Our mission is to reduce the complexity; however, this would require plenty of experience to define the shortcuts. We will compare all spending (upstream value chain or suppliers) and all income (downstream value chain or buyers) to the know spending of all similar organizations in your country in the comparison year.\nTo reduce the data need, we only take into consideration cost/income groups that meet the financial materiality treshold, i.e. 3% of your total costs or total business-to-business sales. We offer free, manual calculation in the first phase to ensure we define these simplifications well. To reduce the time needed to collect data about your purchases and sales, we will rely on a part of the “trial balance”, because this is available in your accounting system (and can be exported by your accounting software.) The trial balance is an annual summary of the general ledger accounts. We need only the expenses and revenues accounts, and do not need assets, liabilities, gains and losses.\nA part of a fictitious Italian trial balance with Italian and English language labels. The blurred numbers are randomized from an actual trial balance and presented in a different currency than the original. Why the trial balance? We start from a document that every company has, and does not require extra management time to prepare, the so-called trial balance. This is an accounting document that can be obtained from the company’s accountant.\nA trial balance is a report that lists the balances of all general ledger accounts of a company at a certain point in time. The accounts reflected on a trial balance are related to all major accounting items, including assets, liabilities, equity, revenues, expenses, gains, and losses. It is primarily used to identify the balance of debits and credits entries from the transactions recorded in the general ledger at a certain point in time.\nNo extra management time is needed: it is already recorded by every company’s accountant. The general ledger is recorded by your …","date":1641340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641340800,"objectID":"9e71d783e6c0dba55262514cb31af7a3","permalink":"https://surveyharmonies.eu/project/eviota/","publishdate":"2022-01-05T00:00:00Z","relpermalink":"/project/eviota/","section":"project","summary":"Connected financial and sustainability reporting","tags":["Eviota"],"title":"Eviota","type":"project"},{"authors":["Daniel Antal"],"categories":null,"content":" A reprezentative sample of n=100793 from 5 years on the most serious global problem. Get the tidy dataset from our repository or API. Imagine if you could compare data easily from surveys taken about climate change from all European countries, maybe even from other continents, from different years? If you could work with a sample of not only n=1000, n=4000, or n=10,000 but n=100,000? What type of granularity it would give you about the perception of climate change or supported policy measures? That is exactly what our survey harmonization software allows for you to do.\nYou can use and verify our software: it is a perfectly documented, open source, peer-reviewed scientific software. But for most users, a bit too difficult to handle. This is why we are building the Green Deal Data Observatory as a user-centered API around the software. The Green Deal Data Observatory is processing climate-change related data from variuos survey, sensory, satellite data sources, and places them into tidy, easy-to-import datasets and visualizations.\nSurvey harmonization means various social science, statistical and data processing steps to make data comparable and joinable from various questionnaire answers taken in different countries, languages, and years. To demonstrate the power of retrospective survey harmonization, we have made an indicator, visualizations and a data animation from more than a hundred nationally representative surveys, which asked more than 137,000 Europeans about what they considered to be the single most serious problem facing the world as a whole?\nSurvey data harmonization refers to procedures that improve the data comparability or the possibility to make policy or scientific comparisons between data from surveys conducted in different countries or in different years. Our retroharmonize software helps this tedious, laborous, difficult data processing task.\nThe result is stunning compared to a survey of 1000, 4000 or even 10,000 people. In this video we have harmonized the answers from more than 137,000 Europeans surveyed in more than 20 languages. As you can see in the data animation, people got more and more concerned about climate change… until Covid struck.\nOur data shows that more urban and higher educated people tend to be more and more concerned about climate change. Concern is higher and higher as younger and younger people are asked. (Our data source, the Eurobaromter survey is asking Europeans from the age of 15.)\nThere are huge national differences in Europe: people in the countries that we defined as Nordic (Scandinavia and Finland) are much more serious about climate change than the rest of the continent. It also matters when was the question asked: between 2013-2019 anxiety over the climate has been growing rapidly, but it peaked in 2019. In 2020, the Covid pandemic has altered the problem map of the European population, with ‘infectious diseases’ other important global problems. But apart from the time of asking the question, and the place of asking, there are important patterns emerging all over Europe which are shared regardless of the time and place.\nOur classification tree model shows what factors play an important role in determining if somebody believes that climate change is the most important global problem. People with no formal education rarely think that climate change is the most important global problem. People with secondary school education care less than people with tertiary education, and people with tertiary education or a bachelor’s degree care less than people who have a university degree or who are committed to life-long learning. This effect is further emphasized by level of urbanization: the more urbanized are the respondents, the more likely they think that climate change is the single most important problem facing humanity. (Urban people tend to have higher education levels, too.)\nAnother important factor is age: the younger the respondent, the more likely to believe that climate change is the single most important problem.\nOne takeaway is that generally, people’s climate awareness is rising: Europeans tend to be more urbanized and more educated, and this works in favor of recognizing this problem. The coming younger generations are also more aware of climate change. Yet, as Covid-19 shows, a global trauma can alter the picture quickly.\nUsing the implemented machine learning R software package of Christoph Molnar, we calculated the importance of various socio-demography variables in predicting who will think that climate change is the most important problem facing us.\nOut of the variables we investigated, time spent in education is the most important factor contributing to climate awareness, closely followed by the time when the question was asked. The importance of age, time, and even the time spent in education (age of leaving formal education) show that there is very significant change over time. Unfortunately, this change is not monotonous, until 2019 climate …","date":1637829660,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637829660,"objectID":"abcdf06793f986ac100f2b83ac7b10d1","permalink":"https://surveyharmonies.eu/post/2021-11-19_global_problem/","publishdate":"2021-11-25T09:41:00+01:00","relpermalink":"/post/2021-11-19_global_problem/","section":"post","summary":"Imagine if you could compare data easily from surveys taken about climate change from all European countries, maybe even from other continents, from different years? If you could work with a sample of not only n=1000, n=4000, or n=10,000 but n=100,000? What type of granularity it would give you about the perception of climate change or supported policy measures?  That is exactly what our survey harmonization software allows for you to do.","tags":["survey harmonization","data-as-service","retroharmonize","climate awareness","Europe"],"title":"100,000 Opinions on the Most Pressing Global Problem","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":"Public data sources are often plagued by missng values. Naively you may think that you can ignore them, but think twice: in most cases, missing data in a table is not missing information, but rather malformatted information. This approach of ignoring or dropping missing values will not be feasible or robust when you want to make a beautiful visualization, or use data in a business forecasting model, a machine learning (AI) applicaton, or a more complex scientific model. All of the above require complete datasets, and naively discarding missing data points amounts to an excessive waste of information. In this example we are continuing the example a not-so-easy to find public dataset.\nIn the previous blogpost we explained how we added value with documenting the data following the FAIR principle and with the professional curatorial work of placing the data in context, and linking it to other information sources that are not depending on the English language, and can connect our radio dataset to other data, books, publications, regardless if they are described in English, or in German, or Slovak. Photo: Atmospheric Research Observatory, South Pole, Antarctica Photo: NOAA. Completing missing datapoints requires statistical production information (why might the data be missing?) and data science knowhow (how to impute the missing value.) If you do not have a good statistician or data scientist in your team, you will need high-quality, complete datasets. This is what our automated data observatories provide.\nWhy is data missing? International organizations offer many statistical products, but usually they are on an ‘as-is’ basis. For example, Eurostat is the world’s premiere statistical agency, but it has no right to overrule whatever data the member states of the European Union, and some other cooperating European countries give to them. And they cannot force these countries to hand over data if they fail to do so. As a result, there will be many data points that are missing, and often data points that have wrong (obsolete) descriptions or geographical dimensions. We will show the geographical aspect of the problem in a separate blogpost; for now, we only focus on missing data.\nSome countries have only recently started providing data to the Eurostat umbrella organization, and it is likely that you will find few datapoints for North Macedonia or Bosnia-Herzegovina. Other countries provide data with some delay, and the last one or two years are missing. And there are gaps in some countries’ data, too.\nSee the authoritative copy of the dataset. This is a headache if you want to use the data in some machine learning application or in a multiple or panel regression model. You can, of course, discard countries or years where you do not have full data coverage, but this approach usually wastes too much information–if you work with 12 years, and only one data point is available, you would be discarding an entire country’s 11-years’ worth of data. Another option is to estimate the values, or otherwise impute the missing data, when this is possible with reasonable precision. This is where things get tricky, and you will likely need a statistician or a data scientist onboard.\nWhat can we improve? Consider that the data is only missing from one year for a particular country, 2015. The naive solution would be to omit 2015 or the country at hand from the dataset. This is pretty destructive, because we know a lot about the R\u0026amp;D allocations in this country and in this year! But leaving 2015 blank will not look good on a chart, and will make your machine learning application or your regression model stop.\nA statistician or an innovation expert will tell you that you know more-or-less the missing information: the total allocation was most likely not zero in that year. With some statistical or innovation, or public finance specific knowledge you will use the 2014, or 2016 value, or a combination of the two and keep the country and year in the dataset.\nOur improved dataset added backcasted (using the best time series model fitting the country’s actually present data), forecasted (again, using the best time series model), and approximated data (using linear approximation.) In a few cases, we add the last or next known value. To give a few quantiative indicators about our work:\nIncreased number of observations: 29.2% Reduced missing values: -26.4% Increased non-missing subset for regression or AI: +64.7% If your organization is working with panel (longitudional multiple) regressions or various machine learning applications, then your team knows that not havint the +66.67% gain would be a deal-breaker in the choice of models and punctuality of estimates or KPIs or other quantiative products. And that they would spent about 90% of their data resources on achieving this +66.67% gain in usability.\nIf you happen to work in an NGO, a business unit or a research institute that does not employ data scientists, then it is likely that you can …","date":1636362000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636452000,"objectID":"130d55eda7515ea65b8eb94137c6c13d","permalink":"https://surveyharmonies.eu/post/2021-11-08-indicator_value_added/","publishdate":"2021-11-08T10:00:00+01:00","relpermalink":"/post/2021-11-08-indicator_value_added/","section":"post","summary":"Public data sources are often plagued with missng values. Naively you may think that you can ignore them, but think twice: in most cases, missing data in a table is not missing information, but rather malformatted information which will destroy your beautiful visualization or stop your application from working. In this example we show how we increase the usable subset of a public dataset by 66.7%, rendering useful what would otherwise have been a deal-breaker in panel regressions or machine learning applications.","tags":["music","data-as-service","API","metadata","forecasting","missing data"],"title":"How We Add Value to Public Data With Imputation and Forecasting?","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":"In this example, we show a simple indicator: the Government Budget Allocations for R\u0026amp;D in Environment in many European countries. (In our Digital Music Observatory we give a more relevant example about the turnover of the radio industry in Europe.)\nThis dataset comes from a public datasource, the data warehouse of the European statistical agency, Eurostat. Yet it is not trivial to use: unless you are familiar with the nomenclature for the analysis and comparison of scientific programmes and budgets or the Frascati Manual, you will probably not find this dataset on the Eurostat website.\nThe raw data can be retrieved GBARD by socioeconomic objectives (NABS 2007)[gba_nabsfin07] Eurostat folder (if you find it.) Our version of this statistical indicator is documented following the FAIR principles: our data assets are findable, accessible, interoperable, and reusable. While the Eurostat data warehouse partly fulfills these important data quality expectations, we can improve them significantly. And we can also improve the dataset, too, as we will show in the next blogpost.\nFindable Data Our data observatories add value by curating the data–we bring this indicator to light with a more descriptive name, and we place it in context with our Green Deal Data Observatory. While many people may need this dataset in the environmental policy organizations, NGOs, scientific journalists, or researchers, most of them has no training in the nomenclatures of scientific and R\u0026amp;D spending or public budget accounts. Our curated data observatories bring together many available data around important domains. Our Green Deal Data Observatory, for example, aims to form an ecosystem of climate policy and climate change mitigation data users and producers.\nWe added descriptive metadata that help you find our data and match it with other relevant data sources. We added descriptive metadata that help you find our data and match it with other relevant data sources. For example, we add keywords and standardized metadata identifiers from the Library of Congress Linked Data Services, probably the world’s largest standardized knowledge library description. This makes sure that you can find relevant data about the same concept (environmental protection) besides our turnover data. This help unambigously connect our dataset with other information source that use the same concept, but maybe different keywords, such as Protection of environment, or maybe Umweltschutz in German, or Ochrana životného prostredia in Slovak. Or avoid confusion with Human environment.\nAccessible Data Our data is accessible in two forms: in csv tabular format (which can be read with Excel, OpenOffice, Numbers, SPSS and many similar spreadsheet or statistical applications) and in JSON for automated importing into your databases. We can also provide our users with SQLite databases, which are fully functional, single user relational databases.\nTidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This makes the data easier to clean, and far more easier to use in a much wider range of applications than the original data we used. In theory, this is a simple objective, yet we find that even governmental statistical agencies–and even scientific publications–often publish untidy data. This poses a significant problem that implies productivity loses: tidying data will require long hours of investment, and if a reproducible workflow is not used, data integrity can also be compromised: chances are that the process of tidying will overwrite, delete, or omit a data or a label.\nTidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. While the original data source, the Eurostat data warehouse is accessible, too, we added value with bringing the data into a tidy format. Tidy data can immediately be imported into a statistical application like SPSS or STATA, or into your own database. It is immediately available for plotting in Excel, OpenOffice or Numbers.\nInteroperability Our data can be easily imported with, or joined with data from other internal or external sources.\nAll our indicators come with standardized descriptive metadata, and statistical (processing) metadata. See our API All our indicators come with standardized descriptive metadata, following two important standards, the Dublin Core and DataCite–implementing not only the mandatory, but the recommended descriptions, too. This will make it far easier to connect the data with other data sources, e.g. turnover with the number of radio broadcasting enterprises or radio stations within specific territories.\nOur passion for documentation standards and best practices goes much further: our data uses Statistical Data and Metadata eXchange standardized codebooks, unit …","date":1636358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636441320,"objectID":"0f375332304d9b26d06a73003cca93bd","permalink":"https://surveyharmonies.eu/post/2021-11-08-indicator_findable/","publishdate":"2021-11-08T09:00:00+01:00","relpermalink":"/post/2021-11-08-indicator_findable/","section":"post","summary":"Many people ask if we can really add value to free data that can be downloaded from the Internet by anybody. We do not only work with easy-to-download data, but we know that free, public data usually requires a lot of work to become really valuable. To start with, it is not always easy to find.","tags":["API","metadata","FAIR principle","data interoperability","better documentation","data curation","Research \u0026 development"],"title":"How We Add Value to Public Data With Better Curation And Documentation?","type":"post"},{"authors":["Daniel Antal, CFA"],"categories":null,"content":"Every year, the EU announces that billions and billions of data are now “open” again, but this is not gold. At least not in the form of nicely minted gold coins, but in gold dust and nuggets found in the muddy banks of chilly rivers. There is no rush for it, because panning out its value requires a lot of hours of hard work. Our goal is to automate this work to make open data usable at scale, even in trustworthy AI solutions.\nSummary In his presentation, Daniel compared the current state of open data (including governmental open data and scientific open data) to a thrift store. You can often find bargains, or historical data that would be impossible to source from data vendors, but on a strictly as-is basis, without a catalogue, service, or guarantee. Therefore, working with open data requires a careful reprocessing, validation, and in many cases, frequent re-validation. Open data is often over-estimated: it is never a finished product, often it cannot even be downloaded, therefore it requires further investment to make it valuable. However, because most open data arrives from the governmental sector, you can tap into information sources where no market alternative exists. Open data in some cases may be a cheaper substitute to market vendors, but often it is an exclusive source of information that do not have any market vendors.\nSisyphus was punished by being forced to roll an immense boulder up a hill only for it to roll down every time it neared the top, repeating this action for eternity. This is the price that project managers and analysts pay for the inadequate documentation of their data assets. The practices related to the exploitation of open data are not only relevant in an open data context: these are good data ingestion and procurement practices for any third party data, and in large organizations, for any cross-departmental data. (See the blogpost: The Data Sisyphus.)\nCase Study: Belgian Drought/Flood Risk Awareness, Financial Capacity \u0026amp; Hydrology a complex integration of various open data sources.\nIn the second part of the presentation, Daniel talked about our modern data observatory concept. We have reviewed about 80 functioning and already defunct international data collection programs. Data observatories, like Copernicus’ Observatory, are permanent infrastructure to record various domain-specific data, such as alternative fuel information, information on homelessness, or on the European music business. In our assessment, most of the EU, OECD, UNESCO recognized or endorsed observatories use obsolete technology and do not rely on the new achievements of data science. Reprex, our start-up offers an open source, open data based alternative solution to build largely automated data observatories. We believe that human judgement is needed in data curation, but processing, documentation and validation is best done by computers.\nCase Study: Reprocessing geographical information with administrative boundary changes At last, he presented a few development directions with our open-source software, mentioning our work withing the rOpenGov community. This part of the presentation was originally meant to open the way for a half-day open data workshop, but due to the current pandemic situation, the physical part of the conference and the workshops were not held.\nThe presentation largely included the topics of our Data \u0026amp; Lyrics blogpost: Open Data—The New Gold Without the Rush\nPresentation Slides See the presentation slides here.\n","date":1633687800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633687800,"objectID":"89932f77e8ba55e817c9e24cc0f405ef","permalink":"https://surveyharmonies.eu/talk/crunchconf-open-data-new-gold-without-the-rush/","publishdate":"2021-10-08T10:10:00Z","relpermalink":"/talk/crunchconf-open-data-new-gold-without-the-rush/","section":"event","summary":"Every year, the EU announces that billions and billions of data are now “open” again, but this is not gold. At least not in the form of nicely minted gold coins, but in gold dust and nuggets found in the muddy banks of chilly rivers. There is no rush for it, because panning out its value requires a lot of hours of hard work. Our goal is to automate this work to make open data usable at scale, even in trustworthy AI solutions.","tags":["Open data"],"title":"Crunchconf: Open Data, New Gold Without the Rush","type":"event"},{"authors":["Daniel Antal"],"categories":null,"content":" Sisyphus was punished by being forced to roll an immense boulder up a hill only for it to roll down every time it neared the top, repeating this action for eternity. This is the price that project managers and analysts pay for the inadequate documentation of their data assets. When was a file downloaded from the internet? What happened with it sense? Are their updates? Did the bibliographical reference was made for quotations? Missing values imputed? Currency translated? Who knows about it – who created a dataset, who contributed to it? Which is an intermediate format of a spreadsheet file, and which is the final, checked, approved by a senior manager?\nBig data creates inequality and injustice. On aspect of this inequality is the cost of data processing and documentation – a greatly underestimated, and usually not reported cost item. In small organizations, where there are no separate data science and data engineering roles, data is usually supposed to be processed and documented by (junior) analysts or researchers. This a very important source of the gap between Big Tech and them: the data usually ends up very expensive, ill-formatted, not readable by computers that use machine learning and AI. Usually the documentation steps are completely omitted.\n“Data is potential information, analogous to potential energy: work is required to release it.” – Jeffrey Pomerantz\nMetadata, which is information about the history of the data, and information how it can be technically and legally reused, has a hidden cost. Cheap or low-quality external data comes with poor or no metadata, and small organizations lack the resources to add high-quality metadata to their datasets. However, this only perpetuates the problem.\nThe hidden cost item behind the unbillable hours As we have shown with our research partners, such metadata problems are not unique to data analysis. Independent artists and small labels are suffering on music or book sales platforms, because their copyrighted content is not well documented. If you automatically document tens of thousands of songs or datasets, the documentation cost is very small per item. If you, do it manually, the cost may be higher than the expected revenue from the song, or the total cost of the dataset itself. (See our research consortiums’ preprint paper: Ensuring the Visibility and Accessibility of European Creative Content on the World Market: The Need for Copyright Data Improvement in the Light of New Technologies)\nIn the short run, small consultancies, NGOs, or as a matter of fact, musicians, seem to logically give up on high-quality documentation and logging. In the long run, this has two devastating consequences: computers, such as machine learning algorithms cannot read their documents, data, songs. And as memory fades, the ill-documented resources need to be re-created, re-checked, reformatted. Often, they are even hard to find on your internal server or laptop archive.\nMetadata is a hidden destroyer of the competitiveness of corporate or academic research, or independent content management. It never quoted on external data vendor invoices, it is not planned as a cost item, because metadata, the description of a dataset, a document, a presentation, or song, is meaningless without the resource that it describes. You never buy metadata. But if your dataset comes without proper metadata documentation, you are bound, like Sisyphus, to search for it, to re-arrange it, to check its currency units, its digits, its formatting. Data analysts are reported to spend about 80% of their working hours on data processing and not data analysis – partly, because data processing is a very laborious task that can be done by computers at a scale far cheaper, and partly because they do not know if the person who sat before them at the same desk has already performed these tasks, or if the person responsible for quality control checked for errors.\nUncut diamonds need to be cut, polished, and you have to make sure that they come from a legal source. Data is similar: it needs to be tidied up, checked and documented before use. Photo: Dave Fischer. Undocumented data is hardly informative – it may be a page in a book, a file in an obsolete file format on a governmental server, an Excel sheet that you do not remember to have checked for updates. Most data are useless, because we do not know how it can inform us, or we do not know if we can trust it. The processing can be a daunting task, not to mention the most boring and often neglected documentation duties after the dataset is final and pronounced error-free by the person in charge of quality control.\nOur observatory automatically processes and documents the data The good news about documentation and data validation costs is that they can be shared. If many users need GDP/capita data from all over the world in euros, then it is enough if only one entity, a data observatory, collects all GDP and population data expresed in dollars, korunas, and euros, …","date":1625734800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625734800,"objectID":"7fe21d2727803b3edcddd706a67fe6a2","permalink":"https://surveyharmonies.eu/post/2021-07-08-data-sisyphus/","publishdate":"2021-07-08T09:00:00Z","relpermalink":"/post/2021-07-08-data-sisyphus/","section":"post","summary":"Sisyphus was punished by being forced to roll an immense boulder up a hill only for it to roll down every time it neared the top, repeating this action for eternity.  When was a file downloaded from the internet?  What happened with it sense?  Are their updates? Did the bibliographical reference was made for quotations?  Missing values imputed?  Currency translated? Who knows about it – who created a dataset, who contributed to it?  Which is the final, checked, approved by a senior manager?","tags":["Metadata","API"],"title":"The Data Sisyphus","type":"post"},{"authors":null,"categories":null,"content":"Adding metadata exponentially increases the value of data. Did your region add a new town to its boundaries? How do you adjust old data to conform to constantly changing geographic boundaries? What are some practical ways of combining satellite sensory data with my organization’s records? And do I have the right to do so? Metadata logs the history of data, providing instructions on how to reuse it, also setting the terms of use. We automate this labor-intensive process applying the FAIR data concept.\nIn our observatory we apply the concept of FAIR (findable, accessibe, interoperable, and reusable digital assets) in our APIs and in our open-source statistical software packages.\nThe hidden cost item Metadata gets less attention than data, because it is never acquired separately, it is not on the invoice, and therefore it remains an a hidden cost, and it is more important from a budgeting and a usability point of view than the data itself. Metadata is responsible for industry non-billable hours or uncredited working hours in academia. Poor data documentation, lack of reproducible processing and testing logs, inconsistent use of currencies, keywords, and storing messy data make reusability and interoperability, integration with other information impossible.\nFAIR Data and the Added Value of Rich Metadata we introduce how we apply the concept of FAIR (findable, accessibe, interoperable, and reusable digital assets) in our APIs.\nOrganizations pay many times for the same, repeated work, because these boring tasks, which often comprise of tens of thousands of microtasks, are neglected. Our solution creates automatic documentation and metadata for your own historical internal data or for acquisitions from data vendors. We apply the more general Dublin Core and the more specific, mandatory and recommended values of DataCite for datasets – these are new requirements in EU-funded research from 2021. But they are just the minimal steps, and there is a lot more to do to create a diamond ring from an uncut gem.\nMap your data: bibliographis, catalogues, codebooks, versioning Updating descriptive metadata, such as bibliographic citation files, descriptions and sources to data files downloaded from the internet, versioning spreadsheet documents and presentations is usually a hated and often neglected task withing organization, and rightly so: these boring and error-prone tasks are best left to computers.\nAlready adjusted spreadsheets are re-adjusted and re-checked. Hours are spent on looking for the right document with the rigth version. Duplicates multiply. Already downloaded data is downloaded again, and miscategorized, again. Finding the data without map is a treasure hunt. Photo: © N. The lack of time and resources spend on documentation over time reduces reusability and significantly increases data processing and supervision or auditing costs.\nOur observatory metadata is compliant with the Dublin Core Cross-Domain Attribute Set metadata standard, but we use different formatting. We offer simple re-formatting from the richer DataCite to Dublin Core for interoperability with a wider set of data sources. We use all mandatory DataCite metadata fields, all the the recommended and optional ones. It complies with the tidy data principles. In other words: very easy to import into your databases, or join with other databases, and the information is easy to find. Corrections, updates can automatically managed.\nWhat happened with the data before? We are creating Codebooks that are following the SDMX statistical metadata codelists, and resemble the SMDX concepts used by international statistical agencies. (See more technical information here.) Small organizations often cannot afford to have data engineers and data scientists on staff, and they employ analysts who work with Excel, OpenOffice, PowerBI, SPSS or Stata. The problem with these applications is that they often require the user to manually adjust the data, with keyboard entries or mouse clicks. Furthermore, they do not provide a precise logging of the data processing, manipulation history. The manual data processing and manipulation is very error prone and makes the use of complex and high value resources, such as harmonized surveys or symmetric input-output tables, to name two important source we deal with, impossible to use. The use of these high-value data sources often requires tens of thousands of data processing steps: no human can do it faultlessly.\nWhat is even more problematic that simple applications for analysis do not provide a log of these manipulations’ steps: pulling over a column with the mouse, renaming a row, adding a zero to an empty cell. This makes senior supervisory oversight and external audit very costly.\nOur data comes with full history: all changes are visible, and we even open the code or algorithm that processed the raw data. Your analysts can still use their favourite spreadsheet or statistical software application, but they can start from a …","date":1625616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625616000,"objectID":"8ac5f27972f957d4b5d4188584738e4c","permalink":"https://surveyharmonies.eu/services/metadata/","publishdate":"2021-07-07T00:00:00Z","relpermalink":"/services/metadata/","section":"services","summary":"Adding metadata exponentially increases the value of data. Did somebody already adjust old data to conform to constantly changing geographic boundaries? What are some practical ways of combining satellite sensory data with my organization's records? And do I have the right to do so? Metadata logs the history of data, providing instructions on how to reuse it, also setting the terms of use. We automate this labor-intensive process applying the FAIR data concept.","tags":["metadata"],"title":"Metadata","type":"services"},{"authors":null,"categories":null,"content":"We provide retrospecitve, ex post, and ex ante survey harmonization to our partners.\nThe aim of retrospective survey harmonization is to pool data from pre-existing surveys made with a similar methodology in different points in time and different countries or territories. Ex post survey harmonization is in a way a passive form of pooling research funding because you can utilize information from surveying that were made on somebody else’s expense. The Arab Barometer surveys do not have a consolidated codebook, but our retroharmonize software created one, and put together data from three years and collected in many countries about various public policy issues. The aim of ex ante survey harmonization is to maximize the value from future retrospective harmonization; in a way, it is an active form of pooling research funding, because you benefit from money spent on related open governmental and open science survey programs. In this example we designed a survey representative among music professionals that it can be compared with large-sample, national surveys on living conditions and attitudes, and with occupational groups. Nationally representative surveys do not question enough musicians to allow such specific use; musician only surveys do not allow comparison. retorhamonize is a peer-reviewed, scientfic statistcal software that allows the programmatic retrospective harmonization of surveys, such as the last 35 years of all Eurobarometer microdata, or all Afrobarometer microdata. Eurobarometer grew out of certain CEE member states’ need for comparable data about their music and audiovisual sectors. We commissioned surveys following ESSNet-Culture guidelines and combined our survey data with open access European microdata-level surveys.\nregions solves the problems caused by Europe’s shifting regional boundaries, which have undergone changes in several thousand places over the last twenty years, meaning member states’ and Eurostat’s regional statistics are not comparable over more than two to three years. This software validates and, where possible, changes the regional coding from NUTS1999 until the not yet used NUTS2021, opening up vast, valuable, untapped data sources that can be used for longitudinal analysis or for panel analysis far more precise than what national data alone would allow. It was originally designed in a research project at IVIR in the University of Amsterdam to understand the geographical dynamics of book piracy. Because of the needs this software fills, it had 700 users in the first month after publication. It is particularly useful to re-code old surveys, as regional boundaries are changing in each decade several hundred times in Europe.\n","date":1625472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625472000,"objectID":"395517f5bd9a85f85c9fbd68e92b6d7d","permalink":"https://surveyharmonies.eu/data/surveys/","publishdate":"2021-07-05T08:00:00Z","relpermalink":"/data/surveys/","section":"data","summary":"Ex post survey harmonization is in a way a passive form of pooling research funding because you can utilize information from surveying that were made on somebody else’s expense.  The aim of ex ante survey harmonization is to maximize the value from future retrospective harmonization; in a way, it is an active form of pooling research funding, because you benefit from money spent on related open governmental and open science survey programs.","tags":["surveys","survey harmonization"],"title":"Survey Harmonization","type":"data"},{"authors":["Daniel Antal"],"categories":null,"content":"A new version of the retroharmonize R package – which is working with retrospective, ex post harmonization of survey data – was released yesterday after peer-review on CRAN. It allows us to compare opinion polling data from the Arab Barometer with the Eurobarometer and Afrorbarometer. This is the first version that is released in the rOpenGov community, a community of R package developers on open government data analytics and related topics.\nSurveys are the most important data sources in social and economic statistics – they ask people about their lives, their attitudes and self-reported actions, or record data from companies and NGOs. Survey harmonization makes survey data comparable across time and countries. It is very important, because often we do not know without comparison if an indicator value is low or high. If 40% of the people think that climate change is a very serious problem, it does not really tell us much without knowing what percentage of the people answered this question similarly a year ago, or in other parts of the world.\nWith the help of Ahmed Shabani and Yousef Ibrahim, we created a third case study after the Eurobarometer, and Afrobarometer, about working with the Arab Barometer harmonized survey data files.\nEx ante survey harmonization means that researchers design questionnaires that are asking the same questions with the same survey methodology in repeated, distinct times (waves), or across different countries with carefully harmonized question translations. Ex post harmonizations means that the resulting data has the same variable names, same variable coding, and can be joined into a tidy data frame for joint statistical analysis. While seemingly a simple task, it involves plenty of metadata adjustments, because established survey programs like Eurobarometer, Afrobarometer or Arab Barometer have several decades of history, and several decades of coding practices and file formatting legacy.\nVariable harmonization means that if the same question is called in one microdata source Q108 and the other eval-parl-elections then we make sure that they get a harmonize and machine readable name without spaces and special characters. Variable label harmonization means that the same questionnaire items get the same numeric coding and same categorical labels. Missing case harmonization means that various forms of missingness are treated the same way. For the climate awareness dataset get the country averages and aggregates from Zenodo, and the plot in jpg or png from figshare. In our new Arab Barometer case study, the evaulation of parliamentary elections has the following labels. We code them consistently 1: free_and_fair, 2: some_minor_problems, 3: some_major_problems and 4: not_free.\n“0. missing” “1. they were completely free and fair” “2. they were free and fair, with some minor problems” “3. they were free and fair, with some major problems” “4. they were not free and fair” “8. i don’t know” “9. declined to answer” “Missing” “They were completely free and fair” “They were free and fair, with some minor breaches” “They were free and fair, with some major breaches” “They were not free and fair” “Don’t know” “Refuse” “Completely free and fair” “Free and fair, but with minor problems” “Free and fair, with major problems” “Not free or fair” “Don’t know (Do not read)” “Decline to answer (Do not read)” Of course, this harmonization is essential to get clean results like this:\nFor evaluation or reuse of parliamentary elections dataset get the replication data and the code from the Zenodo open repository. In our case study, we had three forms of missingness: the respondent did not know the answer, the respondent did not want to answer, and at last, in some cases the respondent was not asked, because the country held no parliamentary elections. While in numerical processing, all these answers must be left out from calculating averages, for example, in a more detailed, categorical analysis they represent very different cases. A high level of refusal to answer may be an indicator of surpressing democratic opinion forming in itself.\nSurvey harmonization with many countries entails tens of thousands of small data management task, which, unless automatically documented, logged, and created with a reproducible code, is a helplessly error-prone process. We believe that our open-source software will bring many new statistical information to the light, which, while legally open, was never processed due to the large investment needed.\nWe also started building experimental APIs data is running retroharmonize regularly. We will place cultural access and participation data in the Digital Music Observatory, climate awareness, policy support and self-reported mitigation strategies into the Green Deal Data Observatory, and economy and well-being data into our Economy Data Observatory.\nFurther plans Retrospective survey harmonization is a far more complex task than this blogpost suggest. Retrospective survey …","date":1624870800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624870800,"objectID":"ed5892c8c036e56b13a3ea3390d80d8d","permalink":"https://surveyharmonies.eu/post/2021-06-28-arabbarometer/","publishdate":"2021-06-28T09:00:00Z","relpermalink":"/post/2021-06-28-arabbarometer/","section":"post","summary":"A new version of the retroharmonize R package – which is working with retrospective, ex post harmonization of survey data – was released yesterday after peer-review on CRAN. It allows us to compare opinion polling data from the Arab Barometer with the Eurobarometer and Afrorbarometer. This is the first version that is released in the rOpenGov community, a community of R package developers on open government data analytics and related topics.","tags":["Open data","Open science","R","Data collection","Arab Barometer","survey harmonization"],"title":"Including Indicators from Arab Barometer in Our Observatory","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":"If open data is the new gold, why even those who release fail to reuse it? We created an open collaboration of data curators and open-source developers to dig into novel open data sources and/or increase the usability of existing ones. We transform reproducible research software into research- as-service.\nEvery year, the EU announces that billions and billions of data are now “open” again, but this is not gold. At least not in the form of nicely minted gold coins, but in gold dust and nuggets found in the muddy banks of chilly rivers. There is no rush for it, because panning out its value requires a lot of hours of hard work. Our goal is to automate this work to make open data usable at scale, even in trustworthy AI solutions.\nThere is no rush for it, because panning out its value requires a lot of hours of hard work. Our goal is to automate this work to make open data usable at scale, even in trustworthy AI solutions. Most open data is not public, it is not downloadable from the Internet – in the EU parlance, “open” only means a legal entitlement to get access to it. And even in the rare cases when data is open and public, often it is mired by data quality issues. We are working on the prototypes of a data-as-service and research-as-service built with open-source statistical software that taps into various and often neglected open data sources.\nWe are in the prototype phase in June and our intentions are to have a well-functioning service by the time of the conference, because we are working only with open-source software elements; our technological readiness level is already very high. The novelty of our process is that we are trying to further develop and integrate a few open-source technology items into technologically and financially sustainable data-as-service and even research-as-service solutions.\nOur review of about 80 EU, UN and OECD data observatories reveals that most of them do not use these organizations’s open data - instead they use various, and often not well processed proprietary sources. We are taking a new and modern approach to the data observatory concept, and modernizing it with the application of 21st century data and metadata standards, the new results of reproducible research and data science. Various UN and OECD bodies, and particularly the European Union support or maintain more than 60 data observatories, or permanent data collection and dissemination points, but even these do not use these organizations and their members open data. We are building open-source data observatories, which run open-source statistical software that automatically processes and documents reusable public sector data (from public transport, meteorology, tax offices, taxpayer funded satellite systems, etc.) and reusable scientific data (from EU taxpayer funded research) into new, high quality statistical indicators.\nWe are taking a new and modern approach to the ‘data observatory’ concept, and modernizing it with the application of 21st century data and metadata standards, the new results of reproducible research and data science We are building various open-source data collection tools in R and Python to bring up data from big data APIs and legally open, but not public, and not well served data sources. For example, we are working on capturing representative data from the Spotify API or creating harmonized datasets from the Eurobarometer and Afrobarometer survey programs. Open data is usually not public; whatever is legally accessible is usually not ready to use for commercial or scientific purposes. In Europe, almost all taxpayer funded data is legally open for reuse, but it is usually stored in heterogeneous formats, processed into an original government or scientific need, and with various and low documentation standards. Our expert data curators are looking for new data sources that should be (re-) processed and re-documented to be usable for a wider community. We would like to introduce our service flow, which touches upon many important aspects of data scientist, data engineer and data curatorial work. We believe that even such generally trusted data sources as Eurostat often need to be reprocessed, because various legal and political constraints do not allow the common European statistical services to provide optimal quality data – for example, on the regional and city levels. With rOpenGov and other partners, we are creating open-source statistical software in R to re-process these heterogenous and low-quality data into tidy statistical indicators to automatically validate and document it. We are carefully documenting and releasing administrative, processing, and descriptive metadata, following international metadata standards, to make our data easy to find and easy to use for data analysts. We are automatically creating depositions and authoritative copies marked with an individual digital object identifier (DOI) to maintain data integrity. We are building simple databases and supporting APIs …","date":1624035600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624035600,"objectID":"a5962e972994ae1aa5811355d21781c6","permalink":"https://surveyharmonies.eu/post/2021-06-18-gold-without-rush/","publishdate":"2021-06-18T17:00:00Z","relpermalink":"/post/2021-06-18-gold-without-rush/","section":"post","summary":"If open data is the new gold, why even those who release fail to reuse it? We created an open collaboration of data curators and open-source developers to dig into novel open data sources and/or increase the usability of existing ones. We transform reproducible research software into research- as-service.","tags":["Open data","Open science","R","Data collection"],"title":"Open Data - The New Gold Without the Rush","type":"post"},{"authors":["Daniel Antal","rOpenGov","leo_lahti","kasia_kulma"],"categories":["R-bloggers"],"content":" The new version of our rOpenGov R package regions was released today on CRAN. This package is one of the engines of our experimental open data-as-service Green Deal Data Observatory, Economy Data Observatory, Digital Music Observatory prototypes, which aim to place open data packages into open-source applications.\nClick to expand table of contents of the post Table of Contents Get the Package Join us In international comparison the use of nationally aggregated indicators often have many disadvantages: they inhibit very different levels of homogeneity, and data is often very limited in number of observations for a cross-sectional analysis. When comparing European countries, a few missing cases can limit the cross-section of countries to around 20 cases which disallows the use of many analytical methods. Working with sub-national statistics has many advantages: the similarity of the aggregation level and high number of observations can allow more precise control of model parameters and errors, and the number of observations grows from 20 to 200-300.\nThe change from national to sub-national level comes with a huge data processing price: internal administrative boundaries, their names, codes codes change very frequently. Yet the change from national to sub-national level comes with a huge data processing price. While national boundaries are relatively stable, with only a handful of changes in each recent decade. The change of national boundaries requires a more-or-less global consensus. But states are free to change their internal administrative boundaries, and they do it with large frequency. This means that the names, identification codes and boundary definitions of sub-national regions change very frequently. Joining data from different sources and different years can be very difficult.\nOur regions R package helps the data processing, validation and imputation of sub-national, regional datasets and their coding. There are numerous advantages of switching from a national level of the analysis to a sub-national level comes with a huge price in data processing, validation and imputation, and the regions package aims to help this process.\nYou can review the problem, and the code that created the two map comparisons, in the Maping Regional Data, Maping Metadata Problems vignette article of the package. A more detailed problem description can be found in Working With Regional, Sub-National Statistical Products.\nThis package is an offspring of the eurostat package on rOpenGov. It started as a tool to validate and re-code regional Eurostat statistics, but it aims to be a general solution for all sub-national statistics. It will be developed parallel with other rOpenGov packages.\nGet the Package You can install the development version from GitHub with:\ndevtools::install_github(\u0026#34;rOpenGov/regions\u0026#34;) or the released version from CRAN:\ninstall.packages(\u0026#34;regions\u0026#34;) You can review the complete package documentation on regions.dataobservaotry.eu. If you find any problems with the code, please raise an issue on Github. Pull requests are welcome if you agree with the Contributor Code of Conduct\nIf you use regions in your work, please cite the package as: Daniel Antal. (2021, June 16). regions (Version 0.1.7). CRAN. http://doi.org/10.5281/zenodo.4965909\nDownload the BibLaTeX entry.\nJoin us Join our Green Deal Data Observatory collaboration! Join our open collaboration Green Deal Data Observatory team as a data curator, developer or business developer. More interested in economic policies, particularly computation antitrust, innovation and small enterprises? Check out our Economy Music Observatory team! Or your interest lies more in data governance, trustworthy AI and other digital market problems? Check out our Digital Music Observatory team!\n","date":1623844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623844800,"objectID":"bddf0a5d0c8dcdea162b11029d3b81b5","permalink":"https://surveyharmonies.eu/post/2021-06-16-regions-release/","publishdate":"2021-06-16T12:00:00Z","relpermalink":"/post/2021-06-16-regions-release/","section":"post","summary":"There are numerous advantages of switching from a national level of the analysis to a sub-national level comes with a huge price in data processing, validation and imputation, and the regions package aims to help this process.","tags":["Open data","Open science","Regional data","R","Small area statistics"],"title":"Analyze Locally, Act Globally: New regions R Package Release","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":" Open data is like gold in the mud below the chilly waves of mountain rivers. Panning it out requires a lot of patience, or a good machine. As the founder of the automated data observatories that are part of Reprex’s core activities, what type of data do you usually use in your day-to-day work?\nThe automated data observatories are results of syndicated research, data pooling, and other creative solutions to the problem of missing or hard-to-find data. The music industry is a very fragmented industry, where market research budgets and data are scattered in tens of thousands of small organizations in Europe. Working for the music and film industry as a data analyst and economist was always a pain because most of the efforts went into trying to find any data that can be analyzed. I spent most of the last 7-8 years trying to find any sort of information—from satellites to government archives—that could be formed into actionable data. I see three big sources of information: textual,numeric, and continuous recordings for on-site, offsite, and satellite sensors. I am much better with numbers than with natural language processing, and I am improving with sensory sources. But technically, I can mint any systematic information—the text of an old book, a satellite image, or an opinion poll—into datasets.\nFor you, what would be the ultimate dataset, or datasets that you would like to see in the Green Deal Data Observatory?\nOur retroharmonize and regions packages can create regional statistics from Eurobarometer and Afrobarometer surveys on how people think locally about climate change. I would like to combine this with local information on observable climate change, such as drought, urban heat, and extreme weather conditions. Do people have to feel the pain of climate change to believe in the phenomenon? How do self-reported mitigation steps correlate with what people already feel in their local environment? Suzan is talking about measuring mitigation and damage control, because she’s aware of the already present health risks in overheating urban environments. I am more interested in what people think.\nSee our case study on connecting local tax revenues, climate awareness poll data and drought data in Belgium - we want to extend this to Europe and then to Africa. We also published the code how to do it with tutorials 1, 2 for our International Open Data Day 2021 Event. Is there a number or piece of information that recently surprised you? If so, what was it?\nThere were a few numbers that surprised me, and some of them were brought up by our observatory teams. Karel is talking about the fact that not all green energy is green at all: many hydropower stations contribute to the greenhouse effect and not reduce it. Annette brought up the growing interest in the Dalmatian breed after the Disney 101 Dalmatians movies, and it reminded me of the astonishing growth in interest for chess sets, chess tutorials, and platform subscriptions after the success of Netflix’s The Queen’s Gambit.\nThe Queen’s Gambit’ Chess Boom Moves Online By Rachael Dottle on bloomberg.com Annette is talking about the importance of cultural influencers, and on that theme, what could be more exciting that Netflix’s biggest success so far is not a detective series or a soap opera but a coming-of-age story of a female chess prodigy. Intelligence is sexy, and we are in the intelligence business.\nBut to tell a more serious and more sobering number, I recently read with surprise that there are more people smoking cigarettes on Earth in 2021 than in 1990. Population growth in developing countries replaced the shrinking number of developed country smokers. While I live in Europe, where smoking is strongly declining, it reminds me that Europe’s population is a small part of the world. We cannot take for granted that our home-grown experiences about the world are globally valid.\nDo you have a good example of really good, or really bad use of data?\nFiveThirtyEight.com had a wonderful podcast series, produced by Jody Avirgan, called What’s the Point. It is exactly about good and bad uses of data, and each episode is super interesting. Maybe the most memorable is Why the Bronx Really Burned. New York City tried to measure fire response times, identify redundancies in service, and close or re-allocate fire stations accordingly. What resulted, though, was a perfect storm of bad data: The methodology was flawed, the analysis was rife with biases, and the results were interpreted in a way that stacked the deck against poorer neighborhoods. It is similar to many stories told in a very compelling argument by Catherine D’Ignazio and Lauren F. Klein in their much celebrated book, Data Feminism. Usually, the bad use of data starts with a bad data collection practice. Data analysts in corporations, NGOs, public policy organizations and even in science usually analyze the data that is available.\nYou can find these examples, together with many more that our contributors …","date":1623308400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623315600,"objectID":"3e8ae6bc2572e2e950d012bbe16a1c35","permalink":"https://surveyharmonies.eu/post/2021-06-10-founder-daniel-antal/","publishdate":"2021-06-10T07:00:00Z","relpermalink":"/post/2021-06-10-founder-daniel-antal/","section":"post","summary":"Open data is like gold in the mud below the chilly waves of mountain rivers. Panning it out requires a lot of patience, or a good machine. I think we will come to as surprising and strong findings as Bellingcat, but we are not focusing on individual events and stories, but on social and environmental processes and changes.","tags":["Open data","Open science","Trustworthy AI","Service design","Data collection"],"title":"Open Data is Like Gold in the Mud Below the Chilly Waves of Mountain Rivers","type":"post"},{"authors":["karel_volckaert"],"categories":null,"content":"As a consultant, what type of data do you usually work with?\nI work at the intersection between strategy, finance and organisation. My usual dataset is quite broad - and sometimes unstructured. Oftentimes, the most decisive data are ones that cross domains: economic data coupled with environmental measurements, sociodemographic characteristics linked with online analytics.\nIf you were able to pick, what would be the ultimate dataset, or datasets that you would like to see in the Green Deal Data Observatory? And the Economy Data Observatory?\nIf I may venture that far, the interesting point is where these two data observatories meet. But high on my wishlist would be anything related to geospatial dispersion of environmental and climate data: land erosion, aerosols, solar incidence. From an economic perspective, my interest would go especially to - again - dispersion across regions or other geographical domains of, say, number of new enterprises, disposable income, tax incidence…\nSee our case study on connecting local tax revenues, climate awareness poll data and drought data in Belgium. Why did you decide to join the challenge and why do you think that this would be a game changer for policymakers and for business leaders?\nThere is, both from an ecological and a societal point of view, an urgent need for open-access, real-time, trustworthy data to base decisions on. Ever since Kydland \u0026amp; Prescott’s analyses of “rules rather than discretion” and even earlier analyses of investment under uncertainty, the dynamic rules for optimal decision-making (including investment) require fast-response reliable data.\nDo you have a favorite, or most used open governmental or open science data source? What do you think about it? Could it be improved?\nLet me give one example: the AMECO annual macro-economic database is great for long-term historical analyses but its components ought to be real-time available. As an anecdote, as a fund manager in emerging markets we needed to anticipate macro-economic evolutions and in particular the manner in which capital markets anticipate these evolutions by adjusting foreign exchange rates or positioning themselves along yield curves. To some extent, we needed to predict what AMECO would tell us one year later by means of any real-time trustworthy assessments of the financial or economic situation. The latter data is what we would ideally have in an observatory.\nTo some extent, we needed to predict what AMECO would tell us one year later by means of any real-time trustworthy assessments of the financial or economic situation. The latter data is what we would ideally have in an observatory. Is there a piece of information that recently surprised you? What was it?\nI am currently working on water-related issues and came across a result reported in Nature Energy earlier this year that in more than one in ten hydropower stations, the extra warming from the dark surface of the water reservoir was enough to outbalance its “green” electricity generation potential, leading to no net climate benefits.\nThe researchers found that almost half of the reservoirs they surveyed took just four years to reach a net climate benefit. Unfortunately, they also found that 19% of those surveyed took more than 40 years to do so, and approximately 12% of them took 80 years—the average lifetime of a hydroelectric plant. Calculating the albedo-climate penalty of hydropower dammed reservoirs\nAgain: spatial distribution matters…\nPhoto: Kees Streefkerk, Unplash License From your experience, what do you think the greatest problem with open data in 2021 will be?\nTrust. In a society where “value” and even “truth” is determined more by the amount of (web) links to a particular “fact” than by its intrinsic characteristics, we need to be able to trust data — open data because it’s open and “closed” data because it’s closed.\nWhat can our automated data observatories do to make open data more credible in the European economic policy and climate change or mitigation community and be more accepted as verified information?\nIf I may refer to the previous answer: credibility is enhanced through cross-links between different data from different domains that “does not disprove” one another or that is internally consistent. If, say, data on taxable income goes in one direction and taxes in another, it is the reasoned reconciliation of the - alleged or real - inconsistency that will validate the comprehensive data set. So I am a great believer in broad, real-time observatories where not only the data capture, but the data reconciliation is automated, sometimes by means of a simple comparative statics analysis, in other cases maybe through quite elaborate artificial intelligence.\nJoin our open collaboration Green Deal Data Observatory team as a data curator, developer or business developer, or share your data in our public repository Green Deal Data Observatory on Zenodo. Join us Join our open collaboration Green Deal Data Observatory team …","date":1623178200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623178200,"objectID":"5f2e5a9c56b7d4c024cfc4bce9a32290","permalink":"https://surveyharmonies.eu/post/2021-06-08-data-curator-karel-volckaert/","publishdate":"2021-06-08T18:50:00Z","relpermalink":"/post/2021-06-08-data-curator-karel-volckaert/","section":"post","summary":"Credibility is enhanced through cross-links between different data from different domains that “does not disprove” one another or that is internally consistent. If, say, data on taxable income goes in one direction and taxes in another, it is the reasoned reconciliation of the - alleged or real - inconsistency that will validate the comprehensive data set. So I am a great believer in broad, real-time observatories where not only the data capture, but the data reconciliation is automated, sometimes by means of a simple comparative statics analysis, in other cases maybe through quite elaborate artificial intelligence.","tags":["Open data","Open science","Reproducible research","Open government","Contributors"],"title":"Credibility is Enhanced Through Cross Links Between Different Data from Different Domains","type":"post"},{"authors":["pyry_kantanen"],"categories":null,"content":"As a developer at rOpenGov, and as an economic sociologist, what type of data do you usually use in your work?\nGenerally speaking, people’s access to (or inequalities in accessing) different types of resources and their ability in transforming these resources to other types of resources is what interests me. The data I usually work with is the kind of data that is actually nicely covered by existing rOpenGov tools: data about population demographics and administrative units from Statistics Finland, statistical information on welfare and health from Sotkanet and also data from Eurostat. Aside from these a lot of information is of course data from surveys and texts scraped from the internet.\nWe are placing the growing number of rOpenGov tools in a modern application with a user-friendly service and a modern data API. In your ideal data world, what would be the ultimate dataset, or datasets that you would like to see in the Music Data Observatory?\nLate spring and early summer time is, at least for me, defined by the Eurovision Song Contest. Every year watching the contest makes me ponder the state of the music industry in my home country Finland as well as in Europe. Was the song produced by homegrown talent or was it imported? Was it better received by the professional jury or the public? How well does the domestic appeal of an artist translate to the international stage? Many interesting phenomena are difficult to quantify in a meaningful way and writing a catchy song with international appeal is probably more an art than a science. Nevertheless that should not deter us from trying as music, too, is bound by certain rules and regularities that can be researched.\nMusic, too, is bound by certain rules and regularities that can be researched. Our Digital Music Observatory and its Listen Local experimental App does this exactly, and we would love to create Eurovision musicology datasets. Photo: Eurovision Song Contest 2021 press photo by Jordy Brada Why did you decide to join the EU Datathon challenge team and why do you think that this would be a game changer for researchers and policymakers?\nThe challenge has, in my opinion, great potential in leading by example when it comes to open data access and reproducible research. Comparing data to oil is a common phrase but fitting in the sense that crude oil has to go through a number of steps and pipes before it becomes useful. Most users and especially policymakers appreciate ease-of-use of the finished product, but the quality of the product and the process must also be guaranteed somehow. Openness and peer-review practices are the best guarantors in the field of data, just as industrial standards and regulations are in the oil industry.\nWe provide many layers of fully transparent quality control about the data we are placing in our data APIs and provide for our end-users. Join us Join our open collaboration Music Data Observatory team as a data curator, developer or business developer. More interested in antitrust, innovation policy or economic impact analysis? Try our Economy Data Observatory team! Or your interest lies more in climate change, mitigation or climate action? Check out our Green Deal Data Observatory team!\n","date":1623060000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623060000,"objectID":"fdb475346ab7d01f9ead6530e48ade00","permalink":"https://surveyharmonies.eu/post/2021-06-07-data-curator-pyry-kantanen/","publishdate":"2021-06-07T10:00:00Z","relpermalink":"/post/2021-06-07-data-curator-pyry-kantanen/","section":"post","summary":"Many interesting phenomena are difficult to quantify in a meaningful way and writing a catchy song with international appeal is probably more an art than a science. Nevertheless that should not deter us from trying as music, too, is bound by certain rules and regularities that can be researched.","tags":["Open data","Open science","reproducible research","open government"],"title":"Comparing Data to Oil is a Cliché: Crude Oil Has to Go Through a Number of Steps and Pipes Before it Becomes Useful","type":"post"},{"authors":["leo_lahti"],"categories":null,"content":"As a developer at rOpenGov, what type of data do you usually use in your work?\nAs an academic data scientist whose research focuses on the development of general-purpose algorithmic methods, I work with a range of applications from life sciences to humanities. Population studies play a big role in our research, and often the information that we can draw from public sources - geospatial, demographic, environmental - provides invaluable support. We typically use open data in combination with sensitive research data but some of the research questions can be readily addressed based on open data from statistical authorities such as Statistics Finland or Eurostat.\nIn your ideal data world, what would be the ultimate dataset, or datasets that you would like to see in the Music Data Observatory?\nOne line of our research analyses the historical trends and spread of knowledge production, in particular book printing based on large-scale metadata collections. It would be interesting to extend this research to music, to understand the contemporary trends as well as the broader historical developments. Gaining access to a large systematic collection of music and composition data from different countries across long periods of time would make this possible.\nWhy did you decide to join the challenge and why do you think that this would be a game changer for researchers and policymakers?\nJoining the challenge was a natural development based on our overall activities in this area; the rOpenGov project has been around for a decade now, since the early days of the broader open data movement. This has also created an active international developer network and we felt well equipped for picking up the challenge. The game changer for researchers is that the project highlights the importance of data quality, even when dealing with official statistics, and provides new methods to solve these issues efficiently through the open collaboration model. For policymakers, this provides access to new high-quality curated data and case studies that can support evidence-based decision-making.\nDo you have a favorite, or most used open governmental or open science data source? What do you think about it? Could it be improved?\nRegarding open government data, one of my favorites is not a single data source but a data representation standard. The px format is widely used by statistical authorities in various countries, and this has allowed us to create R tools that allow the retrieval and analysis of official statistics from many countries across Europe, spanning dozens of statistical institutions. Standardization of open data formats allows us to build robust algorithmic tools for downstream data analysis and visualization. Open government data is still too often shared in obscure, non-standard or closed-source file formats and this is creating significant bottlenecks for the development of scalable and interoperable AI and machine learning methods that can harness the full potential of open data.\nRegarding open government data, one of my favorites is not a single data source but a data representation standard, the Px format. From your perspective, what do you see being the greatest problem with open data in 2021?\nAlthough there are a variety of open data sources available (and the numbers continue to increase), the availability of open algorithmic tools to interpret and communicate open data efficiently is lagging behind. One of the greatest challenges for open data in 2021 is to demonstrate how we can maximize the potential of open data by designing smart tools for open data analytics.\nWhat can our automated data observatories do to make open data more credible in the European economic policy community and be accepted as verified information?\nThe role of the professional network backing up the project, and the possibility of getting critical feedback and later adoption by the academic communities will support the efforts. Transparency of the data harmonization operations is the key to credibility, and will be further supported by concrete benchmarks that highlight the critical differences in drawing conclusions based on original sources versus the harmonized high-quality data sets.\nWe need to get critical feedback and later adoption by the academic communities. How we can ensure the long-term sustainability of the efforts?\nThe extent of open data space is such that no single individual or institution can address all the emerging needs in this area. The open developer networks play a huge role in the development of algorithmic methods, and strong communities have developed around specific open data analytical environments such as R, Python, and Julia. These communities support networked collaboration and provide services such as software peer review. The long-term sustainability will depend on the support that such developer communities can receive, both from individual contributors as well as from institutions and governments.\nJoin us Join our …","date":1622800800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622800800,"objectID":"fcf382863b28f6f8f721290cf2d05608","permalink":"https://surveyharmonies.eu/post/2021-06-04-developer-leo-lahti/","publishdate":"2021-06-04T10:00:00Z","relpermalink":"/post/2021-06-04-developer-leo-lahti/","section":"post","summary":"Although there are a variety of open data sources available (and the numbers continue to increase), the availability of open algorithmic tools to interpret and communicate open data efficiently is lagging behind. One of the greatest challenges for open data in 2021 is to demonstrate how we can maximize the potential of open data by designing smart tools for open data analytics.","tags":["Open data","Open science","reproducible research","open government"],"title":"Creating Algorithmic Tools to Interpret and Communicate Open Data Efficiently","type":"post"},{"authors":["Daniel Antal"],"categories":["R-bloggers"],"content":"We have released a new version of iotables as part of the rOpenGov project. The package, as the name suggests, works with European symmetric input-output tables (SIOTs). SIOTs are among the most complex governmental statistical products. They show how each country’s 64 agricultural, industrial, service, and sometimes household sectors relate to each other. They are estimated from various components of the GDP, tax collection, at least every five years.\nThis code tutorial is not outdated, but the iotables R package has a new release with more environmental impact analysis featues. Click to expand table of contents of the post Table of Contents Accessing and tidying the data programmatically Example Vignettes Environmental Impact Analysis rOpenGov and the EU Datathon Challenges SIOTs offer great value to policy-makers and analysts to make more than educated guesses on how a million euros, pounds or Czech korunas spent on a certain sector will impact other sectors of the economy, employment or GDP. What happens when a bank starts to give new loans and advertise them? How is an increase in economic activity going to affect the amount of wages paid and and where will consumers most likely spend their wages? As the national economies begin to reopen after COVID-19 pandemic lockdowns, is to utilize SIOTs to calculate direct and indirect employment effects or value added effects of government grant programs to sectors such as cultural and creative industries or actors such as venues for performing arts, movie theaters, bars and restaurants.\nMaking such calculations requires a bit of matrix algebra, and understanding of input-output economics, direct, indirect effects, and multipliers. Economists, grant designers, policy makers have those skills, but until now, such calculations were either made in cumbersome Excel sheets, or proprietary software, as the key to these calculations is to keep vectors and matrices, which have at least one dimension of 64, perfectly aligned. We made this process reproducible with iotables and eurostat on rOpenGov\nOur iotables package creates direct, indirect effects and multipliers programatically. Our observatory will make those indicators available for all European countries. Accessing and tidying the data programmatically The iotables package is in a way an extension to the eurostat R package, which provides a programmatic access to the Eurostat data warehouse. The reason for releasing a new package is that working with SIOTs requires plenty of meticulous data wrangling based on various metadata sources, apart from actually accessing the data itself. When working with matrix equations, the bar is higher than with tidy data. Not only your rows and columns must match, but their ordering must strictly conform the quadrants of the a matrix system, including the connecting trade or tax matrices.\nWhen you download a country’s SIOT table, you receive a long form data frame, a very-very long one, which contains the matrix values and their labels like this:\n## Table naio_10_cp1700 cached at C:\\Users\\...\\Temp\\RtmpGQF4gr/eurostat/naio_10_cp1700_date_code_FF.rds # we save it for further reference here saveRDS(naio_10_cp1700, \u0026#34;not_included/naio_10_cp1700_date_code_FF.rds\u0026#34;) # should you need to retrieve the large tempfiles, they are in dir (file.path(tempdir(), \u0026#34;eurostat\u0026#34;)) dplyr::slice_head(naio_10_cp1700, n: 5) ## # A tibble: 5 x 7 ## unit stk_flow induse prod_na geo time values ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 MIO_EUR DOM CPA_A01 B1G EA19 2019-01-01 141873. ## 2 MIO_EUR DOM CPA_A01 B1G EU27_2020 2019-01-01 174976. ## 3 MIO_EUR DOM CPA_A01 B1G EU28 2019-01-01 187814. ## 4 MIO_EUR DOM CPA_A01 B2A3G EA19 2019-01-01 0 ## 5 MIO_EUR DOM CPA_A01 B2A3G EU27_2020 2019-01-01 0 The metadata reads like this: the units are in millions of euros, we are analyzing domestic flows, and the national account items B1-B2 for the industry A01. The information of a 64x64 matrix (the SIOT) and its connecting matrices, such as taxes, or employment, or C**O2 emissions, must be placed exactly in one correct ordering of columns and rows. Every single data wrangling error will usually lead in an error (the matrix equation has no solution), or, what is worse, in a very difficult to trace algebraic error. Our package not only labels this data meaningfully, but creates very tidy data frames that contain each necessary matrix of vector with a key column.\niotables package contains the vocabularies (abbreviations and human readable labels) of three statistical vocabularies: the so called COICOP product codes, the NACE industry codes, and the vocabulary of the ESA2010 definition of national accounts (which is the government equivalent of corporate accounting).\nOur package currently solves all equations for direct, indirect effects, multipliers and inter-industry linkages. Backward linkages show what happens with the suppliers of an industry, such as catering or advertising in the case of music festivals, if the …","date":1622736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661857260,"objectID":"fc5992af6c8de60c64b311b09922b493","permalink":"https://surveyharmonies.eu/post/2021-06-03-iotables-release/","publishdate":"2021-06-03T16:00:00Z","relpermalink":"/post/2021-06-03-iotables-release/","section":"post","summary":"rOpenGov, Reprex, and other open collaboration partners teamed up to build on our expertise of open source statistical software development further: we want to create a technologically and financially feasible data-as-service to put our reproducible research products into wider user for the business analyst, scientific researcher and evidence-based policy design communities. Our new release will help with automated economic impact and environmental impact analysis.","tags":["Open data","Open science","iotables","Economic Impact Analysis","Enviromental Impact Analysis"],"title":"Economic and Environment Impact Analysis, Automated for Data-as-Service","type":"post"},{"authors":["Daniel Antal","botond_vitos"],"categories":null,"content":"Our observatory has a new data API which allows access to our daily refreshing open data. You can access the API via api.economy.dataobservatory.eu (apologies for the ugly, temporary subdomain masking!).\nAll the data and the metadata are available as open data, without database use restrictions, under the ODbL license. However, the metadata contents are not finalized yet. We are currently working on a solution that applies the FAIR Guiding Principles for scientific data management and stewardship, and fulfills the mandatory requirements of the Dublic Core metadata standards and at the same time the mandatory requirements, and most of the recommended requirements of DataCite. These changes will be effective before 1 July 2021.\nThe Competition Data Observatory temporarily shares an API with the Economy Data Observatory, which serves as an incubator for similar economy-oriented reproducible research resources.\nIndicator table The indicator table contains the actual values, and the various estimated/imputed values of the indicator, clearly marking missing values, too.\napi.economy.dataobservatory.eu: indicator retrieval You can get the data in CSV or json format, or write SQL querries. (Tutorials in SQL, R, Python will be posted shortly.)\nDescription metadata table Processing Metadata table The metadata table contains various data processing information, such as the first and last actual observation of the indicator, the number of approximated, forecasted, backcasted values, last update at source and in our system, and so on.\napi.economy.dataobservatory.eu: processing metadata Authoritative Copies Greendeal Data Observatory on Zenodo\n","date":1622545200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625655600,"objectID":"1fa58637f02a8941c5b61274a0207f37","permalink":"https://surveyharmonies.eu/data/api/","publishdate":"2021-06-01T11:00:00Z","relpermalink":"/data/api/","section":"data","summary":"Get data from the Competition Data Observatory via our API","tags":["api"],"title":"Data API","type":"data"},{"authors":["Daniel Antal"],"categories":null,"content":"Our observatory has a new data API which allows access to our daily refreshing open data. You can access the API via api.economy.dataobservatory.eu (apologies for the ugly, temporary subdomain masking!).\nAll the data and the metadata are available as open data, without database use restrictions, under the ODbL license. However, the metadata contents are not finalized yet. We are currently working on a solution that applies the FAIR Guiding Principles for scientific data management and stewardship, and fulfills the mandatory requirements of the Dublic Core metadata standards and at the same time the mandatory requirements, and most of the recommended requirements of DataCite. These changes will be effective before 1 July 2021.\nThe Competition Data Observatory temporarily shares an API with the Economy Data Observatory, which serves as an incubator for similar economy-oriented reproducible research resources.\napi.economy.dataobservatory.eu: processing metadata Descriptive Metadata Identifier An unambiguous reference to the resource within a given context. (Dublin Core item), but several identifiders allowed, and we will use several of them. Creator The main researchers involved in producing the data, or the authors of the publication, in priority order. To supply multiple creators, repeat this property. (Extends the Dublin Core with multiple authors, and legal persons, and adds affiliation data.) Title A name given to the resource. Extends Dublin Core with alternative title, subtitle, translated Title, and other title(s). Publisher The name of the entity that holds, archives, publishes prints, distributes, releases, issues, or produces the resource. This property will be used to formulate the citation, so consider the prominence of the role. For software, use Publisher for the code repository. (Dublin Core item.) Publication Year The year when the data was or will be made publicly available. Resource Type We publish Datasets, Images, Report, and Data Papers. (Dublin Core item with controlled vocabulary.) Recommended for discovery The Recommended (R) properties are optional, but strongly recommended for interoperability.\nSubject The topic of the resource. (Dublin Core item.) Contributor The institution or person responsible for collecting, managing, distributing, or otherwise contributing to the development of the resource. (Extends the Dublin Core with multiple authors, and legal persons, and adds affiliation data.) When applicable, we add Distributor (of the datasets and images), Contact Person, Data Collector, Data Curator, Data Manager, Hosting Institution, Producer (for images), Project Manager, Researcher, Research Group, Rightsholder, Sponsor, Supervisor Date A point or period of time associated with an event in the lifecycle of the resource, besides the Dublin Core minimum we add Collected, Created, Issued, Updated, and if necessary, Withdrawn dates to our datasets. Related Identifier An identifier or identifiers other than the primary Identifier applied to the resource being registered. Rights We give SPDX License List standards rights description with URLs to the actual license. (Dublin Core item: Rights Management) Description Recommended for discovery.(Dublin Core item.) GeoLocation Similar to Dublin Core item Coverage The Subject property: we need to set standard coding schemas for each observatory. Contributor property: DataCurator the curator of the dataset, who sets the mandatory properties. DataManager the person who keeps the dataset up-to-date. ContactPerson the person who can be contacted for reuse requests or bug reports. The Date property contains the following dates, which are set automatically by the dataobservatory R package: Updated when the dataset was updated; EarliestObservation, which the earliest, not backcasted, estimated or imputed observation. LatestObservation, which the earliest, not backcasted, estimated or imputed observation. UpdatedatSource, when the raw data source was last updated. The GeoLocation is automatically created by the dataobservatory R package. The Description property optional elements, and we adopted them as follows for the observatories: The Abstract is a short, textual description; we try to automate its creation as much as a possible, but some curatorial input is necessary. In the TechnicalInfo sub-field, we record automatically the utils::sessionInfo() for computational reproducability. This is automatically created by the dataobservatory R package. In the Other sub-field, we record the keywords for structuring the observatory. Optional The Optional (O) properties are optional and provide richer description. For findability they are not so important, but to create a web service, they are essential. In the mandatory and recommended fields, we are following other metadata standards and codelists, but in the optional fields we have to build up our own system for the observatories.\nLanguage A language of the resource. (Dublin Core item.) Alternative …","date":1622545200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625735400,"objectID":"882a6a70ec8965c6a0246cec45271324","permalink":"https://surveyharmonies.eu/data/metadata/","publishdate":"2021-06-01T11:00:00Z","relpermalink":"/data/metadata/","section":"data","summary":"Uncut diamonds need to be cut, polished, and you have to make sure that they come from a legal source.","tags":["metadata"],"title":"Metadata","type":"data"},{"authors":null,"categories":null,"content":"we would like to actively encourage the sharing of data assets.\n","date":1621123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621123200,"objectID":"ebd4557c0a6e81227426e984ce16d9a4","permalink":"https://surveyharmonies.eu/data/data-sharing/","publishdate":"2021-05-16T00:00:00Z","relpermalink":"/data/data-sharing/","section":"data","summary":"Data altruismm, sharing, and collaborative data resources.","tags":["data-sharing","data-altruism"],"title":"Data Sharing","type":"data"},{"authors":null,"categories":null,"content":"Many countries in the world allow access to a vast array of information, such as documents under freedom of information requests, statistics, datasets. In the European Union, most taxpayer financed data in government administration, transport, or meteorology, for example, can be usually re-used. More and more scientific output is expected to be reviewable and reproducible, which implies open access.\nWhat’s the Problem with Open Data? How We Add Value? Is There Value in It? If it’s money on the street, why nobody’s picking it up? Datasets Should Work Together to Give InformationData is only potential information, raw and unprocessed. What’s the Problem with Open Data? “Data is stuff. It is raw, unprocessed, possibly even untouched by human hands, unviewed by human eyes, un-thought-about by human minds.” [1]\nMost open data cannot be just “downloaded.” Often, you need to put more than $100 value of work into processing, validating, documenting a dataset that is worth $100. But you can share this investment with our data observatories. Open data is almost always lacking of documentation, and no clear references to validate if the data is reliable or not corrupted. This is why we always start with reprocessing and redocumenting. Our review of about 80 EU, UN and OECD data observatories reveals that most of them do not use these organizations’s open data - instead they use various, and often not well processed proprietary sources. Read more: Open Data - The New Gold Without the Rush\nHow We Add Value? We believe that even such generally trusted data sources as Eurostat often need to be reprocessed, because various legal and political constraints do not allow the common European statistical services to provide optimal quality data – for example, on the regional and city levels. With rOpenGov and other partners, we are creating open-source statistical software in R to re-process these heterogenous and low-quality data into tidy statistical indicators to automatically validate and document it. Metadata is a potentially informative data record about a potentially informative dataset. We are carefully documenting and releasing administrative, processing, and descriptive metadata, following international metadata standards, to make our data easy to find and easy to use for data analysts. We are automatically creating depositions and authoritative copies marked with an individual digital object identifier (DOI) to maintain data integrity. Is There Value in Open Data? A well-known story tells of a finance professor and a student who come across a $100 bill lying on the ground. As the student stops to pick it up, the professor says, “Don’t bother—if it were really a $100 bill, it wouldn’t be there.”\nBut this is not the case with open data. Often, you need to put more than $100 into processing, validating, documenting a dataset that is worth $100.\nIn the EU, open data is governed by the Directive on open data and the re-use of public sector information - in short: Open Data Directive (EU) 2019 / 1024. It entered into force on 16 July 2019. It replaces the Public Sector Information Directive, also known as the PSI Directive which dated from 2003 and was subsequently amended in 2013.\nOpen Data is potentially useful data that can potentially replace costlier or hard to get data sources to build information. It is analogous to potential energy: work is required to release it. We build automated systems that reduce this work and increase the likelihood that open data will offer the best value for money.\nMost open data is not publicy accessible, and available upon request. Our real curatorial advantage is that we know where it is and how to get this request processed. Most European open data comes from tax authorities, meteorological offices, managers of transport infrastructure, and other governmental bodies whose data needs are very different from yours. Their data must be carefully evaluated, re-processed, and if necessary, imputed to be usable for your scientific, business or policy goals. The use of open science data is problematic in different ways: usually understanding the data documentation requires domain-specific specialist knowledge. Open science data is even more scattered and difficult to access than technically open, but not public governmental data. From Datasets to Data Integration, Data to Information “Data is only potential information, raw and unprocessed, prior to anyone actually being informed by it.” ^[2]\nWe are building simple databases and supporting APIs that release the data without restrictions, in a tidy format that is easy to join with other data, or easy to join into databases, together with standardized metadata. Our service flow and value chain FAQ Why Downloading Does Not Work? Most open data is not available on the internet. If it is available, it is not in a form that you can easily import into a spreadsheet application like Excel or OpenOffice, or into a statistical application like SPSS or …","date":1621123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624633200,"objectID":"c02f0e35bf46fd958fd3ec2bd0a929fb","permalink":"https://surveyharmonies.eu/data/open-gov/","publishdate":"2021-05-16T00:00:00Z","relpermalink":"/data/open-gov/","section":"data","summary":"Many countries in the world allow access to a vast array of information, such as documents under freedom of information requests, statistics, datasets. In the European Union, most taxpayer financed data in government administration, transport, or meteorology, for example, can be usually re-used. More and more scientific output is expected to be reviewable and reproducible, which implies open access.","tags":["open-data","FOI","PSI"],"title":"Open Data","type":"data"},{"authors":["SurveyHarmonies","Daniel Antal"],"categories":null,"content":"In our use case we are merging data about Europe’s coal regions, harmonized surveys about the acceptance of climate policies, and socio-economic data. While the work starts out from existing European research, our retroharmonize survey harmonization solution, our regions sub-national boundary harmonization solution and iotables allows us to connect open data and open knowledge from other coal regions of the world, for example, from the Appalachian economy.\nPolicy Context The Just Transition Platform aims to assist EU countries and regions to unlock the support available through the Just Transition Mechanism. It builds on and expands the work of the existing Initiative for Coal Regions in Transition, which already supports fossil fuel producing regions across the EU in achieving a just transition through tailored, needs-oriented assistance and capacity-building.\nThe Initiative has a secretariat that is co-run by Ecorys, Climate Strategies, ICLEI Europe, and the Wuppertal Institute for Climate. While the initiative is an EU project, it cooperates with other similar initiatives, for example, with the Coalfield Development social enterprise in the Appalachian economy.\nData Sources Coal regions: Our starting point is the EU coal regions: opportunities and challenges ahead publication Joint Research Centre (JRC), the European Commission’s science and knowledge service. This publication maps Europe’s coal dependent energy and transport infrastructure, and regions that depend on coal-related jobs.\nHarmonized Survey Data: The dataset of the Eurobarometer 91.3 (April 2019) harmonized survey. Our transition policy variable is the four-level agreement with the statement More public financial support should be given to the transition to clean energies even if it means subsidies to fossil fuels should be reduced (EN) and Davantage de soutien financier public devrait être donné à la transition vers les énergies propres même si cela signifie que les subventions aux énergies fossiles devraient être réduites (FR) which is then translated to the language use of all participating country.\nEnvironmental Variables: We used data on pm and SO2 polution measured by participating stations in the European Environmental Agency’s monitoring program. The station locations were mapped by Milos to the NUTS sub-national regions.\nExploratory Data Analysis Our coal-dependency dummy variable is base on the policy document Coal regions in transition.\nreadRDS(file.path(\u0026#34;data\u0026#34;, \u0026#34;coal_regions.rds\u0026#34;)) ## # A tibble: 253 x 5 ## country_code_is~ region_nuts_nam~ region_nuts_cod~ coal_region is_coal_region ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 BE Brussels hoofds~ BE10 \u0026lt;NA\u0026gt; 0 ## 2 BE Liege BE33 \u0026lt;NA\u0026gt; 0 ## 3 BE Brabant Wallon BE31 \u0026lt;NA\u0026gt; 0 ## 4 BE Antwerpen BE21 \u0026lt;NA\u0026gt; 0 ## 5 BE Limburg [BE] BE22 \u0026lt;NA\u0026gt; 0 ## 6 BE Oost-Vlaanderen BE23 \u0026lt;NA\u0026gt; 0 ## 7 BE Vlaams Brabant BE24 \u0026lt;NA\u0026gt; 0 ## 8 BE West-Vlaanderen BE25 \u0026lt;NA\u0026gt; 0 ## 9 BE Hainaut BE32 \u0026lt;NA\u0026gt; 0 ## 10 BE Namur BE35 \u0026lt;NA\u0026gt; 0 ## # ... with 243 more rows Our exploratory data analysis shows that respondent in 2019, agreement with the policy measure significantly differed among EU member states and regions.\ntransition_policy \u0026lt;- eb19_raw %\u0026gt;% rowid_to_column() %\u0026gt;% mutate ( transition_policy = normalize_text(transition_policy)) %\u0026gt;% fastDummies::dummy_cols(select_columns = \u0026#39;transition_policy\u0026#39;) %\u0026gt;% mutate ( transition_policy_agree = case_when( transition_policy_totally_agree + transition_policy_tend_to_agree \u0026gt; 0 ~ 1, TRUE ~ 0 )) %\u0026gt;% mutate ( transition_policy_disagree = case_when( transition_policy_totally_disagree + transition_policy_tend_to_disagree \u0026gt; 0 ~ 1, TRUE ~ 0 )) eb19_df \u0026lt;- transition_policy %\u0026gt;% left_join ( air_pollutants, by = \u0026#39;region_nuts_codes\u0026#39; ) %\u0026gt;% mutate ( is_poland = ifelse ( country_code == \u0026#34;PL\u0026#34;, 1, 0)) Preliminary Results Significantly more people agree where\nthere are more polutants who are younger where people are more educated Significantly less people agree\nin rural areas where more people are older where more people are less educated in less polluted areas in coal regions A simple model run:\nc(\u0026#34;transition_policy_totally_agree\u0026#34; , \u0026#34;pm10\u0026#34;, \u0026#34;so2\u0026#34;, \u0026#34;age_exact\u0026#34;, \u0026#34;is_highly_educated\u0026#34; , \u0026#34;is_rural\u0026#34;) ## [1] \u0026#34;transition_policy_totally_agree\u0026#34; \u0026#34;pm10\u0026#34; ## [3] \u0026#34;so2\u0026#34; \u0026#34;age_exact\u0026#34; ## [5] \u0026#34;is_highly_educated\u0026#34; \u0026#34;is_rural\u0026#34; summary( glm ( transition_policy_totally_agree ~ pm10 + so2 + age_exact + is_highly_educated + is_rural + is_coal_region + country_code, data = eb19_df, family = binomial )) ## ## Call: ## glm(formula = transition_policy_totally_agree ~ pm10 + so2 + ## age_exact + is_highly_educated + is_rural + is_coal_region + ## country_code, family = binomial, data = eb19_df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7690 -1.0253 -0.8165 1.2264 1.9085 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -0.1975096 0.0921551 -2.143 0.032095 * ## pm10 0.0068505 0.0017445 3.927 8.60e-05 *** ## so2 0.1381994 0.0405867 3.405 0.000662 *** ## age_exact -0.0075018 0.0007873 -9.529 \u0026lt; 2e-16 *** ## …","date":1615852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615852800,"objectID":"76bbab8f6e17ec9b2c049461abf4b1e3","permalink":"https://surveyharmonies.eu/publication/political-roadblocks/","publishdate":"2021-03-16T00:00:00Z","relpermalink":"/publication/political-roadblocks/","section":"publication","summary":"Our first use case is about identifying potential political roadblocks for climate policies.  We are combining survey data about attitudes to climate change policies with socio-economic coal mining and and voting data.  We examine the relationship between voter attitudes and economic dependency on coal mining.","tags":["coal","use-case","regions"],"title":"Identifying Roadblocks to Net Zero Legislation","type":"publication"},{"authors":["Daniel Antal","milos_popovic"],"categories":null,"content":"If you live in a polluted area, does it mean that you take climate change seriously? Following the Reprex Open Data Day 2021, we embarked on a quest to explore this question using a unique combination of micro-level data from Eurobarometer surveys, Eurostat’s sub-national socio-economic data and satellite imagery from European Environmental Agency (EEA) and NASA. Before venturing forth into the forest of open data we, as all visual creatures out there, first mapped the road ahead.\nWe used three sensory sources on pollution and deforestation, all of which are closely related to environmental degradation, to create these maps. In the first set of maps, we draw on EEA’s Air Quality e-Reporting data on environmental pollution (particulate matter 2.5 and 10) for the period 2014–2016. What makes these data complex is their organization on the level of the reporting stations. So, this means that we had to first figure out the nearest aerial distance from every reporting station to local administrative unit (LAU), assign the annual pollution levels to every LAU and, finally, create our fine-grained map. Using this approach, we are able to aggregate the data to any NUTS level and, with help of the retroharmonize and regions R packages, work with public opinion and sub-national data to tackle our initial question.\nBelow you will notice that findings are constrained to countries for which EEA commonly collects environmental data. Far from being Euro-centric, our project is inclusive of other countries and continents for which the pollution data is available – with the aforementioned packages we could work with any nation’s or larger regions data. In fact, we would like to invite contributors with greater knowledge of reliable data sources from all continents.\nOur joined dataset allows hypothesis testing on how much people’s perception and attitudes to environmental degradation depends on the quality of the environment that surrounds them. Our joined dataset allows hypothesis testing on how much people’s perception and attitudes to environmental degradation depends on the quality of the environment that surrounds them. In the next map, we go beyond the EU/EEA/EU candidate focus to depict light pollution for the whole European continent. We used the Harmonized VIIRS nighttime light data for 2014–2018, which is a novel open source with calibrated global information on nightlight. This outstanding source offers an unparalleled opportunity to measure the intensity of the socioeconomic activities and urbanization. We showcase this in our map of estimated average size of urban areas for every LAU using DN values higher than 30. This is a tip of an iceberg as our mapping capabilities may extend to any available subnational data around the globe.\nThe VIIRS nighttime light dataset excels particularly in countries and regions where GDP estimation and desagregation is patchy or non-existent. We would like to find collaborators from Africa, the Arab World, the Caucasus and Latin America, where we have harmonized, individual level survey data and socio-econometric data, to join forces with us to build relevant sub-national regional dictionaries for the regions package, which can do the rest of the work.\nNighttime lights are accurate predictors of local income, energy use and contribution to carbon emissions. In the final map, we use the Copernicus Tree Cover Density dataset to compute how much deforestation has taken place on the LAU level in Europe between 2015 and 2019. Using our regions package, these data could easily be paired with public opinion and NUTS-level data to analyze how deforestation influences individual attitudes on climate change.\nDeforestration is a key factor in carbon emission, because trees store so much carbon. Any path to net zero carbon emission requires a vast re-forestration of the Earth. As we can see, in most of Europe deforestation is ongoing. This is partly caused by effects of climate change, but partly further aggravate the situation as the fallen trees release previously captured CO2. For example, in Slovakia the Tatra mountains lost many trees in a devastating storm; such extreme weather conditions kill vulnerable tree cover, leading to soil errosion. Again, the Copernicus tree cover data available for the entire Earth, and our regions package only requires local geocoding and geographical vocabulary additions to allow analysis on almost all continents.\nAll this artwork barely scratches the surface of possibilities that mapping sensoring data could offer to NGOs, think-tanks, small enterprises as well as academic institutions. Most importantly, this powerful approach could help these actors effectively link patterns in environmental change to individual attitudes and subnational socio-economic data.\n","date":1615420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615420800,"objectID":"e7a419d682d3aa3ade2b176e012edb02","permalink":"https://surveyharmonies.eu/post/2021-03-11-environmental_data/","publishdate":"2021-03-11T00:00:00Z","relpermalink":"/post/2021-03-11-environmental_data/","section":"post","summary":"Using sub-national geographical information, we can connect air pollution measurements and satellite images of light pollution or deforestration with social and economic indicators, and even individual opinion polling data.","tags":["environmental data","nightlight","trees"],"title":"Connecting the Dots to Environmental Degradation Open Data","type":"post"},{"authors":[],"categories":null,"content":"Open Data Day is an annual celebration of open data all over the world. It is an opportunity to show the benefits of open data and encourage the adoption of open data policies in government, business, and civil society. Reprex is a start-up that utilizes open data with open-source reproducible research: please challenge us with your data requests and participate in our web events.\nThe Reprex Open Data Day 2021 will be two informal conversations based on a series of run up introductory blogposts centered around two themes. Because important guests became ill in the last days, we are going to consolidate the two talks into one with less structure. We want to create an informal, inclusive, collaborative online event on International Open Data Day 2021. Please, grab a tea, coffee, or even a beer, and join us for an informal conversation. We hope that we will finish the afternoon with ideas on new, open-data driven collaborations.\n9.30 EST / 15.30 CET: Open collaboration in business, policy and science. Creating evidence-based policy, business strategy or scientific research with small contributions with independent components with incentives. Short introduction with examples: joining environmental sensory data and public opinion data on maps; creating harmonized datasets across the Arab world. Survey harmonization, mapping, data products. Scaling up open collaboration: making small organizations competitive with big tech in the big data era. Data sharing, data pooling, data altruism and observatories. The new European trustworthy AI and data governance agenda.\nYou can click through a short presentation to familiarize yourself with our topics.\nSee you here.\nCase studies:\nWe are connecting raw survey data about Climate Awareness in Eurobarometer surveys. Here is the reproduction code (intermediate to advanced R needed.) You should use the development version of our retroharmonize package at github.com/antaldaniel/retroharmonize\nWe are tracking changes in the boundaries of provinces, states, counties, parishes with our regions open source software – reproduction code here. You will need our regions package which is available on CRAN or in the rOpenGov GitHub repo.\nWe will talk about how to join this with air pollution data and put it on the map with Milos Popovic, who prepared this nice choropleth animation.\nWe will discuss data observatories (permanent data collection programs), open collaboration (open-source inspired way of cooperation among small and large independent actors) and data altruism. Any questions: send Daniel a message on Keybase, Whatsapp or email.\nHello on International #OpenDataDay2021 from🌷 the Hague!\n- We have brought some new data to the light about 🌡climate change awareness - We created some tutorials how to harmonize survey and geographical data\n- Join us at 9.30 EST/15.30 CET 👇https://t.co/7J7pvi3sPC #ODD2021 pic.twitter.com/DwkGQaDhW1\n— dataandlyrics (@dataandlyrics) March 6, 2021 ","date":1615037400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615037400,"objectID":"ecc73affcfc1427fc7b6300d0b5e126b","permalink":"https://surveyharmonies.eu/talk/reprex-open-data-day-2021/","publishdate":"2021-02-03T10:10:00Z","relpermalink":"/talk/reprex-open-data-day-2021/","section":"event","summary":"Open Data Day 2021 focusing on environmental, sustainability and public spending data mapping.","tags":["open-data"],"title":"Reprex Open Data Day 2021","type":"event"},{"authors":["Daniel Antal"],"categories":["R-bloggers"],"content":"library(regions) library(lubridate) library(dplyr) if ( dir.exists(\u0026#39;data-raw\u0026#39;) ) { data_raw_dir \u0026lt;- \u0026#34;data-raw\u0026#34; } else { data_raw_dir \u0026lt;- file.path(\u0026#34;..\u0026#34;, \u0026#34;..\u0026#34;, \u0026#34;data-raw\u0026#34;) } Going beyond the national level Let’s start with a dirty averaging by sub-national unit. The w1 weighting variable contains the post-stratification weight for the national samples. The Eurobarometer samples represent nations (with the exception of East and West Germany, Northern Ireland and Great Britain.) The average of the w1 variable is 1.00 for each sample, but it is not necessarily 1 for smaller territorial units. If sum(w)\u0026gt;1 for say, AT23 it only means that the AT23 region was undersampled relatively to the rest of Austria, and responses must be over-weighted in post-stratification.\nThere is no way to make the samples become regionally representative, and a correct post-stratification would require further data about the sampel design. But we can simply adjust to over/undersampling by making sure that oversampled territorial averages are proportionally increased and undersampled ones are decreased. [Another ‘dirty’ averaging would be the use of an unweighted average, but our method is better, because it more-or-less adjusts gender and education level biases, but leaves intra-country regional biases in the sample.]\npanel \u0026lt;- readRDS((file.path(data_raw_dir, \u0026#34;climate-panel.rds\u0026#34;))) climate_data \u0026lt;- panel %\u0026gt;% mutate ( year: lubridate::year(date_of_interview)) %\u0026gt;% select ( all_of(c(\u0026#34;isocntry\u0026#34;, \u0026#34;geo\u0026#34;, \u0026#34;w1\u0026#34;)), contains(\u0026#34;problem\u0026#34;) ) %\u0026gt;% mutate ( # use the post-stratification weights for national samples serious_world_problems_first: w1*serious_world_problems_first , serious_world_problems_climate_change: w1*serious_world_problems_climate_change) %\u0026gt;% group_by ( .data$geo ) %\u0026gt;% summarise( serious_world_problems_first: mean(serious_world_problems_first, na.rm=TRUE), serious_world_problems_climate_change: mean (serious_world_problems_climate_change, na.rm=TRUE), mean_w1: mean(w1) ) %\u0026gt;% mutate ( # adjust for post-stratification weight bias due to regional over/undersampling climate_first: serious_world_problems_first / mean_w1, climate_mentioned: serious_world_problems_climate_change / mean_w1 ) So, we averaged, weighted and adjusted the mentioning of climate change as the world’s most serious, or one of the most serious problems by NUTS regions.\nAggregation level The problem is that most statistical data is available in for the NUTS regional boundaries according to the NUTS2016 definition. However, GESIS uses NUTS2013 regions, so 252 regional codes in the four survey waves are invalid. Some data is available only on national level, but it can be projected to regional level, because small countries like Luxembourg have no regional divisions. Larger countries like Germany are divided only on state level (NUTS1), while small countries are divided on NUTS3 level.\nThis leads to various problems. Many data is available only on NUTS2 level, in which case NUTS1 data should be projected to its constituent smaller NUTS2 regions, and NUTS3 level data must be aggregated up to larger, containing NUTS2 levels.\nOf course, we also must choose if we use `NUTS2013 or NUTS2016 boundaries. Sub-national boundaries have changed many thousand times in the EU27 countries alone since 1999.\n## # A tibble: 5 x 2 ## validate n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 country 15 ## 2 invalid 252 ## 3 nuts_level_1 132 ## 4 nuts_level_2 452 ## 5 nuts_level_3 141 Recoding the Regions Our regions package was designed to keep track of sub-national regional boundary changes. It can validate regional data codes, and to some extent carry out recoding, imputation or simple aggregation.\nRecoding means that the boundaries are unchanged, but the country changed the names/codes of regions, because there were other boundary changes which did not affect our observation unit. Imputation must not be done with usual, general imputation tools, because our data is regionally structured. However, some imputations are very simple, because we can use equality equasions like MT: MT0, MT00. Often the boundary change is additive, and merged territorial units can simple aggregated for comparison in earlier data. regional_coding_2016 \u0026lt;- panel %\u0026gt;% mutate ( year: lubridate::year(date_of_interview)) %\u0026gt;% select ( all_of(c(\u0026#34;isocntry\u0026#34;, \u0026#34;geo\u0026#34;, \u0026#34;region\u0026#34;, \u0026#34;year\u0026#34;) ) ) %\u0026gt;% distinct_all() %\u0026gt;% recode_nuts() regional_coding_2013 \u0026lt;- panel %\u0026gt;% mutate ( year: lubridate::year(date_of_interview)) %\u0026gt;% select ( all_of(c(\u0026#34;isocntry\u0026#34;, \u0026#34;geo\u0026#34;, \u0026#34;region\u0026#34;, \u0026#34;year\u0026#34;) ) ) %\u0026gt;% distinct_all() %\u0026gt;% recode_nuts( nuts_year: 2013) climate_data_recoded \u0026lt;- climate_data %\u0026gt;% left_join ( regional_coding_2016, by: \u0026#39;geo\u0026#39; ) %\u0026gt;% left_join ( regional_coding_2013 %\u0026gt;% select ( all_of(c(\u0026#34;geo\u0026#34;, \u0026#34;code_2013\u0026#34;))), by: \u0026#34;geo\u0026#34;) %\u0026gt;% distinct_all() saveRDS ( climate_data_recoded , file.path(tempdir(), \u0026#34;climate_panel_recoded_agr.rds\u0026#34;), version: 2) # not evaluated saveRDS( climate_data_recoded , file: file.path(\u0026#34;data-raw\u0026#34;, \u0026#34;climate_panel_recoded_agr.rds\u0026#34;)) ","date":1614988800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614988800,"objectID":"ec13f61bf2ce95d552c409a57ad5f4d9","permalink":"https://surveyharmonies.eu/post/2021-03-06-regions-climate/","publishdate":"2021-03-06T00:00:00Z","relpermalink":"/post/2021-03-06-regions-climate/","section":"post","summary":"In the previous example we created a longitudional dataset that contains data on the attitudes European people in various countries, provinces and regions thought climate change was a serious world problem back in 2013, 2015, 2017 and 2019. We will now fix the geographical information for mapping.","tags":["retrospective-harmonization","surveys","climate-change","climate-awareness"],"title":"Regional Geocoding Harmonization Case Study - Regional Climate Change Awareness Datasets","type":"post"},{"authors":["Daniel Antal"],"categories":["R-bloggers"],"content":"library(regions) library(lubridate) library(dplyr) if ( dir.exists(\u0026#39;data-raw\u0026#39;) ) { data_raw_dir \u0026lt;- \u0026#34;data-raw\u0026#34; } else { data_raw_dir \u0026lt;- file.path(\u0026#34;..\u0026#34;, \u0026#34;..\u0026#34;, \u0026#34;data-raw\u0026#34;) } The first results of our longitudinal table were difficult to map, because the surveys used an obsolete regional coding. We will adjust the wrong coding, when possible, and join the data with the European Environment Agency’s (EEA) Air Quality e-Reporting (AQ e-Reporting) data on environmental pollution. We recoded the annual level for every available reporting stations [not shown here] and all values are in μg/m3. The period under observation is 2014-2016. Data file: https://www.eea.europa.eu/data-and-maps/data/aqereporting-8 (European Environment Agency 2021).\nRecoding the Regions Recoding means that the boundaries are unchanged, but the country changed the names and codes of regions because there were other boundary changes which did not affect our observation unit. We explain the problem and the solution in greater detail in our tutorial that aggregates the data on regional levels.\npanel \u0026lt;- readRDS((file.path(data_raw_dir, \u0026#34;climate-panel.rds\u0026#34;))) climate_data_geocode \u0026lt;- panel %\u0026gt;% mutate ( year: lubridate::year(date_of_interview)) %\u0026gt;% recode_nuts() Let’s join the air pollution data and join it by corrected geocodes:\nload(file.path(\u0026#34;data\u0026#34;, \u0026#34;air_pollutants.rda\u0026#34;)) ## good practice to use system-independent file.path climate_awareness_air \u0026lt;- climate_data_geocode %\u0026gt;% rename ( region_nuts_codes : .data$code_2016) %\u0026gt;% left_join ( air_pollutants, by: \u0026#34;region_nuts_codes\u0026#34; ) %\u0026gt;% select ( -all_of(c(\u0026#34;w1\u0026#34;, \u0026#34;wex\u0026#34;, \u0026#34;date_of_interview\u0026#34;, \u0026#34;typology\u0026#34;, \u0026#34;typology_change\u0026#34;, \u0026#34;geo\u0026#34;, \u0026#34;region\u0026#34;))) %\u0026gt;% mutate ( # remove special labels and create NA_numeric_ age_education: retroharmonize::as_numeric(age_education)) %\u0026gt;% mutate_if ( is.character, as.factor) %\u0026gt;% mutate ( # we only have responses from 4 years, and this should be treated as a categorical variable year: as.factor(year) ) %\u0026gt;% filter ( complete.cases(.) ) The climate_awareness_air data frame contains the answers of 75086 individual respondents. 17.07% thought that climate change was the most serious world problem and 33.6% mentioned climate change as one of the three most important global problems.\nsummary ( climate_awareness_air ) ## rowid serious_world_problems_first ## ZA5877_v2-0-0_1 : 1 Min. :0.0000 ## ZA5877_v2-0-0_10 : 1 1st Qu.:0.0000 ## ZA5877_v2-0-0_100 : 1 Median :0.0000 ## ZA5877_v2-0-0_1000 : 1 Mean :0.1707 ## ZA5877_v2-0-0_10000: 1 3rd Qu.:0.0000 ## ZA5877_v2-0-0_10001: 1 Max. :1.0000 ## (Other) :75080 ## serious_world_problems_climate_change isocntry ## Min. :0.000 BE : 3028 ## 1st Qu.:0.000 CZ : 3023 ## Median :0.000 NL : 3019 ## Mean :0.336 SK : 3000 ## 3rd Qu.:1.000 SE : 2980 ## Max. :1.000 DE-W : 2978 ## (Other):57058 ## marital_status age_education ## (Re-)Married: without children :13242 18 :15485 ## (Re-)Married: children this marriage :12696 19 : 7728 ## Single: without children : 7650 16 : 5840 ## (Re-)Married: w children of this marriage: 6520 still studying: 5098 ## (Re-)Married: living without children : 6225 17 : 5092 ## Single: living without children : 4102 15 : 4528 ## (Other) :24651 (Other) :31315 ## age_exact occupation_of_respondent ## Min. :15.0 Retired, unable to work :22911 ## 1st Qu.:36.0 Skilled manual worker : 6774 ## Median :51.0 Employed position, at desk : 6716 ## Mean :50.1 Employed position, service job: 5624 ## 3rd Qu.:65.0 Middle management, etc. : 5252 ## Max. :99.0 Student : 5098 ## (Other) :22711 ## occupation_of_respondent_recoded ## Employed (10-18 in d15a) :32763 ## Not working (1-4 in d15a) :37125 ## Self-employed (5-9 in d15a): 5198 ## ## ## ## ## respondent_occupation_scale_c_14 ## Retired (4 in d15a) :22911 ## Manual workers (15 to 18 in d15a) :15269 ## Other white collars (13 or 14 in d15a): 9203 ## Managers (10 to 12 in d15a) : 8291 ## Self-employed (5 to 9 in d15a) : 5198 ## Students (2 in d15a) : 5098 ## (Other) : 9116 ## type_of_community is_student no_education ## DK : 34 Min. :0.0000 Min. :0.000000 ## Large town :20939 1st Qu.:0.0000 1st Qu.:0.000000 ## Rural area or village :24686 Median :0.0000 Median :0.000000 ## Small or middle sized town: 9850 Mean :0.0679 Mean :0.008151 ## Small/middle town :19577 3rd Qu.:0.0000 3rd Qu.:0.000000 ## Max. :1.0000 Max. :1.000000 ## ## education year region_nuts_codes country_code ## Min. :14.00 2013:25103 LU : 1432 DE : 4531 ## 1st Qu.:17.00 2015: 0 MT : 1398 GB : 3538 ## Median :18.00 2017:25053 CY : 1192 BE : 3028 ## Mean :19.61 2019:24930 SK02 : 1053 CZ : 3023 ## 3rd Qu.:22.00 EL30 : 974 NL : 3019 ## Max. :30.00 EE : 973 SK : 3000 ## (Other):68064 (Other):54947 ## pm2_5 pm10 o3 BaP ## Min. : 2.109 Min. : 5.883 Min. : 66.37 Min. :0.0102 ## 1st Qu.: 9.374 1st Qu.: 28.326 1st Qu.: 90.89 1st Qu.:0.1779 ## Median :11.866 Median : 33.673 Median :102.81 Median :0.4105 ## Mean :12.954 Mean : 38.637 Mean :101.49 Mean :0.8759 ## 3rd Qu.:15.890 3rd Qu.: 49.488 3rd Qu.:110.73 3rd Qu.:1.0692 ## Max. :41.293 Max. …","date":1614988800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614988800,"objectID":"a62b7e3d7582ea5a29027c11a0b18178","permalink":"https://surveyharmonies.eu/post/2021-03-06-individual-join/","publishdate":"2021-03-06T00:00:00Z","relpermalink":"/post/2021-03-06-individual-join/","section":"post","summary":"We created a longitudinal dataset that contains data on the attitudes European people in various countries, provinces and regions thought climate change was a serious world problem back in 2013, 2015, 2017 and 2019. We join the data with air pollution data so that we can see how serious is the environmental degradation in the smaller area of each (anonymous) respondent.","tags":["retrospective-harmonization","surveys","climate-change","climate-awareness"],"title":"Where Are People More Likely To Treat Climate Change as the Most Serious Global Problem?","type":"post"},{"authors":["Daniel Antal"],"categories":["R-bloggers"],"content":"Retrospective survey harmonization comes with many challenges, as we have shown in the introduction to this tutorial case study. In this example, we will work with Eurobarometer’s data.\nThis code tutorial is not outdated, but the retroharmonize R package has a new (development) release with more featues. Click to expand table of contents of the post Table of Contents Get the Data Metadata analysis Metadata: Protocol Variables Metadata: Geographical information Socio-demography and Weights Harmonizing Variable Labels Creating the Longitudional Table Putting It on a Map Please use the development version of retroharmonize:\ndevtools::install_github(\u0026#34;antaldaniel/retroharmonize\u0026#34;) library(retroharmonize) library(dplyr) # this is necessary for the example library(lubridate) # easier date conversion ## Warning: package \u0026#39;lubridate\u0026#39; was built under R version 4.0.4 library(stringr) # You can also use base R string processing functions Get the Data retroharmonize is not associated with Eurobarometer, or its creators, Kantar, or its archivists, GESIS. We assume that you have acquired the necessary files from GESIS after carefully reading their terms and you placed it on a path that you call gesis_dir. The precise documentation of the data we use can be found in this supporting blogpost. To reproduce this blogpost, you will need ZA5877_v2-0-0.sav, ZA6595_v3-0-0.sav, ZA6861_v1-2-0.sav, ZA7488_v1-0-0.sav, ZA7572_v1-0-0.sav in a directory that you will name gesis_dir.\n#Not run in the blogpost. In the repo we have a saved version. climate_change_files \u0026lt;- c(\u0026#34;ZA5877_v2-0-0.sav\u0026#34;, \u0026#34;ZA6595_v3-0-0.sav\u0026#34;, \u0026#34;ZA6861_v1-2-0.sav\u0026#34;, \u0026#34;ZA7488_v1-0-0.sav\u0026#34;, \u0026#34;ZA7572_v1-0-0.sav\u0026#34;) eb_waves \u0026lt;- read_surveys(file.path(gesis_dir, climate_change_files), .f=\u0026#39;read_spss\u0026#39;) if (dir.exists(\u0026#34;data-raw\u0026#34;)) { save ( eb_waves, file: file.path(\u0026#34;data-raw\u0026#34;, \u0026#34;eb_climate_change_waves.rda\u0026#34;) ) } if ( file.exists( file.path(\u0026#34;data-raw\u0026#34;, \u0026#34;eb_climate_change_waves.rda\u0026#34;) )) { load (file.path( \u0026#34;data-raw\u0026#34;, \u0026#34;eb_climate_change_waves.rda\u0026#34; ) ) } else { load (file.path(\u0026#34;..\u0026#34;, \u0026#34;..\u0026#34;, \u0026#34;data-raw\u0026#34;, \u0026#34;eb_climate_change_waves.rda\u0026#34;) ) } The eb_waves nested list contains five surveys imported from SPSS to the survey class of retroharmonize. The survey class is a data.frame that retains important metadata for further harmonization.\ndocument_waves (eb_waves) ## # A tibble: 5 x 5 ## id filename ncol nrow object_size ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ZA5877_v2-0-0 ZA5877_v2-0-0.sav 604 27919 139352456 ## 2 ZA6595_v3-0-0 ZA6595_v3-0-0.sav 519 27718 119370440 ## 3 ZA6861_v1-2-0 ZA6861_v1-2-0.sav 657 27901 151397528 ## 4 ZA7488_v1-0-0 ZA7488_v1-0-0.sav 752 27339 169465928 ## 5 ZA7572_v1-0-0 ZA7572_v1-0-0.sav 348 27655 80562432 Beware the object sizes. If you work with many surveys, memory-efficient programming becomes imperative. We will be subsetting whenever possible.\nMetadata analysis As noted before, prepare to work with nested lists. Each imported survey is nested as a data frame in the eb_waves list.\nMetadata: Protocol Variables Eurobarometer calls certain metadata elements, like interviewee cooperation level or the date of a survey interview as protocol variable. Let’s start here. This will be our template to harmonize more and more aspects of the five surveys (which are, in fact, already harmonization of about 30 surveys conducted in a single ‘wave’ in multiple countries.)\n# select variables of interest from the metadata eb_protocol_metadata \u0026lt;- eb_climate_metadata %\u0026gt;% filter ( .data$label_orig %in% c(\u0026#34;date of interview\u0026#34;) | .data$var_name_orig: = \u0026#34;rowid\u0026#34;) %\u0026gt;% suggest_var_names( survey_program: \u0026#34;eurobarometer\u0026#34; ) # subset and harmonize these variables in all nested list items of \u0026#39;waves\u0026#39; of surveys interview_dates \u0026lt;- harmonize_var_names(eb_waves, eb_protocol_metadata ) # apply similar data processing rules to same variables interview_dates \u0026lt;- lapply (interview_dates, function (x) x %\u0026gt;% mutate ( date_of_interview: as_character(.data$date_of_interview) ) ) # join the individual survey tables into a single table interview_dates \u0026lt;- as_tibble ( Reduce (rbind, interview_dates) ) # Check the variable classes. vapply(interview_dates, function(x) class(x)[1], character(1)) ## rowid date_of_interview ## \u0026#34;character\u0026#34; \u0026#34;character\u0026#34; This is our sample workflow for each block of variables.\nGet a unique identifier. Add other variables Harmonize the variable names Subset the data leaving out anything that you do not harmonize in this block. Apply some normalization in a nested list. When the variables are harmonized to same name, class, merge them into a data.frame-like tibble object. Now finish the harmonization. Wednesday, 31st October 2018 should become a Date type 2018-10-31.\nrequire(lubridate) harmonize_date \u0026lt;- function(x) { x \u0026lt;- tolower(as.character(x)) x \u0026lt;- gsub(\u0026#34;monday|tuesday|wednesday|thursday|friday|saturday|sunday|\\\\,|th|nd|rd|st\u0026#34;, \u0026#34;\u0026#34;, x) x \u0026lt;- gsub(\u0026#34;decemberber\u0026#34;, \u0026#34;december\u0026#34;, x) # all those annoying real-life data problems! x \u0026lt;- stringr::str_trim (x, \u0026#34;both\u0026#34;) x \u0026lt;- gsub(\u0026#34;^0\u0026#34;, \u0026#34;\u0026#34;, x ) x \u0026lt;- …","date":1614902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661858040,"objectID":"34fc3064720d1b75c4aeb3057ca0f45e","permalink":"https://surveyharmonies.eu/post/2021-03-05-retroharmonize-climate/","publishdate":"2021-03-05T00:00:00Z","relpermalink":"/post/2021-03-05-retroharmonize-climate/","section":"post","summary":"In this example we are working with data from surveys that were ex ante harmonized to a certain degree – in our tutorials we are choosing questions that were asked in the same way in many natural languages.  For example, you can compare what percentage of the European people in various countries, provinces and regions thought climate change was a serious world problem back in 2013, 2015, 2017 and 2019.","tags":["Surveys","Climate change","Climate awareness","R"],"title":"Retrospective Survey Harmonization Case Study - Climate Awareness Change in Europe 2013-2019.","type":"post"},{"authors":["Daniel Antal"],"categories":["R-bloggers"],"content":"Reproducible ex post harmonization of survey microdata Retrospective survey harmonization allows the comparison of opinion poll data conducted in different countries or time. In this example we are working with data from surveys that were ex ante harmonized to a certain degree – in our tutorials we are choosing questions that were asked in the same way in many natural languages. For example, you can compare what percentage of the European people in various countries, provinces and regions thought climate change was a serious world problem back in 2013, 2015, 2017 and 2019.\nWe developed the retroharmonize R package to help this process. We have tested the package with about 80 Eurobarometer, 5 Afrobarometer survey files extensively, and a bit with Arabbarometer files. This allows the comparison of various survey answers in about 70 countries. This policy-oriented survey programs were designed to be harmonized to a certain degree, but their ex post harmonization is still necessary, challenging and errorprone. Retrospective harmonization includes harmonization of the different coding used for questions and answer options, post-stratification weights, and using different file formats.\nEurobarometer, Afrobaromer, Arab Barometer and Latinobarómetro make survey files that are harmonized across countries available for research with various terms. Our retroharmonize is not affiliated with them, and to run our examples, you must visit their websites, carefully read their terms, agree to them, and download their data yourself. What we add as a value is that we help to connect their files across time (from different years) or across these programs.\nThe survey programs mentioned above publish their data in the proprietary SPSS format. This file format can be imported and translated to R objects with the haven package; however, we needed to re-design haven’s labelled_spss class to maintain far more metadata, which, in turn, a modification of the labelled class. The haven package was designed and tested with data stored in individual SPSS files.\nThe author of labelled, Joseph Larmarange describes two main approaches to work with labelled data, such as SPSS’s method to store categorical data in the Introduction to labelled.\nTwo main approaches of labelled data conversion. Our approach is a further extension of Approach B. Survey harmonization in our case always means the joining data from several SPSS files, which requires a consistent coding among several data sources. This means that data cleaning and recoding must take place before conversion to factors, character or numeric vectors. This is particularly important with factor data (and their simple character conversions) and numeric data that occasionally contains labels, for example, to describe the reason why certain data is missing. Our tutorial vignette labelled_spss_survey gives you more information about this.\nIn the next series of tutorials, we will deal with an array of problems. These are not for the faint heart – you need to have a solid intermediate level of R to follow.\nTidy, joined survey data The original files identifiers may not be unique, we have to create new, truly unique identifiers. Weighting may not be straightforward. Neither the number of observations or the number of variables (which represents the survey questions and their translation to coded data) is the same. Certain data may be only present in one survey and not the other. This means that you will likely to run loops on lists and not data.frames, but eventually you must carefully join them. Class conversion Similar questions may be imported from a non-native R format, in our case, from an SPSS files, in an inconsistent manner. SPSS’s variable formats cannot be translated unambiguously to R classes. retroharmonize introduced a new S3 class system that handles this problem, but eventually you will have to choose if you want to see a numeric or character coding of each categorical variable. The harmonized surveys, with harmonized variable names and harmonized value labels, must be brought to consistent R representations (most statistical functions will only work on numeric, factor or character data) and carefully joined into a single data table for analysis. Harmonization of variables and variable labels Same variables may come with dissimilar variable names and variable labels. It may be a challenge to match age with age. We need to harmonize the names of variables. The harmonized variables may have different labeling. One may call refused answers as declined and the other refusal. On a simple choice, climate change may be ‘Climate change’ or Problem: Climate change. Binary choices may have survey-specific coding conventions. Value labels must be harmonized. There are good tools to do this in a single file - but we have to work with several of them. Missing value harmonization There are likely to be various types of missing values. Working with missing values is probably where most human …","date":1614816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614816000,"objectID":"1cacf8f9f78c3f525ce934648cb8eca4","permalink":"https://surveyharmonies.eu/post/2021-03-04_retroharmonize_intro/","publishdate":"2021-03-04T00:00:00Z","relpermalink":"/post/2021-03-04_retroharmonize_intro/","section":"post","summary":"Retrospective survey harmonization allows the comparison of opinion poll data conducted in different countries or time.  In this example we are working with data from surveys that were ex ante harmonized to a certain degree – in our tutorials we are choosing questions that were asked in the same way in many natural languages.  For example, you can compare what percentage of the European people in various countries, provinces and regions thought climate change was a serious world problem back in 2013, 2015, 2017 and 2019.","tags":["R","Surveys","Survey harmonization"],"title":"What is Retrospective Survey Harmonization?","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":"Milos Popovic is a researcher, a data scientist, Marie Curie postdoc \u0026amp; Top 10 dataviz \u0026amp; R contributor on Twitter according to NodeXL. He took part in policy debates about terrorism and military intervention and appeared on a number of TV channels including N1 (the CNN affiliate in the Western Balkans), Serbian National Television and Al-Jazeera Balkans. My research interests are at the intersection of civil war dynamics and postwar politics in the Balkans. He is going to join the Data \u0026amp; Lyrics team on International Open Data Day to help us put harmonized environmental degradation perception and environmental sensory data on maps. We asked him four questions about his passion, mapping data. Please join us 6 March 2021 9.30 EST / 15.30 CET for an informal digital coffee.\nAs a researcher, why are you so much drawn into maps? Is this connected to your interest in territorial conflicts, or you have some other inspiration?\nThat’s a great question that really makes me pause and look back at the past 5 years. My mapping story started out of curiosity: I found interesting data on the post-WWII violence in Serbia and thought how cool it would be to make a map in R. I quickly made an unimpressive choropleth map and noticed some unexpected patterns. Then I realized just how much unused violence and census data sits out there while we have no clue about geographic patterns. So, it began. I started off with map-making but my curiosity took me to the world of georeferencing and geospatial analysis. In the process, I created over 300 maps hosted on my website as well as dozens of shapefiles from the scratch.\nI used to think that my interest is linked to growing up in a war-torn country. But, as my map-making evolved, I discovered that my passion is to use maps as a way to democratize the data: to take the scores of unused, and often buried datasets, place them on the map and share the dataviz with people.\nCan you show us an example of the best use of mapped data, and the best map that you have personally created? What is their distinctive value?\nI’m immensely proud of my work that required making the shapefiles from the scratch. For instance, my shapefile of over 1500 Kosovo cadastral settlements came into being after I turned dozens of high-resolution raster files into a shapefile fully compatible with Open Street Maps. After months of hard work, I managed to merge the shapefile with the 2011 Kosovo census and present several laser-focused demographic maps to my audience. Same goes for the settlement shapefile of Republika Srpska [the Serb-speaking entity of Bosnia-Herzegovina — the editor], which I made out of a pdf file and merged with the 2013 census data. Whereas most existing maps take a bird’s eye view, my work offers a more fine-grained view of the local dynamics to stakeholders.\nAnother similar undertaking was my transformation of the pre-WWII German military map of Yugoslavia into a unique shapefile of a few hundred Yugoslav municipalities. I combined this shapefile with the 1931 census data, 80 years after it was first published (better late than never!). It took me almost a year to complete this tremendous project but I enjoyed every bit of it. I have teamed up with my brother who is a web developer and we even made an interactive map of Yugoslavia based on the 1931 census.[The screenshot of this interactive map is the top image in the post – the editor] We hope this project would serve not only scholars but also history enthusiasts to better understand a history of the country that is no more.\nCheck out Milos’s beautiful static and interactive maps on https://milosp.info/ What do you think about collaboration based on open data and open-source software that processes such data?\nIt’s a fantastic opportunity for small teams to bypass traditional gatekeepers such as state institutions or big companies and use open source apps for the benefit of their local communities. For example, the access to Open Street Map allows small teams to map pressing communal issues as crime, deceases, or environmental degradation and come up with innovative solutions. In my work, too, I used OSM has helped me create several fine-grained maps that shed more light on local problems in Serbia such as pollution, car accidents or violence.\nWe are hoping to bring together environmental, sensory data and public attitude data on environmental issues? How can mapping help? What do you expect from this project?\nMore than ever, we are compelled to figure out how maladies spreads locally. Without mapping the hotspots, our understanding of the consequences of, for example, viral transmission or pollution is shrouded with a lot of uncertainty. We might have no clue how environmental issues shape public attitudes in localities until we use the mapping to turn on the light. Mapping would help this project pin down geographic clusters that require immediate attention from the private and public stakeholders.\nPlease join us for a digital coffee, tea …","date":1614802980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614802980,"objectID":"c54d327e4abb3f2b980e418fceacd701","permalink":"https://surveyharmonies.eu/post/2021-03-03-ood_interview_maps/","publishdate":"2021-03-03T22:23:00+02:00","relpermalink":"/post/2021-03-03-ood_interview_maps/","section":"post","summary":"Milos Popovic is a researcher, a data scientist, Marie Curie postdoc \u0026 Top 10 dataviz \u0026 R contributor on Twitter according to NodeXL. He is going to join the Data \u0026 Lyrics team on International Open Data Day to help us put harmonized environmental degradation perception and environmental sensory data on maps. We asked him four questions about his passion, mapping data. Please join us 6 March 2021 9.30 EST / 15.30 CET for an informal digital coffee.","tags":["open data","music-observatory","open data day","maps"],"title":"Open Data Day Interview: Mapping Data with Milos Popovic","type":"post"},{"authors":["Daniel Antal"],"categories":["R-bloggers"],"content":"In our tutorial series, we are going to harmonize the following questionnaire items from five Eurobarometer harmonized survey files. The Eurobarometer survey files are harmonized across countries, but they are only partially harmonized in time.\nAll data must be downloaded from the GESIS Data Archive in Cologne. We are not affiliated with GESIS and you must read and accept their terms to use the data.\nEurobarometer 80.2 (2013) GESIS Data Archive, Cologne. ZA5877 Data file Version 2.0.0, https://doi.org/10.4232/1.12792\nData file: ZA6595 data file (European Commission 2017). Questionnaire: Eurobarometer 83.4 Basic Bilingual Questionnaire Citation: ZA6595 Bibtex QA1a Which of the following do you consider to be the single most serious problem facing the world as a whole? (single choice)\nQA1b Which others do you consider to be serious problems? (multiple choice)\nQA2 And how serious a problem do you think climate change is at this moment? Please use a scale from 1 to 10, with \u0026#39;1\u0026#39; meaning it is \u0026#34;not at all a serious problem (scale 1-10)\nQA4 To what extent do you agree or disagree with each of the following statements? - Fighting climate change and using energy more efficiently can boost the economy and jobs in the EU (agreement-disagreement 4-scale)\nQA4 To what extent do you agree or disagree with each of the following statements? - Reducing fossil fuel imports from outside the EU could benefit the EU economically (agreement-disagreement 4-scale)\nQA5 Have you personally taken any action to fight climate change over the past six months? (binary)\nEurobarometer 83.4 (2015) European Commission, Brussels; Directorate General Communication COMM.A.1 ´Strategy, Corporate Communication Actions and Eurobarometer´GESIS Data Archive, Cologne. ZA6595 Data file Version 3.0.0, https://doi.org/10.4232/1.13146\nData file: ZA6595 data file (European Commission 2018). Questionnaire: Eurobarometer 83.4 Basic Bilingual Questionnaire Citation: ZA6595 Bibtex Eurobarometer 87.1 (2017) European Commission, Brussels; Directorate General Communication, COMM.A.1 ‘Strategic Communication’; European Parliament, Directorate-General for Communication, Public Opinion Monitoring Unit GESIS Data Archive, Cologne. ZA6861 Data file Version 1.2.0, https://doi.org/10.4232/1.12922\nData file: ZA6861 data file. Questionnaire: Eurobarometer 90.2 Basic Bilingual Questionnaire Citation: ZA6861 Bibtex QC1a Which of the following do you consider to be the single most serious problem facing the world as a whole? (single choice)\nQC1b Which others do you consider to be serious problems? (multiple choice)\nQC2 And how serious a problem do you think climate change is at this moment? Please use a scale from 1 to 10, with \u0026#39;1\u0026#39; meaning it is \u0026#34;not at all a serious problem (scale 1-10)\nQc4 To what extent do you agree or disagree with each of the following statements? - Fighting climate change and using energy more efficiently can boost the economy and jobs in the EU (agreement-disagreement 4-scale)\nQc4 To what extent do you agree or disagree with each of the following statements? - Promoting EU expertise in new clean technologies to countries outside the EU can benefit the EU economically (agreement-disagreement 4-scale)\nQc4 To what extent do you agree or disagree with each of the following statements? - Reducing fossil fuel imports from outside the EU can benefit the EU economically (agreement-disagreement 4-scale)\nQc4 To what extent do you agree or disagree with each of the following statements? - Reducing fossil fuel imports from outside the EU can increase the security of EU energy supplies (agreement-disagreement 4-scale)\nQc4 To what extent do you agree or disagree with each of the following statements? - More public financial support should be given to the transition to clean energies even if it means subsidies to fossil fuels should be reduced. (agreement-disagreement 4-scale)\nQc5 Have you personally taken any action to fight climate change over the past six months? (binary)\nEurobarometer 90.2 (2018) European Commission, Brussels; Directorate General Communication, COMM.A.3 ‘Media Monitoring and Eurobarometer’ GESIS Data Archive, Cologne. ZA7488 Data file Version 1.0.0, https://doi.org/10.4232/1.13289\nData file: ZA7488 data file (European Commission 2019a) Questionnaire: Eurobarometer 90.2 Basic Bilingual Questionnaire Citation: ZA7488 Bibtex QB5 To what extent do you agree or disagree with each of the following statements? - Fighting climate change and using energy more efficiently can boost the economy and jobs in the EU (agreement-disagreement 4-scale)\nQB5 To what extent do you agree or disagree with each of the following statements? - Promoting EU expertise in new clean technologies to countries outside the EU can benefit the EU economically (agreement-disagreement 4-scale)\nQB5 To what extent do you agree or disagree with each of the following statements? - Reducing fossil fuel imports from outside the EU can benefit the EU economically (agreement-disagreement …","date":1614729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614729600,"objectID":"458c154b8079191eb3ad0e229857dd05","permalink":"https://surveyharmonies.eu/post/2021-03-04-eurobarometer_data/","publishdate":"2021-03-03T00:00:00Z","relpermalink":"/post/2021-03-04-eurobarometer_data/","section":"post","summary":"In our [tutorial series](http://netzero.dataobservatory.eu/post/2021-03-04_retroharmonize_intro/), we are going to harmonize the following questionnaire items from five Eurobarometer harmonized survey files. The Eurobarometer survey files are harmonized across countries, but they are only partially harmonized in time.","tags":["surveys","Eurobarometer"],"title":"Eurobarometer Surveys Used In Our Project","type":"post"},{"authors":null,"categories":null,"content":"If you cannot find the right data for your policy evaluation, your consulting project, your PhD thesis, your market research, or your scientific research project, it does not mean that the data does not exist, or that it is not available for free. In our experience, up to 95% of available open data is never used, because potential users do not realize it exists or do not know how to access it.\nEvery day, thousands of new datasets become available via the EU open data regime, freedom of information legislation in the United States and other jurisdictions, or open science and scientific reproducibility requirements — but as these datasets have been packaged or processed for different primary, original uses, they often require open data experts to locate them and adapt them to a usable form for reuse in business, scientific, or policy research.\nThe creative and cultural industries often do not participate in government statistics programs because these industries are typically comprised of microenterprises that are exempted from statistical reporting and that file only simplified financial statements and tax returns. This means that finding the appropriate private or public data sources for creative and cultural industry uses requires particularly good data maps.\nData curation means that we are continuously mapping potential data sources and sending requests to download and quality test the most current data sources. Our CEEMID project has produced several thousand indicators, of which a few dozen are available in our Demo Music Observatory.If you have specific data needs for a scientific research, policy evaluation, or business project, we can find and provide the most suitable, most current, and best value data for analysis or for ethical AI applications.\n","date":1611187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611187200,"objectID":"26cccae28579337ef19d4e2d3e6c56c6","permalink":"https://surveyharmonies.eu/services/data-curation/","publishdate":"2021-01-21T00:00:00Z","relpermalink":"/services/data-curation/","section":"services","summary":"We create high value key business and policy evaluation indicators. Scientific proofs require the combination of correctly matching, formatting, and verifying controlled pieces of data. Our data comes from verified and legal sources, with information about use rights and a complete history. You can always take a look at the processing code, too. We do not deal in blood diamonds.","tags":["curation"],"title":"Data Curation","type":"services"},{"authors":null,"categories":null,"content":"Data analysts spend 80% of their time on data processing, even though computers can perform these task much faster, with far less errors, and they can document the process automatically. Data processing can be shared: an analyst in a company and an analyst in an NGO does not have to reprocess the very same data twice*\nSee our blogpost How We Add Value to Public Data With Imputation and Forecasting?.\nPublic data sources are often plagued by missng values. Naively you may think that you can ignore them, but think twice: in most cases, missing data in a table is not missing information, but rather malformatted information. This approach of ignoring or dropping missing values will not be feasible or robust when you want to make a beautiful visualization, or use data in a business forecasting model, a machine learning (AI) applicaton, or a more complex scientific model. All of the above require complete datasets, and naively discarding missing data points amounts to an excessive waste of information. In this example we are continuing the example a not-so-easy to find public dataset.\nCompleting missing datapoints requires statistical production information (why might the data be missing?) and data science knowhow (how to impute the missing value.) If you do not have a good statistician or data scientist in your team, you will need high-quality, complete datasets. This is what our automated data observatories provide.\nSee our blogpost about the Data Sisyphus blogpost. We have a better solution. You can always rely on our API to import directly the latest, best data, but if you want to be sure, you can use our regular backups on Zenodo. Zenodo is an open science repository managed by CERN and supported by the European Union. On Zenodo, you can find an authoritative copy of our indicator (and its previous versions) with a digital object identifier, for example, 10.5281/zenodo.5652118. These datasets will be preserved for decades, and nobody can manipulate them. You cannot accidentally overwrite them, and we have no backdoor access to modify them.\n","date":1611187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611187200,"objectID":"d51ed70385001d6a6fc7e9ebd3da38c2","permalink":"https://surveyharmonies.eu/services/data-processing/","publishdate":"2021-01-21T00:00:00Z","relpermalink":"/services/data-processing/","section":"services","summary":"We create high value key business and policy evaluation indicators. Scientific proofs require the combination of correctly matching, formatting, and verifying controlled pieces of data. Our data comes from verified and legal sources, with information about use rights and a complete history. You can always take a look at the processing code, too. We do not deal in blood diamonds.","tags":["data-processing"],"title":"Data Processing","type":"services"},{"authors":null,"categories":null,"content":"We want to ensure that individual researchers, artists, and professionals, as well as NGOs and small and large organizations can benefit equally from big data in the age of artificial intelligence.\nBig data creates inequality and injustice because it is only the big corporations, big government agencies, and the biggest, best endowed universities that can finance long-lasting, comprehensive data collection programs. Big data, and large, well-processed, tidy, and accurately imputed datasets allow them to unleash the power of machine learning and AI. These large entities are able to create algorithms that decide the commercial success of your product and your artwork, giving them a competitive edge against smaller competitors while helping them evade regulations.\nCheck out our iotables software that helps the use of national accounts data from all EU members states to create economic direct, indirect and induced economic impact calculation, such as employment multipliers or GVA affects of various cultural and creative economy policies.\nCheck out our regions software that helps the harmonization of various European and African standardized surveys.\nCheck out our retroharmonize software that helps the harmonization of various European and African standardized surveys.\n","date":1611187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625616000,"objectID":"96b2cb649d5ffdcef063e548e1e2e92f","permalink":"https://surveyharmonies.eu/services/data-as-service/","publishdate":"2021-01-21T00:00:00Z","relpermalink":"/services/data-as-service/","section":"services","summary":"We provide our clients with simple datasets, databases, harmonized survey data, and various other rich data applications; we provide them with continuous access to high-quality, re-processed, re-usable public sector and scientific data.","tags":["daas","api"],"title":"Data-as-Service","type":"services"},{"authors":["Daniel Antal"],"categories":null,"content":"Retrospective data harmonization The aim of retroharmonize is to provide tools for reproducible retrospective (ex-post) harmonization of datasets that contain variables measuring the same concepts but coded in different ways. Ex-post data harmonization enables better use of existing data and creates new research opportunities. For example, harmonizing data from different countries enables cross-national comparisons, while merging data from different time points makes it possible to track changes over time.\nRetrospective data harmonization is associated with challenges including conceptual issues with establishing equivalence and comparability, practical complications of having to standardize the naming and coding of variables, technical difficulties with merging data stored in different formats, and the need to document a large number of data transformations. The retroharmonize package assists with the latter three components, freeing up the capacity of researchers to focus on the first.\nSpecifically, the retroharmonize package proposes a reproducible workflow, including a new class for storing data together with the harmonized and original metadata, as well as functions for importing data from different formats, harmonizing data and metadata, documenting the harmonization process, and converting between data types. See here for an overview of the functionalities.\nThe new labelled_spss_survey() class is an extension of haven’s labelled_spss class. It not only preserves variable and value labels and the user-defined missing range, but also gives an identifier, for example, the filename or the wave number, to the vector. Additionally, it enables the preservation – as metadata attributes – of the original variable names, labels, and value codes and labels, from the source data, in addition to the harmonized variable names, labels, and value codes and labels. This way, the harmonized data also contain the pre-harmonization record. The stored original metadata can be used for validation and documentation purposes.\nThe vignette Working With The labelled_spss_survey Class provides more information about the labelled_spss_survey() class.\nIn Harmonize Value Labels we discuss the characteristics of the labelled_spss_survey() class and demonstrates the problems that using this class solves.\nWe also provide three extensive case studies illustrating how the retroharmonize package can be used for ex-post harmonization of data from cross-national surveys:\nAfrobarometer Arab Barometer Eurobarometer The creators of retroharmonize are not affiliated with either Afrobarometer, Arab Barometer, Eurobarometer, or the organizations that designs, produces or archives their surveys.\nWe started building an experimental APIs data is running retroharmonize regularly and improving known statistical data sources. See: Digital Music Observatory, Green Deal Data Observatory, Economy Data Observatory.\nCitations and related work Citing the data sources Our package has been tested on three harmonized survey’s microdata. Because retroharmonize is not affiliated with any of these data sources, to replicate our tutorials or work with the data, you have download the data files from these sources, and you have to cite those sources in your work.\nAfrobarometer data: Cite Afrobarometer Arab Barometer data: cite Arab Barometer. Eurobarometer data: The Eurobarometer data Eurobarometer raw data and related documentation (questionnaires, codebooks, etc.) are made available by GESIS, ICPSR and through the Social Science Data Archive networks. You should cite your source, in our examples, we rely on the GESIS data files.\nCiting the retroharmonize R package For main developer and contributors, see the package homepage.\nThis work can be freely used, modified and distributed under the GPL-3 license:\ncitation(\u0026#34;retroharmonize\u0026#34;) #\u0026gt; #\u0026gt; To cite package \u0026#39;retroharmonize\u0026#39; in publications use: #\u0026gt; #\u0026gt; Daniel Antal (2021). retroharmonize: Ex Post Survey Data #\u0026gt; Harmonization. R package version 0.1.17. #\u0026gt; https://retroharmonize.dataobservatory.eu/ #\u0026gt; #\u0026gt; A BibTeX entry for LaTeX users is #\u0026gt; #\u0026gt; @Manual{, #\u0026gt; title = {retroharmonize: Ex Post Survey Data Harmonization}, #\u0026gt; author = {Daniel Antal}, #\u0026gt; year = {2021}, #\u0026gt; doi = {10.5281/zenodo.5006056}, #\u0026gt; note = {R package version 0.1.17}, #\u0026gt; url = {https://retroharmonize.dataobservatory.eu/}, #\u0026gt; } Contact For contact information, contributors, see the package homepage.\nCode of Conduct Please note that the retroharmonize project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.\nClick the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1598313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624870800,"objectID":"d6f90fd73adcc2b6e69ea678f8058c1c","permalink":"https://surveyharmonies.eu/software/retroharmonize/","publishdate":"2020-08-25T00:00:00Z","relpermalink":"/software/retroharmonize/","section":"software","summary":"The goal of retroharmonize is to facilitate retrospective (ex-post) harmonization of data, particularly survey data, in a reproducible manner.","tags":["Surveys","Survey harmonization"],"title":"retroharmonize R package for survey harmonization","type":"software"},{"authors":["Daniel Antal"],"categories":null,"content":"iotables processes all the symmetric input-output tables of the EU member states, and calculates direct, indirect and induced effects, multipliers for GVA, employment, taxation. These are important inputs into policy evaluation, business forecasting, or granting/development indicator design. iotables is used by about 800 experts around the world.\nCode of Conduct Please note that the iotables project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.\nClick the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1591142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644487200,"objectID":"e5736a65571a6c82cc15b66bbb8da8b4","permalink":"https://surveyharmonies.eu/software/iotables/","publishdate":"2020-06-03T00:00:00Z","relpermalink":"/software/iotables/","section":"software","summary":"The goal of iotables is to make allow a programmatic acces to the symmetric input-output tables of Eurostat. It creates multipliers, calculates direct, indirect and induced effects from European SIOT tables.","tags":["Environmental impact analysis","Economic impact analysis"],"title":"iotables R package for working with symmetric input-output tables","type":"software"},{"authors":["Daniel Antal"],"categories":null,"content":"Installation You can install the development version from GitHub with:\ndevtools::install_github(\u0026#34;rOpenGov/regions\u0026#34;) or the released version from CRAN:\ninstall.packages(\u0026#34;devtools\u0026#34;) regions currently takes care of 20,000 sub-divisional boundary changes in Europe since 1999. Comparing departments of France in 2013, with 2007 vojvodinas of Poland and 2018 megyék in Hungary? This extremely errorprone work is automated, as a result, you can compare 110-260 regions for far better analysis. regions was downloaded about 600 researchers in the first month after release.\nYou can review the complete package documentation on regions.dataobservatory.eu. If you find any problems with the code, please raise an issue on Github. Pull requests are welcome if you agree with the Contributor Code of Conduct\nIf you use regions in your work, please cite the package.\nMotivation Working with sub-national statistics has many benefits. In policymaking or in social sciences, it is a common practice to compare national statistics, which can be hugely misleading. The United States of America, the Federal Republic of Germany, Slovakia and Luxembourg are all countries, but they differ vastly in size and social homogeneity. Comparing Slovakia and Luxembourg to the federal states or even regions within Germany, or the states of Germany and the United States can provide more adequate insights. Statistically, the similarity of the aggregation level and high number of observations can allow more precise control of model parameters and errors.\nThe advantages of switching from a national level of the analysis to a sub-national level comes with a huge price in data processing, validation and imputation. The package Regions aims to help this process.\nThis package is an offspring of the eurostat package on rOpenGov. It started as a tool to validate and re-code regional Eurostat statistics, but it aims to be a general solution for all sub-national statistics. It will be developed parallel with other rOpenGov packages.\nSub-national Statistics Have Many Challenges Frequent boundary changes: as opposed to national boundaries, the territorial units, typologies are often change, and this makes the validation and recoding of observation necessary across time. For example, in the European Union, sub-national typologies change about every three years and you have to make sure that you compare the right French region in time, or, if you can make the time-wise comparison at all.\nHierarchical aggregation and special imputation: missingness is very frequent in sub-national statistics, because they are created with a serious time-lag compared to national ones, and because they are often not back-casted after boundary changes. You cannot use standard imputation algorithms because the observations are not similarly aggregated or averaged. Often, the information is seemingly missing, and it is present with an obsolete typology code.\nPackage functionality Generic vocabulary translation and joining functions for geographically coded data Keeping track of the boundary changes within the European Union between 1999-2021 Vocabulary translation and joining functions for standardized European Union statistics Vocabulary translation for the ISO-3166-2 based Google data and the European Union Imputation functions from higher aggregation hierarchy levels to lower ones, for example from NUTS1 to NUTS2 or from ISO-3166-1 to ISO-3166-2 (impute down) Imputation functions from lower hierarchy levels to higher ones (impute up) Aggregation function from lower hierarchy levels to higher ones, for example from NUTS3 to NUTS1 or from ISO-3166-2 to ISO-3166-1 (aggregate; under development) Disaggregation functions from higher hierarchy levels to lower ones, again, for example from NUTS1 to NUTS2 or from ISO-3166-1 to ISO-3166-2 (disaggregate; under development) Vignettes / Articles Working With Regional, Sub-National Statistical Products Validating Your Typology Recoding And Relabelling The Typology Of The Google Mobility Reports (COVID-19) Feedback? Raise and issue on Github or get in touch. Downloaders from CRAN: Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624276800,"objectID":"ffb0c296df8a28131e6ab084f18b3963","permalink":"https://surveyharmonies.eu/software/regions/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/software/regions/","section":"software","summary":"regions currently takes care of 20,000 sub-divisional boundary changes in Europe since 1999.","tags":["Regional statistics"],"title":"regions R package to create sub-national statistical indicators","type":"software"},{"authors":["Daniel Antal","Rita Kiss"],"categories":null,"content":"Original title in Hungarian: A közintézmények újrahasznosítható információinak piaca Magyarországon.\n","date":1260835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1260835200,"objectID":"7c2231e03baa31cc993a73cfe2b3f4b3","permalink":"https://surveyharmonies.eu/publication/hungary_psi_2009/","publishdate":"2009-12-15T00:00:00Z","relpermalink":"/publication/hungary_psi_2009/","section":"publication","summary":"Market size of the re-usable public sector information in Hungary","tags":"-open-government -open-data","title":"Market size of the re-usable public sector information in Hungary","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://surveyharmonies.eu/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]